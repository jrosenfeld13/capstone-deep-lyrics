{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.tokenize\n",
    "import itertools\n",
    "import datetime\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "\n",
    "from copy import copy, deepcopy\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Lyrics Generator - ULMFiT\n",
    "\n",
    "## Set up instructions\n",
    "\n",
    "### Create VM Instance\n",
    "\n",
    "- Go to cloud.google.com, and create a new VM instance\n",
    "- Disk size: 100GB or more\n",
    "- CPUs + Memory: 2vCPUs, 7.5 GB Memory\n",
    "- GPU: K80 (cheaper, less power) or P100 (2.5x more expensive, more power)\n",
    "- Enable http, https traffic\n",
    "- Boot: Deep learning pytorch instance\n",
    "\n",
    "### Network configuration\n",
    "\n",
    "In Google cloud platform:\n",
    "\n",
    "- Go to Networking -> VPC Network, External IP addresses\n",
    "- Select your VM instance and change the external address type from Ephemeral to Static\n",
    "- Go to Networking -> VPC Network, Firewall Rules\n",
    "- Add a new Rule, called Jupyter, ip ranges 0.0.0.0/0, protocols and ports tcp:8888, apply to all targets\n",
    "\n",
    "### VM + Jupyter Setup\n",
    "\n",
    "- SSH to VM\n",
    "- Enlist into Github repo\n",
    "- Run src/setup.sh\n",
    "- Run jupyter notebook\n",
    "- Open a google cloud shell\n",
    "- Run gcloud init and answer the questions\n",
    "- To set up a tunnel and run jupyter locally, run ```gcloud compute --project \"<your project>\" ssh --zone \"<your zone>\" \"<your instance name>\" -- -L 8888:localhost:8888```\n",
    "- Open jupyter notebook in your local computer and have fun\n",
    "\n",
    "### Notebook first run\n",
    "Here are some steps to run the first time you use the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokens\n",
    "To create the model's tokens with the correct train-test split, run ```src/data_collection/lm_data_lyrics.py -o path/to/save```. \n",
    "We recommend saving in data/models/{MODEL_NAME}. Alternatively, run the magic command below and replace the model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numericalizing train.\n",
      "Numericalizing valid.\n"
     ]
    }
   ],
   "source": [
    "%run ../src/data_collection/lm_data_lyrics.py -o ../data/models/3.1-ULMFiT-108k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created the tokens, let's load them into a `DataBunch` to train our LM further or generate text with a pre-trained LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '3.1-ULMFiT-108k'\n",
    "MODEL_PATH = Path(f'../data/models/{model_name}')\n",
    "MODEL_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10002\n"
     ]
    }
   ],
   "source": [
    "data_lm = TextLMDataBunch.from_tokens(MODEL_PATH,\n",
    "                                      bs=128,\n",
    "                                      max_vocab=10000)\n",
    "\n",
    "print(data_lm.train_ds.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = RNNLearner.language_model(data_lm,\n",
    "                                  pretrained_model=URLs.IMDB,\n",
    "                                  drop_mult=0.5)\n",
    "\n",
    "save_callback = SaveModel(learn, model_name='ULMFiT_3.0-108k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "        \t/* Turns off some styling */\n",
       "        \tprogress {\n",
       "\n",
       "            \t/* gets rid of default border in Firefox and Opera. */\n",
       "            \tborder: none;\n",
       "\n",
       "            \t/* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "            \tbackground-size: auto;\n",
       "            }\n",
       "\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='141772701' class='' max='141772701', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [141772701/141772701 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DOWNLOAD_MODEL_WEIGHTS = True\n",
    "weights_url = 'https://storage.googleapis.com/w210-capstone/models/ULMFiT_3.0-108k_best.pth'\n",
    "\n",
    "if DOWNLOAD_MODEL_WEIGHTS:\n",
    "    Path(MODEL_PATH/'models').mkdir(exist_ok=True)\n",
    "    download_url(weights_url, MODEL_PATH/f'models/{model_name}_best.pth', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_load(self, name:PathOrStr):\n",
    "    \"\"\"Load model onto CPU that was trained on a GPU `name` from `self.model_dir`.\n",
    "       We need these because the fastai load function doesn't allow for a remapping of the storage location.\"\"\"\n",
    "    self.model.load_state_dict(torch.load(self.path/self.model_dir/f'{name}.pth', map_location=lambda storage, loc: storage))\n",
    "\n",
    "setattr(RNNLearner, 'cpu_load', cpu_load) #monkey patch onto our RNNLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not GPU:\n",
    "    learn.cpu_load(f'{model_name}_best')\n",
    "else:\n",
    "    learn.load(f'{model_name}_best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SaveModel(LearnerCallback):\n",
    "    \"\"\"Save Latest Model\"\"\"\n",
    "    def __init__(self, learn:Learner, model_name='saved_model'):\n",
    "        super().__init__(learn)\n",
    "        self.model_name = model_name\n",
    "        self.model_date = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "        self.best_loss = None\n",
    "        self.perplexity = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch:int, metrics, last_metrics, **kwargs):\n",
    "        loss, *_ = last_metrics\n",
    "        perp = np.exp(loss)\n",
    "        self.perplexity.append(perp)\n",
    "        if self.best_loss == None or loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "            self.learn.save(f'{self.model_name}_best')\n",
    "        return False\n",
    "    \n",
    "    def on_train_end(self, epoch:int, **kwargs):\n",
    "        self.learn.save(f'{self.model_name}_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_callback = SaveModel(learn, model_name=f'{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    learn.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    learn.unfreeze()\n",
    "    learn.fit(10, 1e-3, callbacks=[save_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation loss:  None\n"
     ]
    }
   ],
   "source": [
    "print(\"best validation loss: \", learn.save_model.best_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step(learner, context, context_length):\n",
    "\n",
    "    model = learner.model\n",
    "    \n",
    "    if GPU:\n",
    "        context = LongTensor(context[-context_length:]).view(-1,1).cuda()\n",
    "    else:\n",
    "        context = LongTensor(context[-context_length:]).view(-1,1).cpu()\n",
    "    \n",
    "    context = torch.autograd.Variable(context)\n",
    "    \n",
    "    model.reset()\n",
    "    model.eval()\n",
    "    #print('seq model', context)\n",
    "    # forward pass the \"context\" into the model\n",
    "    result, *_ = model(context)\n",
    "    result = result[-1]\n",
    "    #print (result, len(result))\n",
    "    # set unk and pad to 0 prob\n",
    "    # i.e. never pick unknown or pad\n",
    "    result[0] = -np.inf\n",
    "    result[1] = -np.inf\n",
    "\n",
    "    # softmax and normalize\n",
    "    probabilities = F.softmax(result, dim=0)\n",
    "    probabilities = np.asarray(probabilities.detach().cpu(), dtype=np.float)\n",
    "    probabilities /= np.sum(probabilities) \n",
    "    return probabilities\n",
    "\n",
    "def print_words(sequence):\n",
    "    for i in range(len(sequence[0])):\n",
    "        \n",
    "        step = sequence[0][i]\n",
    "\n",
    "        word = data_lm.valid_ds.vocab.textify([step])\n",
    "\n",
    "        if word == 'xeol':\n",
    "            word = '\\n'\n",
    "        elif 'xbol' in word:\n",
    "            continue\n",
    "        elif word == 'xeos': \n",
    "            print(word)\n",
    "            break\n",
    "            \n",
    "        print(word, end=' ')   \n",
    "\n",
    "def generate_text(learner, seed_text=['xbos'], max_len=500, GPU=False, context_length=20):\n",
    "    \"\"\"Generates text with a given learner and prints string to console.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    learner : RNNLearner Language Model (RNNLearner.language_model())\n",
    "        Fastai RNNLearner with tokenized language model data already loaded \n",
    "        \n",
    "    seed_text : list or str\n",
    "        List of strings where each item is a token. (e.g. ['the', 'cat']) or string that is split on white space\n",
    "\n",
    "    max_len : int\n",
    "        Number of words in generated sequence\n",
    "        \n",
    "    gpu : bool\n",
    "        If you're using a GPU or not...\n",
    "    \n",
    "    context_length : int\n",
    "        Amount of words that get input as \"context\" into the model. Set to 0 for no limit\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None : NoneType\n",
    "        Doesn't return anything, prints string to console\n",
    "    \"\"\"\n",
    "        \n",
    "    if isinstance(seed_text, str):\n",
    "        seed_text = data_lm.train_ds.vocab.numericalize(seed_text.split(' '))\n",
    "    \n",
    "    \n",
    "    # Width for the beam search, to be externalized along with general decoding\n",
    "    beam_width = 5\n",
    "\n",
    "    # List of candidate word sequence. We'll maintain #beam_width top sequences here.\n",
    "    # The context is a list of words, the score is the multiplied probabilities of each word\n",
    "    sequences = [[seed_text, 0.0]]\n",
    "    \n",
    "    # Loop over max number of words\n",
    "    for _ in range(max_len):\n",
    "        \n",
    "        print ('Generating word: ', _, '/', max_len)\n",
    "        candidates = list()\n",
    "        \n",
    "        # For each top sequence, generate the next word, and pick #beam_width candidates\n",
    "        for i in range(len(sequences)):\n",
    "            \n",
    "            # Get a new sequence of word indices and log-probability\n",
    "            # Example: [[2, 138, 661], 23.181717]\n",
    "            words, score = sequences[i]\n",
    "\n",
    "            # Obtain probabilities for next word given the context \n",
    "            probabilities = generate_step(learner, words, context_length)\n",
    "\n",
    "            # Multinomial draw from the probabilities\n",
    "            draw = np.random.multinomial(1, probabilities)\n",
    "            next_word_idx = np.argsort(draw)[::-1][0]\n",
    "            \n",
    "            words.append(next_word_idx) \n",
    "\n",
    "            candidate = [words, (score - log(probabilities[next_word_idx]))]\n",
    "            candidates.append(candidate)\n",
    "\n",
    "        # Once we have the candidate words for each top sequence, do a multinomial draw based off the score to pick the top\n",
    "        # Greedy version would be to pick top N scored sequences\n",
    "        probs = [candidate[1] for candidate in candidates]\n",
    "\n",
    "        # Out of all the candidates, select in a greedy way the top # beam_width to limit the breadth\n",
    "        # of the optimization tree\n",
    "        top = np.argsort(probs)[:beam_width]\n",
    "        top_candidates = list()\n",
    "        for r in range(len(top)):\n",
    "            top_candidates.append(candidates[top[r]])\n",
    "        \n",
    "        sequences = top_candidates\n",
    "        \n",
    "    print_words(sequences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating word:  0 / 200\n",
      "candidates 1\n",
      "Generating word:  1 / 200\n",
      "candidates 1\n",
      "Generating word:  2 / 200\n",
      "candidates 1\n",
      "Generating word:  3 / 200\n",
      "candidates 1\n",
      "Generating word:  4 / 200\n",
      "candidates 1\n",
      "Generating word:  5 / 200\n",
      "candidates 1\n",
      "Generating word:  6 / 200\n",
      "candidates 1\n",
      "Generating word:  7 / 200\n",
      "candidates 1\n",
      "Generating word:  8 / 200\n",
      "candidates 1\n",
      "Generating word:  9 / 200\n",
      "candidates 1\n",
      "Generating word:  10 / 200\n",
      "candidates 1\n",
      "Generating word:  11 / 200\n",
      "candidates 1\n",
      "Generating word:  12 / 200\n",
      "candidates 1\n",
      "Generating word:  13 / 200\n",
      "candidates 1\n",
      "Generating word:  14 / 200\n",
      "candidates 1\n",
      "Generating word:  15 / 200\n",
      "candidates 1\n",
      "Generating word:  16 / 200\n",
      "candidates 1\n",
      "Generating word:  17 / 200\n",
      "candidates 1\n",
      "Generating word:  18 / 200\n",
      "candidates 1\n",
      "Generating word:  19 / 200\n",
      "candidates 1\n",
      "Generating word:  20 / 200\n",
      "candidates 1\n",
      "Generating word:  21 / 200\n",
      "candidates 1\n",
      "Generating word:  22 / 200\n",
      "candidates 1\n",
      "Generating word:  23 / 200\n",
      "candidates 1\n",
      "Generating word:  24 / 200\n",
      "candidates 1\n",
      "Generating word:  25 / 200\n",
      "candidates 1\n",
      "Generating word:  26 / 200\n",
      "candidates 1\n",
      "Generating word:  27 / 200\n",
      "candidates 1\n",
      "Generating word:  28 / 200\n",
      "candidates 1\n",
      "Generating word:  29 / 200\n",
      "candidates 1\n",
      "Generating word:  30 / 200\n",
      "candidates 1\n",
      "Generating word:  31 / 200\n",
      "candidates 1\n",
      "Generating word:  32 / 200\n",
      "candidates 1\n",
      "Generating word:  33 / 200\n",
      "candidates 1\n",
      "Generating word:  34 / 200\n",
      "candidates 1\n",
      "Generating word:  35 / 200\n",
      "candidates 1\n",
      "Generating word:  36 / 200\n",
      "candidates 1\n",
      "Generating word:  37 / 200\n",
      "candidates 1\n",
      "Generating word:  38 / 200\n",
      "candidates 1\n",
      "Generating word:  39 / 200\n",
      "candidates 1\n",
      "Generating word:  40 / 200\n",
      "candidates 1\n",
      "Generating word:  41 / 200\n",
      "candidates 1\n",
      "Generating word:  42 / 200\n",
      "candidates 1\n",
      "Generating word:  43 / 200\n",
      "candidates 1\n",
      "Generating word:  44 / 200\n",
      "candidates 1\n",
      "Generating word:  45 / 200\n",
      "candidates 1\n",
      "Generating word:  46 / 200\n",
      "candidates 1\n",
      "Generating word:  47 / 200\n",
      "candidates 1\n",
      "Generating word:  48 / 200\n",
      "candidates 1\n",
      "Generating word:  49 / 200\n",
      "candidates 1\n",
      "Generating word:  50 / 200\n",
      "candidates 1\n",
      "Generating word:  51 / 200\n",
      "candidates 1\n",
      "Generating word:  52 / 200\n",
      "candidates 1\n",
      "Generating word:  53 / 200\n",
      "candidates 1\n",
      "Generating word:  54 / 200\n",
      "candidates 1\n",
      "Generating word:  55 / 200\n",
      "candidates 1\n",
      "Generating word:  56 / 200\n",
      "candidates 1\n",
      "Generating word:  57 / 200\n",
      "candidates 1\n",
      "Generating word:  58 / 200\n",
      "candidates 1\n",
      "Generating word:  59 / 200\n",
      "candidates 1\n",
      "Generating word:  60 / 200\n",
      "candidates 1\n",
      "Generating word:  61 / 200\n",
      "candidates 1\n",
      "Generating word:  62 / 200\n",
      "candidates 1\n",
      "Generating word:  63 / 200\n",
      "candidates 1\n",
      "Generating word:  64 / 200\n",
      "candidates 1\n",
      "Generating word:  65 / 200\n",
      "candidates 1\n",
      "Generating word:  66 / 200\n",
      "candidates 1\n",
      "Generating word:  67 / 200\n",
      "candidates 1\n",
      "Generating word:  68 / 200\n",
      "candidates 1\n",
      "Generating word:  69 / 200\n",
      "candidates 1\n",
      "Generating word:  70 / 200\n",
      "candidates 1\n",
      "Generating word:  71 / 200\n",
      "candidates 1\n",
      "Generating word:  72 / 200\n",
      "candidates 1\n",
      "Generating word:  73 / 200\n",
      "candidates 1\n",
      "Generating word:  74 / 200\n",
      "candidates 1\n",
      "Generating word:  75 / 200\n",
      "candidates 1\n",
      "Generating word:  76 / 200\n",
      "candidates 1\n",
      "Generating word:  77 / 200\n",
      "candidates 1\n",
      "Generating word:  78 / 200\n",
      "candidates 1\n",
      "Generating word:  79 / 200\n",
      "candidates 1\n",
      "Generating word:  80 / 200\n",
      "candidates 1\n",
      "Generating word:  81 / 200\n",
      "candidates 1\n",
      "Generating word:  82 / 200\n",
      "candidates 1\n",
      "Generating word:  83 / 200\n",
      "candidates 1\n",
      "Generating word:  84 / 200\n",
      "candidates 1\n",
      "Generating word:  85 / 200\n",
      "candidates 1\n",
      "Generating word:  86 / 200\n",
      "candidates 1\n",
      "Generating word:  87 / 200\n",
      "candidates 1\n",
      "Generating word:  88 / 200\n",
      "candidates 1\n",
      "Generating word:  89 / 200\n",
      "candidates 1\n",
      "Generating word:  90 / 200\n",
      "candidates 1\n",
      "Generating word:  91 / 200\n",
      "candidates 1\n",
      "Generating word:  92 / 200\n",
      "candidates 1\n",
      "Generating word:  93 / 200\n",
      "candidates 1\n",
      "Generating word:  94 / 200\n",
      "candidates 1\n",
      "Generating word:  95 / 200\n",
      "candidates 1\n",
      "Generating word:  96 / 200\n",
      "candidates 1\n",
      "Generating word:  97 / 200\n",
      "candidates 1\n",
      "Generating word:  98 / 200\n",
      "candidates 1\n",
      "Generating word:  99 / 200\n",
      "candidates 1\n",
      "Generating word:  100 / 200\n",
      "candidates 1\n",
      "Generating word:  101 / 200\n",
      "candidates 1\n",
      "Generating word:  102 / 200\n",
      "candidates 1\n",
      "Generating word:  103 / 200\n",
      "candidates 1\n",
      "Generating word:  104 / 200\n",
      "candidates 1\n",
      "Generating word:  105 / 200\n",
      "candidates 1\n",
      "Generating word:  106 / 200\n",
      "candidates 1\n",
      "Generating word:  107 / 200\n",
      "candidates 1\n",
      "Generating word:  108 / 200\n",
      "candidates 1\n",
      "Generating word:  109 / 200\n",
      "candidates 1\n",
      "Generating word:  110 / 200\n",
      "candidates 1\n",
      "Generating word:  111 / 200\n",
      "candidates 1\n",
      "Generating word:  112 / 200\n",
      "candidates 1\n",
      "Generating word:  113 / 200\n",
      "candidates 1\n",
      "Generating word:  114 / 200\n",
      "candidates 1\n",
      "Generating word:  115 / 200\n",
      "candidates 1\n",
      "Generating word:  116 / 200\n",
      "candidates 1\n",
      "Generating word:  117 / 200\n",
      "candidates 1\n",
      "Generating word:  118 / 200\n",
      "candidates 1\n",
      "Generating word:  119 / 200\n",
      "candidates 1\n",
      "Generating word:  120 / 200\n",
      "candidates 1\n",
      "Generating word:  121 / 200\n",
      "candidates 1\n",
      "Generating word:  122 / 200\n",
      "candidates 1\n",
      "Generating word:  123 / 200\n",
      "candidates 1\n",
      "Generating word:  124 / 200\n",
      "candidates 1\n",
      "Generating word:  125 / 200\n",
      "candidates 1\n",
      "Generating word:  126 / 200\n",
      "candidates 1\n",
      "Generating word:  127 / 200\n",
      "candidates 1\n",
      "Generating word:  128 / 200\n",
      "candidates 1\n",
      "Generating word:  129 / 200\n",
      "candidates 1\n",
      "Generating word:  130 / 200\n",
      "candidates 1\n",
      "Generating word:  131 / 200\n",
      "candidates 1\n",
      "Generating word:  132 / 200\n",
      "candidates 1\n",
      "Generating word:  133 / 200\n",
      "candidates 1\n",
      "Generating word:  134 / 200\n",
      "candidates 1\n",
      "Generating word:  135 / 200\n",
      "candidates 1\n",
      "Generating word:  136 / 200\n",
      "candidates 1\n",
      "Generating word:  137 / 200\n",
      "candidates 1\n",
      "Generating word:  138 / 200\n",
      "candidates 1\n",
      "Generating word:  139 / 200\n",
      "candidates 1\n",
      "Generating word:  140 / 200\n",
      "candidates 1\n",
      "Generating word:  141 / 200\n",
      "candidates 1\n",
      "Generating word:  142 / 200\n",
      "candidates 1\n",
      "Generating word:  143 / 200\n",
      "candidates 1\n",
      "Generating word:  144 / 200\n",
      "candidates 1\n",
      "Generating word:  145 / 200\n",
      "candidates 1\n",
      "Generating word:  146 / 200\n",
      "candidates 1\n",
      "Generating word:  147 / 200\n",
      "candidates 1\n",
      "Generating word:  148 / 200\n",
      "candidates 1\n",
      "Generating word:  149 / 200\n",
      "candidates 1\n",
      "Generating word:  150 / 200\n",
      "candidates 1\n",
      "Generating word:  151 / 200\n",
      "candidates 1\n",
      "Generating word:  152 / 200\n",
      "candidates 1\n",
      "Generating word:  153 / 200\n",
      "candidates 1\n",
      "Generating word:  154 / 200\n",
      "candidates 1\n",
      "Generating word:  155 / 200\n",
      "candidates 1\n",
      "Generating word:  156 / 200\n",
      "candidates 1\n",
      "Generating word:  157 / 200\n",
      "candidates 1\n",
      "Generating word:  158 / 200\n",
      "candidates 1\n",
      "Generating word:  159 / 200\n",
      "candidates 1\n",
      "Generating word:  160 / 200\n",
      "candidates 1\n",
      "Generating word:  161 / 200\n",
      "candidates 1\n",
      "Generating word:  162 / 200\n",
      "candidates 1\n",
      "Generating word:  163 / 200\n",
      "candidates 1\n",
      "Generating word:  164 / 200\n",
      "candidates 1\n",
      "Generating word:  165 / 200\n",
      "candidates 1\n",
      "Generating word:  166 / 200\n",
      "candidates 1\n",
      "Generating word:  167 / 200\n",
      "candidates 1\n",
      "Generating word:  168 / 200\n",
      "candidates 1\n",
      "Generating word:  169 / 200\n",
      "candidates 1\n",
      "Generating word:  170 / 200\n",
      "candidates 1\n",
      "Generating word:  171 / 200\n",
      "candidates 1\n",
      "Generating word:  172 / 200\n",
      "candidates 1\n",
      "Generating word:  173 / 200\n",
      "candidates 1\n",
      "Generating word:  174 / 200\n",
      "candidates 1\n",
      "Generating word:  175 / 200\n",
      "candidates 1\n",
      "Generating word:  176 / 200\n",
      "candidates 1\n",
      "Generating word:  177 / 200\n",
      "candidates 1\n",
      "Generating word:  178 / 200\n",
      "candidates 1\n",
      "Generating word:  179 / 200\n",
      "candidates 1\n",
      "Generating word:  180 / 200\n",
      "candidates 1\n",
      "Generating word:  181 / 200\n",
      "candidates 1\n",
      "Generating word:  182 / 200\n",
      "candidates 1\n",
      "Generating word:  183 / 200\n",
      "candidates 1\n",
      "Generating word:  184 / 200\n",
      "candidates 1\n",
      "Generating word:  185 / 200\n",
      "candidates 1\n",
      "Generating word:  186 / 200\n",
      "candidates 1\n",
      "Generating word:  187 / 200\n",
      "candidates 1\n",
      "Generating word:  188 / 200\n",
      "candidates 1\n",
      "Generating word:  189 / 200\n",
      "candidates 1\n",
      "Generating word:  190 / 200\n",
      "candidates 1\n",
      "Generating word:  191 / 200\n",
      "candidates 1\n",
      "Generating word:  192 / 200\n",
      "candidates 1\n",
      "Generating word:  193 / 200\n",
      "candidates 1\n",
      "Generating word:  194 / 200\n",
      "candidates 1\n",
      "Generating word:  195 / 200\n",
      "candidates 1\n",
      "Generating word:  196 / 200\n",
      "candidates 1\n",
      "Generating word:  197 / 200\n",
      "candidates 1\n",
      "Generating word:  198 / 200\n",
      "candidates 1\n",
      "Generating word:  199 / 200\n",
      "candidates 1\n",
      "xbos [verse-1] \n",
      " the answer is someone made a stranger \n",
      " but not even a fool could be a fool \n",
      " i didn 't even notice in this little bar \n",
      " i was every single one who said it wasn 't there \n",
      " as the brother walked inside the room and said \n",
      " \"this ain 't the fun of our old friends \n",
      " no , you were the best they ever knew \" \n",
      " \n",
      " well , you were right , but you 're mr . right \n",
      " it 's easy to see soi got you in his hands som \n",
      " ja , ja , you , you , you are \n",
      " \n",
      " e . b . , ich . paul . für dich , da \n",
      " la , la , la . (yo . , 2 , 3 drinks . ) \n",
      " \n",
      " oh , teardrops , oh , dried up . oh , oh \n",
      " if it ain 't up to my feet you might as well go \n",
      " can 't get 'em together \n",
      " ce n \n",
      " \n",
      " round , round , round "
     ]
    }
   ],
   "source": [
    "generate_text(learn, GPU=GPU, seed_text='xbos xbol [verse-1] xeol xbol the answer is', max_len=200, context_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 0 1 2 3]\n",
      "[4 0 1]\n",
      "[1 0 0]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "x = [1, 2, 3, 4, 0]\n",
    "print(np.argsort(x))\n",
    "print(np.argsort(x)[:3])\n",
    "x2 = [0.3, 0.6, 0.1]\n",
    "draw = np.random.multinomial(1, x2)\n",
    "print (draw)\n",
    "print(np.argsort(draw)[::-1][0])\n",
    "#torch.multinomial(torch.tensor(x2), 1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0137527074704766"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "x = set()\n",
    "x.add(1)\n",
    "x.add(1)\n",
    "x.add(2)\n",
    "print (len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
