{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.tokenize\n",
    "import itertools\n",
    "import datetime\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "\n",
    "from copy import copy, deepcopy\n",
    "from enum import Enum\n",
    "\n",
    "from graphviz import Digraph\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Lyrics Generator - ULMFiT\n",
    "\n",
    "## Set up instructions\n",
    "\n",
    "### Create VM Instance\n",
    "\n",
    "- Go to cloud.google.com, and create a new VM instance\n",
    "- Disk size: 100GB or more\n",
    "- CPUs + Memory: 2vCPUs, 7.5 GB Memory\n",
    "- GPU: K80 (cheaper, less power) or P100 (2.5x more expensive, more power)\n",
    "- Enable http, https traffic\n",
    "- Boot: Deep learning pytorch instance\n",
    "\n",
    "### Network configuration\n",
    "\n",
    "In Google cloud platform:\n",
    "\n",
    "- Go to Networking -> VPC Network, External IP addresses\n",
    "- Select your VM instance and change the external address type from Ephemeral to Static\n",
    "- Go to Networking -> VPC Network, Firewall Rules\n",
    "- Add a new Rule, called Jupyter, ip ranges 0.0.0.0/0, protocols and ports tcp:8888, apply to all targets\n",
    "\n",
    "### VM + Jupyter Setup\n",
    "\n",
    "- SSH to VM\n",
    "- Enlist into Github repo\n",
    "- Run src/setup.sh\n",
    "- Run jupyter notebook\n",
    "- Open a google cloud shell\n",
    "- Run gcloud init and answer the questions\n",
    "- To set up a tunnel and run jupyter locally, run ```gcloud compute --project \"<your project>\" ssh --zone \"<your zone>\" \"<your instance name>\" -- -L 8888:localhost:8888```\n",
    "- Open jupyter notebook in your local computer and have fun\n",
    "\n",
    "### Notebook first run\n",
    "Here are some steps to run the first time you use the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokens\n",
    "To create the model's tokens with the correct train-test split, run ```src/data_collection/lm_data_lyrics.py -o path/to/save```. \n",
    "We recommend saving in data/models/{MODEL_NAME}. Alternatively, run the magic command below and replace the model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numericalizing train.\n",
      "Numericalizing valid.\n"
     ]
    }
   ],
   "source": [
    "%run ../src/data_collection/lm_data_lyrics.py -o ../data/models/3.1-ULMFiT-108k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created the tokens, let's load them into a `DataBunch` to train our LM further or generate text with a pre-trained LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '3.1-ULMFiT-108k'\n",
    "MODEL_PATH = Path(f'../data/models/{model_name}')\n",
    "MODEL_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10002\n"
     ]
    }
   ],
   "source": [
    "data_lm = TextLMDataBunch.from_tokens(MODEL_PATH,\n",
    "                                      bs=128,\n",
    "                                      max_vocab=10000)\n",
    "\n",
    "print(data_lm.train_ds.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = RNNLearner.language_model(data_lm,\n",
    "                                  pretrained_model=URLs.IMDB,\n",
    "                                  drop_mult=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "        \t/* Turns off some styling */\n",
       "        \tprogress {\n",
       "\n",
       "            \t/* gets rid of default border in Firefox and Opera. */\n",
       "            \tborder: none;\n",
       "\n",
       "            \t/* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "            \tbackground-size: auto;\n",
       "            }\n",
       "\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='141772796' class='' max='141772796', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [141772796/141772796 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DOWNLOAD_MODEL_WEIGHTS = True\n",
    "weights_url = 'https://storage.googleapis.com/capstone-deep-lyrics/3.1-ULMFiT-108k_best.pth'\n",
    "\n",
    "if DOWNLOAD_MODEL_WEIGHTS:\n",
    "    Path(MODEL_PATH/'models').mkdir(exist_ok=True)\n",
    "    download_url(weights_url, MODEL_PATH/f'models/{model_name}_best.pth', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_load(self, name:PathOrStr):\n",
    "    \"\"\"Load model onto CPU that was trained on a GPU `name` from `self.model_dir`.\n",
    "       We need these because the fastai load function doesn't allow for a remapping of the storage location.\"\"\"\n",
    "    self.model.load_state_dict(torch.load(self.path/self.model_dir/f'{name}.pth', map_location=lambda storage, loc: storage))\n",
    "\n",
    "setattr(RNNLearner, 'cpu_load', cpu_load) #monkey patch onto our RNNLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not GPU:\n",
    "    learn.cpu_load(f'{model_name}_best')\n",
    "else:\n",
    "    learn.load(f'{model_name}_best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SaveModel(LearnerCallback):\n",
    "    \"\"\"Save Latest Model\"\"\"\n",
    "    def __init__(self, learn:Learner, model_name='saved_model'):\n",
    "        super().__init__(learn)\n",
    "        self.model_name = model_name\n",
    "        self.model_date = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "        self.best_loss = None\n",
    "        self.perplexity = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch:int, metrics, last_metrics, **kwargs):\n",
    "        loss, *_ = last_metrics\n",
    "        perp = np.exp(loss)\n",
    "        self.perplexity.append(perp)\n",
    "        if self.best_loss == None or loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "            self.learn.save(f'{self.model_name}_best')\n",
    "        return False\n",
    "    \n",
    "    def on_train_end(self, epoch:int, **kwargs):\n",
    "        self.learn.save(f'{self.model_name}_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_callback = SaveModel(learn, model_name=f'{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    learn.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    learn.unfreeze()\n",
    "    learn.fit(10, 1e-3, callbacks=[save_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation loss:  None\n"
     ]
    }
   ],
   "source": [
    "print(\"best validation loss: \", learn.save_model.best_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step(learner, context, context_length, temp=1):\n",
    "\n",
    "    model = learner.model\n",
    "    \n",
    "    if GPU:\n",
    "        context = LongTensor(context[-context_length:]).view(-1,1).cuda()\n",
    "    else:\n",
    "        context = LongTensor(context[-context_length:]).view(-1,1).cpu()\n",
    "    \n",
    "    context = torch.autograd.Variable(context)\n",
    "    \n",
    "    model.reset()\n",
    "    model.eval()\n",
    "\n",
    "    # forward pass the \"context\" into the model\n",
    "    result, *_ = model(context)\n",
    "    result = result[-1]\n",
    "\n",
    "    # set unk and pad to 0 prob\n",
    "    # i.e. never pick unknown or pad\n",
    "    result[0] = -np.inf\n",
    "    result[1] = -np.inf\n",
    "\n",
    "    # softmax and normalize\n",
    "    probabilities = F.softmax(result/temp, dim=0)\n",
    "    probabilities = np.asarray(probabilities.detach().cpu(), dtype=np.float)\n",
    "    probabilities /= np.sum(probabilities) \n",
    "    return probabilities\n",
    "\n",
    "def get_word_from_index(idx):\n",
    "\n",
    "    return data_lm.valid_ds.vocab.textify([idx])\n",
    "\n",
    "\n",
    "def print_words(context):\n",
    "    for i in range(len(context)):\n",
    "        \n",
    "        step = context[i]\n",
    "\n",
    "        word = data_lm.valid_ds.vocab.textify([step])\n",
    "\n",
    "        if word == 'xeol':\n",
    "            word = 'xeol \\n'\n",
    "        elif 'xbol' in word:\n",
    "            word = 'xbol'\n",
    "        elif word == 'xeos': \n",
    "            print(word)\n",
    "            break\n",
    "            \n",
    "        print(word, end=' ')   \n",
    "\n",
    "def generate_text(learner, seed_text=['xbos'], max_len=500, GPU=False, context_length=20, beam_width=5, temp=1, verbose=True, graph=False):\n",
    "    \"\"\"Generates text with a given learner and returns best options.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    learner : RNNLearner Language Model (RNNLearner.language_model())\n",
    "        Fastai RNNLearner with tokenized language model data already loaded \n",
    "        \n",
    "    seed_text : list or str\n",
    "        List of strings where each item is a token. (e.g. ['the', 'cat']) or string that is split on white space\n",
    "\n",
    "    max_len : int\n",
    "        Number of words in generated sequence\n",
    "        \n",
    "    gpu : bool\n",
    "        If you're using a GPU or not...\n",
    "    \n",
    "    context_length : int\n",
    "        Amount of words that get input as \"context\" into the model. Set to 0 for no limit   \n",
    "        \n",
    "    beam_width : int\n",
    "        How many new word indices to try out...computationally expensive\n",
    "    \n",
    "    verbose : bool\n",
    "        If True, prints every possible context for a given word cycle\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    context_and_scores : list of lists\n",
    "        Returns a sorted list of the entire tree search of contexts and their respective scores in the form:\n",
    "        [[context, score], [context, score], ..., [context, score]]\n",
    "    \"\"\"\n",
    "        \n",
    "    if isinstance(seed_text, str):\n",
    "        seed_text = data_lm.train_ds.vocab.numericalize(seed_text.split(' '))\n",
    "    \n",
    "    \n",
    "    # Width for the beam search, to be externalized along with general decoding\n",
    "    beam_width = beam_width\n",
    "    \n",
    "    if graph:\n",
    "        optimization_graph = Digraph()\n",
    "\n",
    "    # List of candidate word sequence. We'll maintain #beam_width top sequences here.\n",
    "    # The context is a list of words, the scores are the sum of the log probabilities of each word\n",
    "    context_and_scores = [[seed_text, 0.0]]\n",
    "    \n",
    "    # Loop over max number of words\n",
    "    for word_number in range(max_len):\n",
    "        print(f'Generating word: {word_number+1} / {max_len}')\n",
    "\n",
    "        candidates = []\n",
    "        \n",
    "        # For each possible context that we've generated so far, generate new probabilities, \n",
    "        # and pick an additional #beam_width next candidates\n",
    "        for i in range(len(context_and_scores)):\n",
    "            # Get a new sequence of word indices and log-probability\n",
    "            # Example: [[2, 138, 661], 23.181717]\n",
    "            context, score = context_and_scores[i]\n",
    "            \n",
    "            # Obtain probabilities for next word given the context \n",
    "            probabilities = generate_step(learner, context, context_length, temp)\n",
    "\n",
    "            # Multinomial draw from the probabilities\n",
    "            multinom_draw = np.random.multinomial(beam_width, probabilities)\n",
    "            top_probabilities = np.argwhere(multinom_draw != 0).flatten()\n",
    "                        \n",
    "            #For each possible new candidate, update the context and scores\n",
    "            for j in range(len(top_probabilities)):\n",
    "                next_word_idx = top_probabilities[j]\n",
    "                new_context = context + [next_word_idx]\n",
    "                candidate = [new_context, (score - np.log(probabilities[next_word_idx]))]\n",
    "                candidates.append(candidate)\n",
    "                \n",
    "                if graph:\n",
    "                    optimization_graph.node(\"%d_%d\" % (word_number, next_word_idx), \"%s (%.2f)\" % (get_word_from_index(next_word_idx), candidate[1]))\n",
    "                    optimization_graph.edge(\"%d_%d\" % (word_number - 1, context[len(context) -1]), \"%d_%d\" % (word_number, next_word_idx))\n",
    "                \n",
    "        #update the running tally of context and scores and sort by probability of each entry\n",
    "        context_and_scores = candidates\n",
    "        context_and_scores = sorted(context_and_scores, key = lambda x: x[1]) #sort by top entries\n",
    "\n",
    "        context_and_scores = context_and_scores[:15] #for now, only keep the top 15 to speed things up but we can/should change this to beam_width or something else\n",
    "        \n",
    "        if verbose:\n",
    "            for context, score in context_and_scores:\n",
    "                print_words(context)\n",
    "                print('\\n')\n",
    "\n",
    "    if graph:\n",
    "        now = str(datetime.now())\n",
    "        optimization_graph.render(directory='graph_viz/', filename=now, cleanup=True)\n",
    "    return context_and_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating word: 1 / 40\n",
      "Generating word: 2 / 40\n",
      "Generating word: 3 / 40\n",
      "Generating word: 4 / 40\n",
      "Generating word: 5 / 40\n",
      "Generating word: 6 / 40\n",
      "Generating word: 7 / 40\n",
      "Generating word: 8 / 40\n",
      "Generating word: 9 / 40\n",
      "Generating word: 10 / 40\n",
      "Generating word: 11 / 40\n",
      "Generating word: 12 / 40\n",
      "Generating word: 13 / 40\n",
      "Generating word: 14 / 40\n",
      "Generating word: 15 / 40\n",
      "Generating word: 16 / 40\n",
      "Generating word: 17 / 40\n",
      "Generating word: 18 / 40\n",
      "Generating word: 19 / 40\n",
      "Generating word: 20 / 40\n",
      "Generating word: 21 / 40\n",
      "Generating word: 22 / 40\n",
      "Generating word: 23 / 40\n",
      "Generating word: 24 / 40\n",
      "Generating word: 25 / 40\n",
      "Generating word: 26 / 40\n",
      "Generating word: 27 / 40\n",
      "Generating word: 28 / 40\n",
      "Generating word: 29 / 40\n",
      "Generating word: 30 / 40\n",
      "Generating word: 31 / 40\n",
      "Generating word: 32 / 40\n",
      "Generating word: 33 / 40\n",
      "Generating word: 34 / 40\n",
      "Generating word: 35 / 40\n",
      "Generating word: 36 / 40\n",
      "Generating word: 37 / 40\n",
      "Generating word: 38 / 40\n",
      "Generating word: 39 / 40\n",
      "Generating word: 40 / 40\n"
     ]
    }
   ],
   "source": [
    "final_scores = generate_text(learn, GPU=GPU, seed_text='xbos', max_len=40, context_length=200, beam_width=2, verbose=False, temp=1, graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xbos xbol i 've been on the run xeol \n",
      " xbol i 've been on the road xeol \n",
      " xbol i 've been on the road xeol \n",
      " xbol i 've been on the road xeol \n",
      " xbol i 've been on the road xeol \n",
      " 28.94448573254871\n",
      "\n",
      "\n",
      "xbos xbol i 've been on the run xeol \n",
      " xbol i 've been on the road xeol \n",
      " xbol i 've been on the road xeol \n",
      " xbol and i 've been on the road xeol \n",
      " xbol i 've been on the road 32.23796877629395\n",
      "\n",
      "\n",
      "xbos xbol i 've been on the run xeol \n",
      " xbol i 've been on the road xeol \n",
      " xbol i 've been on the road xeol \n",
      " xbol and i 've been on the road xeol \n",
      " xbol and i 've been on the 34.835704896095606\n",
      "\n",
      "\n",
      "xbos xbol i 've been on the run xeol \n",
      " xbol i 've been on the road xeol \n",
      " xbol i 've been on the road xeol \n",
      " xbol and i 've been on the road xeol \n",
      " xbol i 've been on a road 35.055909556728565\n",
      "\n",
      "\n",
      "xbos xbol i 've been on the run xeol \n",
      " xbol i 've been on the road xeol \n",
      " xbol i 've been up and i 've been down xeol \n",
      " xbol i 've been on the road xeol \n",
      " xbol i 've been on 35.97820848666805\n",
      "\n",
      "\n",
      "xbos xbol i 've been on the run xeol \n",
      " xbol i 've been on the road xeol \n",
      " xbol i 've been on the road xeol \n",
      " xbol and i 've been on the road xeol \n",
      " xbol and i 've been running xeol \n",
      " 36.25121366995851\n",
      "\n",
      "\n",
      "xbos xbol i 've been on the run xeol \n",
      " xbol i 've been on the road xeol \n",
      " xbol i 've been up and i 've been down xeol \n",
      " xbol i 've been up and i 've been down xeol \n",
      " xbol i 37.162998613421294\n",
      "\n",
      "\n",
      "xbos xbol i 've been on the run xeol \n",
      " xbol i 've been on the road xeol \n",
      " xbol i 've been on the road xeol \n",
      " xbol and i 've been on the run xeol \n",
      " xbol xeol \n",
      " xbol i 've been on 38.13113650517747\n",
      "\n",
      "\n",
      "xbos xbol i 've got nothing to say to you xeol \n",
      " xbol i don 't know what you 'd do xeol \n",
      " xbol i 've got nothing to do xeol \n",
      " xbol i 've got nothing to do xeol \n",
      " xbol xeol \n",
      " xbol i 38.54852376249201\n",
      "\n",
      "\n",
      "xbos xbol i 've been on the run xeol \n",
      " xbol i 've been on the road xeol \n",
      " xbol i 've been on the road again xeol \n",
      " xbol i 've been nowhere to be found xeol \n",
      " xbol xeol \n",
      " xbol i 've been 39.203398716953785\n",
      "\n",
      "\n",
      "xbos xbol i 've been on the run xeol \n",
      " xbol i 've been on the road xeol \n",
      " xbol i 've been up and i 've been down xeol \n",
      " xbol i 've been up and i 've been around xeol \n",
      " xbol xeol \n",
      " 39.60677355559954\n",
      "\n",
      "\n",
      "xbos xbol i 've got nothing to say to you xeol \n",
      " xbol i don 't know what you 'd do xeol \n",
      " xbol i 've got nothing to do xeol \n",
      " xbol i 've got nothing to do xeol \n",
      " xbol xeol \n",
      " xbol you 39.86423761092897\n",
      "\n",
      "\n",
      "xbos xbol i 've been on the run xeol \n",
      " xbol i 've been on the road xeol \n",
      " xbol i 've been on the road xeol \n",
      " xbol and i 've been on the road xeol \n",
      " xbol and i 've been running in 39.9338537533898\n",
      "\n",
      "\n",
      "xbos xbol i 've been on the run xeol \n",
      " xbol i 've been on the road xeol \n",
      " xbol i 've been up and i 've been down xeol \n",
      " xbol i 've been up and i 've been around xeol \n",
      " xbol i 39.97478600217558\n",
      "\n",
      "\n",
      "xbos xbol i 've been on the run xeol \n",
      " xbol i 've been on the road xeol \n",
      " xbol i 've been on the road xeol \n",
      " xbol and i 've been on the run xeol \n",
      " xbol xeol \n",
      " xbol i 've been to 40.36343155957664\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print all of the final options of songs\n",
    "for song, score in final_scores:\n",
    "    print_words(song)\n",
    "    print(score)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
