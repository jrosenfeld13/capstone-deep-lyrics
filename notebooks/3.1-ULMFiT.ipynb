{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.tokenize\n",
    "import itertools\n",
    "import datetime\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "\n",
    "from copy import copy, deepcopy\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Lyrics Generator - ULMFiT\n",
    "\n",
    "## Set up instructions\n",
    "\n",
    "### Create VM Instance\n",
    "\n",
    "- Go to cloud.google.com, and create a new VM instance\n",
    "- Disk size: 100GB or more\n",
    "- CPUs + Memory: 2vCPUs, 7.5 GB Memory\n",
    "- GPU: K80 (cheaper, less power) or P100 (2.5x more expensive, more power)\n",
    "- Enable http, https traffic\n",
    "- Boot: Deep learning pytorch instance\n",
    "\n",
    "### Network configuration\n",
    "\n",
    "In Google cloud platform:\n",
    "\n",
    "- Go to Networking -> VPC Network, External IP addresses\n",
    "- Select your VM instance and change the external address type from Ephemeral to Static\n",
    "- Go to Networking -> VPC Network, Firewall Rules\n",
    "- Add a new Rule, called Jupyter, ip ranges 0.0.0.0/0, protocols and ports tcp:8888, apply to all targets\n",
    "\n",
    "### VM + Jupyter Setup\n",
    "\n",
    "- SSH to VM\n",
    "- Enlist into Github repo\n",
    "- Run src/setup.sh\n",
    "- Run jupyter notebook\n",
    "- Open a google cloud shell\n",
    "- Run gcloud init and answer the questions\n",
    "- To set up a tunnel and run jupyter locally, run ```gcloud compute --project \"<your project>\" ssh --zone \"<your zone>\" \"<your instance name>\" -- -L 8888:localhost:8888```\n",
    "- Open jupyter notebook in your local computer and have fun\n",
    "\n",
    "### Notebook first run\n",
    "Here are some steps to run the first time you use the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokens\n",
    "To create the model's tokens with the correct train-test split, run ```src/data_collection/lm_data_lyrics.py -o path/to/save```. \n",
    "We recommend saving in data/models/{MODEL_NAME}. Alternatively, run the magic command below and replace the model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numericalizing train.\n",
      "Numericalizing valid.\n"
     ]
    }
   ],
   "source": [
    "%run ../src/data_collection/lm_data_lyrics.py -o ../data/models/3.1-ULMFiT-108k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created the tokens, let's load them into a `DataBunch` to train our LM further or generate text with a pre-trained LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '3.1-ULMFiT-108k'\n",
    "MODEL_PATH = Path(f'../data/models/{model_name}')\n",
    "MODEL_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10002\n"
     ]
    }
   ],
   "source": [
    "data_lm = TextLMDataBunch.from_tokens(MODEL_PATH,\n",
    "                                      bs=128,\n",
    "                                      max_vocab=10000)\n",
    "\n",
    "print(data_lm.train_ds.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = RNNLearner.language_model(data_lm,\n",
    "                                  pretrained_model=URLs.IMDB,\n",
    "                                  drop_mult=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1432a6b8e0f409f8d80925567999a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=141772796), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DOWNLOAD_MODEL_WEIGHTS = True\n",
    "weights_url = 'https://storage.googleapis.com/capstone-deep-lyrics/3.1-ULMFiT-108k_best.pth'\n",
    "\n",
    "if DOWNLOAD_MODEL_WEIGHTS:\n",
    "    Path(MODEL_PATH/'models').mkdir(exist_ok=True)\n",
    "    download_url(weights_url, MODEL_PATH/f'models/{model_name}_best.pth', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_load(self, name:PathOrStr):\n",
    "    \"\"\"Load model onto CPU that was trained on a GPU `name` from `self.model_dir`.\n",
    "       We need these because the fastai load function doesn't allow for a remapping of the storage location.\"\"\"\n",
    "    self.model.load_state_dict(torch.load(self.path/self.model_dir/f'{name}.pth', map_location=lambda storage, loc: storage))\n",
    "\n",
    "setattr(RNNLearner, 'cpu_load', cpu_load) #monkey patch onto our RNNLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not GPU:\n",
    "    learn.cpu_load(f'{model_name}_best')\n",
    "else:\n",
    "    learn.load(f'{model_name}_best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SaveModel(LearnerCallback):\n",
    "    \"\"\"Save Latest Model\"\"\"\n",
    "    def __init__(self, learn:Learner, model_name='saved_model'):\n",
    "        super().__init__(learn)\n",
    "        self.model_name = model_name\n",
    "        self.model_date = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "        self.best_loss = None\n",
    "        self.perplexity = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch:int, metrics, last_metrics, **kwargs):\n",
    "        loss, *_ = last_metrics\n",
    "        perp = np.exp(loss)\n",
    "        self.perplexity.append(perp)\n",
    "        if self.best_loss == None or loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "            self.learn.save(f'{self.model_name}_best')\n",
    "        return False\n",
    "    \n",
    "    def on_train_end(self, epoch:int, **kwargs):\n",
    "        self.learn.save(f'{self.model_name}_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_callback = SaveModel(learn, model_name=f'{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    learn.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    learn.unfreeze()\n",
    "    learn.fit(10, 1e-3, callbacks=[save_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation loss:  None\n"
     ]
    }
   ],
   "source": [
    "print(\"best validation loss: \", learn.save_model.best_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step(learner, context, context_length):\n",
    "\n",
    "    model = learner.model\n",
    "    \n",
    "    if GPU:\n",
    "        context = LongTensor(context[-context_length:]).view(-1,1).cuda()\n",
    "    else:\n",
    "        context = LongTensor(context[-context_length:]).view(-1,1).cpu()\n",
    "    \n",
    "    context = torch.autograd.Variable(context)\n",
    "    \n",
    "    model.reset()\n",
    "    model.eval()\n",
    "\n",
    "    # forward pass the \"context\" into the model\n",
    "    result, *_ = model(context)\n",
    "    result = result[-1]\n",
    "\n",
    "    # set unk and pad to 0 prob\n",
    "    # i.e. never pick unknown or pad\n",
    "    result[0] = -np.inf\n",
    "    result[1] = -np.inf\n",
    "\n",
    "    # softmax and normalize\n",
    "    probabilities = F.softmax(result, dim=0)\n",
    "    probabilities = np.asarray(probabilities.detach().cpu(), dtype=np.float)\n",
    "    probabilities /= np.sum(probabilities) \n",
    "    return probabilities\n",
    "\n",
    "def print_words(context):\n",
    "    for i in range(len(context)):\n",
    "        \n",
    "        step = context[i]\n",
    "\n",
    "        word = data_lm.valid_ds.vocab.textify([step])\n",
    "\n",
    "        if word == 'xeol':\n",
    "            word = '\\n'\n",
    "        elif 'xbol' in word:\n",
    "            continue\n",
    "        elif word == 'xeos': \n",
    "            print(word)\n",
    "            break\n",
    "            \n",
    "        print(word, end=' ')   \n",
    "\n",
    "def generate_text(learner, seed_text=['xbos'], max_len=500, GPU=False, context_length=20, beam_width=5, verbose=True):\n",
    "    \"\"\"Generates text with a given learner and returns best options.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    learner : RNNLearner Language Model (RNNLearner.language_model())\n",
    "        Fastai RNNLearner with tokenized language model data already loaded \n",
    "        \n",
    "    seed_text : list or str\n",
    "        List of strings where each item is a token. (e.g. ['the', 'cat']) or string that is split on white space\n",
    "\n",
    "    max_len : int\n",
    "        Number of words in generated sequence\n",
    "        \n",
    "    gpu : bool\n",
    "        If you're using a GPU or not...\n",
    "    \n",
    "    context_length : int\n",
    "        Amount of words that get input as \"context\" into the model. Set to 0 for no limit   \n",
    "        \n",
    "    beam_width : int\n",
    "        How many new word indices to try out...computationally expensive\n",
    "    \n",
    "    verbose : bool\n",
    "        If True, prints every possible context for a given word cycle\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    context_and_scores : list of lists\n",
    "        Returns a sorted list of the entire tree search of contexts and their respective scores in the form:\n",
    "        [[context, score], [context, score], ..., [context, score]]\n",
    "    \"\"\"\n",
    "        \n",
    "    if isinstance(seed_text, str):\n",
    "        seed_text = data_lm.train_ds.vocab.numericalize(seed_text.split(' '))\n",
    "    \n",
    "    \n",
    "    # Width for the beam search, to be externalized along with general decoding\n",
    "    beam_width = beam_width\n",
    "    \n",
    "    # List of candidate word sequence. We'll maintain #beam_width top sequences here.\n",
    "    # The context is a list of words, the scores are the sum of the log probabilities of each word\n",
    "    context_and_scores = [[seed_text, 0.0]]\n",
    "    \n",
    "    # Loop over max number of words\n",
    "    for word_number in range(max_len):\n",
    "        print(f'Generating word: {word_number+1} / {max_len}')\n",
    "\n",
    "        candidates = []\n",
    "        \n",
    "        # For each possible context that we've generated so far, generate new probabilities, \n",
    "        # and pick an additional #beam_width next candidates\n",
    "        for i in range(len(context_and_scores)):\n",
    "            # Get a new sequence of word indices and log-probability\n",
    "            # Example: [[2, 138, 661], 23.181717]\n",
    "            context, score = context_and_scores[i]\n",
    "            \n",
    "            # Obtain probabilities for next word given the context \n",
    "            probabilities = generate_step(learner, context, context_length)\n",
    "\n",
    "            # Multinomial draw from the probabilities\n",
    "            multinom_draw = np.random.multinomial(beam_width, probabilities)\n",
    "            top_probabilities = np.argwhere(multinom_draw != 0).flatten()\n",
    "                        \n",
    "            #For each possible new candidate, update the context and scores\n",
    "            for j in range(len(top_probabilities)):\n",
    "                next_word_idx = top_probabilities[j]\n",
    "                new_context = context + [next_word_idx]\n",
    "                candidate = [new_context, (score + np.log(probabilities[next_word_idx]))]\n",
    "                candidates.append(candidate)\n",
    "        \n",
    "        #update the running tally of context and scores and sort by probability of each entry\n",
    "        context_and_scores = candidates\n",
    "        context_and_scores = sorted(context_and_scores, key = lambda x: x[1], reverse=True) #sort \n",
    "        context_and_scores = context_and_scores[:15] #for now, only keep the top 15 to speed things up but we can/should change this to beam_width or something else\n",
    "        \n",
    "        if verbose:\n",
    "            for context, score in context_and_scores:\n",
    "                print_words(context)\n",
    "                print('\\n')\n",
    "    \n",
    "    return context_and_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating word: 1 / 50\n",
      "Generating word: 2 / 50\n",
      "Generating word: 3 / 50\n",
      "Generating word: 4 / 50\n",
      "Generating word: 5 / 50\n",
      "Generating word: 6 / 50\n",
      "Generating word: 7 / 50\n",
      "Generating word: 8 / 50\n",
      "Generating word: 9 / 50\n",
      "Generating word: 10 / 50\n",
      "Generating word: 11 / 50\n",
      "Generating word: 12 / 50\n",
      "Generating word: 13 / 50\n",
      "Generating word: 14 / 50\n",
      "Generating word: 15 / 50\n",
      "Generating word: 16 / 50\n",
      "Generating word: 17 / 50\n",
      "Generating word: 18 / 50\n",
      "Generating word: 19 / 50\n",
      "Generating word: 20 / 50\n",
      "Generating word: 21 / 50\n",
      "Generating word: 22 / 50\n",
      "Generating word: 23 / 50\n",
      "Generating word: 24 / 50\n",
      "Generating word: 25 / 50\n",
      "Generating word: 26 / 50\n",
      "Generating word: 27 / 50\n",
      "Generating word: 28 / 50\n",
      "Generating word: 29 / 50\n",
      "Generating word: 30 / 50\n",
      "Generating word: 31 / 50\n",
      "Generating word: 32 / 50\n",
      "Generating word: 33 / 50\n",
      "Generating word: 34 / 50\n",
      "Generating word: 35 / 50\n",
      "Generating word: 36 / 50\n",
      "Generating word: 37 / 50\n",
      "Generating word: 38 / 50\n",
      "Generating word: 39 / 50\n",
      "Generating word: 40 / 50\n",
      "Generating word: 41 / 50\n",
      "Generating word: 42 / 50\n",
      "Generating word: 43 / 50\n",
      "Generating word: 44 / 50\n",
      "Generating word: 45 / 50\n",
      "Generating word: 46 / 50\n",
      "Generating word: 47 / 50\n",
      "Generating word: 48 / 50\n",
      "Generating word: 49 / 50\n",
      "Generating word: 50 / 50\n"
     ]
    }
   ],
   "source": [
    "final_scores = generate_text(learn, GPU=GPU, seed_text='xbos xbol', max_len=50, context_length=50, beam_width=2, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xbos i was lost just in time \n",
      " i was moving on \n",
      " i was lost , i was lost \n",
      " i was lost , i was lost \n",
      " i was lost , i was lost \n",
      " \n",
      " i was lost , i was lost -44.950246659196665\n",
      "\n",
      "\n",
      "xbos i was lost just in time \n",
      " i was moving on \n",
      " i was lost , i was lost \n",
      " i was lost , i was lost \n",
      " i was lost , i was lost , i was lost \n",
      " i was lost , i -46.10677669040947\n",
      "\n",
      "\n",
      "xbos [verse-1] \n",
      " it goes like this like this \n",
      " and you know what i 'm saying \n",
      " you know what i 'm saying \n",
      " you know what i 'm saying \n",
      " you know what i 'm saying \n",
      " you know what i 'm saying -47.05252642199677\n",
      "\n",
      "\n",
      "xbos i was lost just in time \n",
      " i was moving on \n",
      " i was lost , i was lost \n",
      " i was lost , i was lost , i was lost \n",
      " i was lost , i was lost , i was lost \n",
      " i -48.11934013221494\n",
      "\n",
      "\n",
      "xbos i was lost just in time \n",
      " i was moving on \n",
      " i was lost , i was lost \n",
      " i was lost , i was lost \n",
      " i was lost , i was lost \n",
      " \n",
      " i was lost , you was found -49.501421277545816\n",
      "\n",
      "\n",
      "xbos i was lost just in time \n",
      " i was moving on \n",
      " i was lost , i was lost \n",
      " i was lost , i was lost \n",
      " i was lost , i was lost \n",
      " \n",
      " i was lost , you was lost -49.54419021254971\n",
      "\n",
      "\n",
      "xbos i was lost just in time \n",
      " i was moving on \n",
      " i was lost , i was lost \n",
      " i was lost , i was lost \n",
      " \n",
      " and i was lost , i was lost \n",
      " and i was lost , i -51.036291091272915\n",
      "\n",
      "\n",
      "xbos i was lost just in time \n",
      " i was moving on \n",
      " i was lost , i was lost \n",
      " i was lost , i was lost \n",
      " i was lost , i was lost \n",
      " \n",
      " i was lost , you found me -51.216957544219945\n",
      "\n",
      "\n",
      "xbos i was lost just in time \n",
      " i was moving on \n",
      " i was lost , i was lost \n",
      " i was lost , i was lost \n",
      " \n",
      " i 'm lost , lost , lost \n",
      " i 'm lost , lost , lost -52.14501149067493\n",
      "\n",
      "\n",
      "xbos i was lost just in time \n",
      " i was moving on \n",
      " i was lost , i was lost \n",
      " i was lost , i was lost \n",
      " i was lost , and i was lost \n",
      " but now i 'm lost \n",
      " \n",
      " -52.77881875230656\n",
      "\n",
      "\n",
      "xbos i was lost just in time \n",
      " i was moving on \n",
      " i was lost , i was lost \n",
      " i was lost , i was lost \n",
      " \n",
      " i 'm lost , i 'm lost \n",
      " lost , lost , lost \n",
      " -53.38590759527061\n",
      "\n",
      "\n",
      "xbos i was lost just in time \n",
      " i was moving on \n",
      " i was lost , i was lost \n",
      " i was lost , i was lost \n",
      " \n",
      " and i was lost , i was lost \n",
      " and i was lost in the -53.89309380438615\n",
      "\n",
      "\n",
      "xbos i was lost just in time \n",
      " i was moving on \n",
      " i was lost , i was lost \n",
      " i was lost , i was lost \n",
      " i was lost , and i was lost \n",
      " but now i 'm lost , and i -54.88584994470338\n",
      "\n",
      "\n",
      "xbos i was lost just in time \n",
      " i was moving on \n",
      " i was lost , i was lost \n",
      " i was lost , i was lost \n",
      " i was lost , i was lost , i was lost \n",
      " all the time \n",
      " -55.07204148987145\n",
      "\n",
      "\n",
      "xbos i was lost just in time \n",
      " i was moving on \n",
      " i was lost , i was lost \n",
      " i was lost , i was lost \n",
      " \n",
      " i 'm lost , lost , lost \n",
      " i 'm lost , lost , you -55.92747815173009\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print all of the final options of songs\n",
    "for song, score in final_scores:\n",
    "    print_words(song)\n",
    "    print(score)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
