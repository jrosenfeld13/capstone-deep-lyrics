{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning PyTorch for Multimodality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Model of Sequential Modules\n",
    "\n",
    "Example from [here](https://github.com/jcjohnson/pytorch-examples#pytorch-optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 640.2095947265625\n",
      "1 627.4595336914062\n",
      "2 614.9490966796875\n",
      "3 602.6520385742188\n",
      "4 590.566650390625\n",
      "5 578.705078125\n",
      "6 567.0449829101562\n",
      "7 555.6082153320312\n",
      "8 544.3845825195312\n",
      "9 533.3677368164062\n",
      "10 522.5365600585938\n",
      "11 511.8955078125\n",
      "12 501.4302062988281\n",
      "13 491.1579284667969\n",
      "14 481.06689453125\n",
      "15 471.17181396484375\n",
      "16 461.476806640625\n",
      "17 451.9712219238281\n",
      "18 442.61285400390625\n",
      "19 433.43414306640625\n",
      "20 424.41656494140625\n",
      "21 415.5590515136719\n",
      "22 406.8478088378906\n",
      "23 398.2962341308594\n",
      "24 389.88116455078125\n",
      "25 381.60052490234375\n",
      "26 373.45965576171875\n",
      "27 365.4594421386719\n",
      "28 357.6005859375\n",
      "29 349.86651611328125\n",
      "30 342.2503356933594\n",
      "31 334.7592468261719\n",
      "32 327.3841857910156\n",
      "33 320.13360595703125\n",
      "34 312.9990234375\n",
      "35 305.9814147949219\n",
      "36 299.0978698730469\n",
      "37 292.3316345214844\n",
      "38 285.6947021484375\n",
      "39 279.1699523925781\n",
      "40 272.75714111328125\n",
      "41 266.4468688964844\n",
      "42 260.23211669921875\n",
      "43 254.1072540283203\n",
      "44 248.07635498046875\n",
      "45 242.1517791748047\n",
      "46 236.3173828125\n",
      "47 230.57749938964844\n",
      "48 224.92654418945312\n",
      "49 219.3736572265625\n"
     ]
    }
   ],
   "source": [
    "N, D_in, Z_in, H, H2, D_out = 64, 400, 20, 300, 200, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "z = torch.randn(N, Z_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(50):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['0.weight', '0.bias', '2.weight', '2.bias'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().get('0.bias').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding to \"Multi-Modal\" with Custom Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So rather than stringing together modules like above, we have to create a custom class for our model. Mainly, this is because we are concatenating multiple inputs.\n",
    "\n",
    "This still hasn't been tested on real data, and I don't know all the \"gotchas\" of these modules yet, but this works as expected thus far, and creates weight matrices as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multimodal(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, Z_in, H2, D_out):\n",
    "        super(Multimodal, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.multi = torch.nn.Linear(H + Z_in, H2)\n",
    "        self.linear2 = torch.nn.Linear(H2, D_out)\n",
    "        \n",
    "    def forward(self, x, z):\n",
    "        h1 = self.linear1(x)\n",
    "        h1_z = torch.cat([h1, z], dim=1)\n",
    "        h2 = self.multi(h1_z)\n",
    "        out = self.linear2(h2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 656.3359985351562\n",
      "1 640.2750244140625\n",
      "2 624.4983520507812\n",
      "3 609.0003662109375\n",
      "4 593.7750244140625\n",
      "5 578.8157958984375\n",
      "6 564.115478515625\n",
      "7 549.6661987304688\n",
      "8 535.4600830078125\n",
      "9 521.4890747070312\n",
      "10 507.74517822265625\n",
      "11 494.2205810546875\n",
      "12 480.9076843261719\n",
      "13 467.7993469238281\n",
      "14 454.88873291015625\n",
      "15 442.1695251464844\n",
      "16 429.6359558105469\n",
      "17 417.2827453613281\n",
      "18 405.105224609375\n",
      "19 393.09930419921875\n",
      "20 381.2615661621094\n",
      "21 369.58917236328125\n",
      "22 358.0799560546875\n",
      "23 346.7322998046875\n",
      "24 335.5452575683594\n",
      "25 324.5184326171875\n",
      "26 313.6520080566406\n",
      "27 302.9466552734375\n",
      "28 292.403564453125\n",
      "29 282.0243225097656\n",
      "30 271.8110046386719\n",
      "31 261.7658996582031\n",
      "32 251.8917236328125\n",
      "33 242.19137573242188\n",
      "34 232.66799926757812\n",
      "35 223.32484436035156\n",
      "36 214.16529846191406\n",
      "37 205.19277954101562\n",
      "38 196.4107208251953\n",
      "39 187.8224639892578\n",
      "40 179.43133544921875\n",
      "41 171.240478515625\n",
      "42 163.2528839111328\n",
      "43 155.47132873535156\n",
      "44 147.8983917236328\n",
      "45 140.536376953125\n",
      "46 133.3872833251953\n",
      "47 126.4528579711914\n",
      "48 119.73450469970703\n",
      "49 113.23332214355469\n"
     ]
    }
   ],
   "source": [
    "N, D_in, Z_in, H, H2, D_out = 64, 400, 20, 300, 200, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "z = torch.randn(N, Z_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "model = Multimodal(D_in, H, Z_in, H2, D_out)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(50):\n",
    "    y_pred = model(x, z)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the \"multi\" weights have input weights of length 320 because we take 300 from the output of `linear1` + 20 from the auxillary input (e.g. z) and then we map this to the output of 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['linear1.weight', 'linear1.bias', 'multi.weight', 'multi.bias', 'linear2.weight', 'linear2.bias'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 320])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().get('multi.weight').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multimodal(\n",
      "  (linear1): Linear(in_features=400, out_features=300, bias=True)\n",
      "  (multi): Linear(in_features=320, out_features=200, bias=True)\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Multimodality to FastAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bring in some sample data using fastai imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([69, 64, 10])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "data_lm = TextLMDataBunch.from_csv(path)\n",
    "\n",
    "for x, y in list(data_lm.train_dl): # just testing with one batch\n",
    "    x, y\n",
    "    \n",
    "z = torch.randn(x.shape[0], x.shape[1], 10,\n",
    "                device=device, requires_grad=True) # making up 10 \"audio features\"\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([69, 64])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure we can run the same model as fastai manually..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sz = 20000\n",
    "emb_sz = 400\n",
    "n_hid = 1150\n",
    "n_layers = 3\n",
    "pad_token= 1\n",
    "qrnn = False\n",
    "bidir = False\n",
    "\n",
    "dps = np.array([0.25, 0.1, 0.2, 0.02, 0.15])\n",
    "\n",
    "hidden_p = dps[4]\n",
    "input_p = dps[0]\n",
    "embed_p = dps[3]\n",
    "weight_p = dps[2]\n",
    "\n",
    "tie_weights = True\n",
    "output_p = dps[1]\n",
    "bias = True\n",
    "\n",
    "audio_sz = 10\n",
    "\n",
    "# Create a full AWD-LSTM.\n",
    "rnn_enc = RNNCore(vocab_sz=vocab_sz,\n",
    "                  emb_sz=emb_sz,\n",
    "                  n_hid=n_hid,\n",
    "                  n_layers=n_layers,\n",
    "                  pad_token=pad_token,\n",
    "                  qrnn=qrnn,\n",
    "                  bidir=bidir,\n",
    "                  hidden_p=hidden_p,\n",
    "                  input_p=input_p,\n",
    "                  embed_p=embed_p,\n",
    "                  weight_p=weight_p)\n",
    "\n",
    "enc = rnn_enc.encoder if tie_weights else None\n",
    "model = SequentialRNN(rnn_enc, LinearDecoder(vocab_sz, emb_sz, output_p, tie_encoder=enc, bias=bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9.899279594421387\n",
      "1 9.893121719360352\n",
      "2 9.886876106262207\n",
      "3 9.880671501159668\n",
      "4 9.872952461242676\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# opt_params = multimodal_rnn.parameters()\n",
    "\n",
    "# this is a hack for now... not sure if this messes up the graph somewhere by doing this\n",
    "opt_params = [par for par in model.parameters() if par.is_leaf]\n",
    "optimizer = torch.optim.Adam(opt_params, lr=learning_rate)\n",
    "\n",
    "y_pred = model(x)\n",
    "\n",
    "for t in range(5):\n",
    "    y_pred = model(x)[0]\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, try porting this into the fastai learner object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "        \t/* Turns off some styling */\n",
       "        \tprogress {\n",
       "\n",
       "            \t/* gets rid of default border in Firefox and Opera. */\n",
       "            \tborder: none;\n",
       "\n",
       "            \t/* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "            \tbackground-size: auto;\n",
       "            }\n",
       "\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "\n",
       "  </tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "        \t/* Turns off some styling */\n",
       "        \tprogress {\n",
       "\n",
       "            \t/* gets rid of default border in Firefox and Opera. */\n",
       "            \tborder: none;\n",
       "\n",
       "            \t/* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "            \tbackground-size: auto;\n",
       "            }\n",
       "\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='progress-bar-interrupted' max='51', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      Interrupted\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-3447b831db17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNNLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         fit(epochs, self.model, self.loss_func, opt=self.opt, data=self.data, metrics=self.metrics,\n\u001b[0;32m--> 162\u001b[0;31m             callbacks=self.callbacks+callbacks)\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, model, loss_func, opt, data, callbacks, metrics)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_loss_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/text/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mnew_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mraw_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhid_dp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mraw_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0mnew_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mraw_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/text/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m#To avoid the warning that comes because the weights aren't flattened.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.reset()\n",
    "learn = RNNLearner(data_lm, model)\n",
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiModalRNN(RNNCore):\n",
    "#     def __init__(self, audio_sz, **kwargs):\n",
    "#         super(MultiModalRNN, self).__init__(**kwargs)\n",
    "#         self.rnns = None\n",
    "#         self.audio_sz = audio_sz\n",
    "#         self.multimode = [nn.LSTM(emb_sz + audio_sz if l == 0 else n_hid,\n",
    "#                                   (n_hid if l != n_layers - 1 else emb_sz + audio_sz)//self.ndir,\n",
    "#                                   1, bidirectional=bidir) for l in range(n_layers)]\n",
    "#         self.multimode = [WeightDropout(rnn, weight_p) for rnn in self.multimode]\n",
    "#         self.multimode = torch.nn.ModuleList(self.multimode)\n",
    "        \n",
    "#     def forward(self, input:LongTensor, input_audio:Tensor)->Tuple[Tensor,Tensor]:\n",
    "#         sl,bs = input.size()\n",
    "#         if bs!=self.bs:\n",
    "#             self.bs=bs\n",
    "#             self.reset()\n",
    "#         raw_output = self.input_dp(self.encoder_dp(input))\n",
    "#         raw_output = torch.cat([raw_output, input_audio], dim=2)\n",
    "#         new_hidden,raw_outputs,outputs = [],[],[]\n",
    "#         for l, (rnn,hid_dp) in enumerate(zip(self.multimode, self.hidden_dps)):\n",
    "#             raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "#             new_hidden.append(new_h)\n",
    "#             raw_outputs.append(raw_output)\n",
    "#             if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n",
    "#             outputs.append(raw_output)\n",
    "#         self.hidden = to_detach(new_hidden)\n",
    "#         return raw_outputs, outputs\n",
    "    \n",
    "#     def _one_hidden(self, l:int)->Tensor:\n",
    "#         \"Return one hidden state.\"\n",
    "#         nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz + self.audio_sz)//self.ndir\n",
    "#         return self.weights.new(self.ndir, self.bs, nh).zero_()\n",
    "\n",
    "#     def reset(self):\n",
    "#         \"Reset the hidden states.\"\n",
    "#         [r.reset() for r in self.multimode if hasattr(r, 'reset')]\n",
    "#         self.weights = next(self.parameters()).data\n",
    "#         if self.qrnn: self.hidden = [self._one_hidden(l) for l in range(self.n_layers)]\n",
    "#         else: self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]\n",
    "    \n",
    "# multimodal_rnn = MultiModalRNN(audio_sz=audio_sz,\n",
    "#                          vocab_sz=vocab_sz,\n",
    "#                          emb_sz=emb_sz,\n",
    "#                          n_hid=n_hid,\n",
    "#                          n_layers=n_layers,\n",
    "#                          pad_token=pad_token,\n",
    "#                          qrnn=qrnn,\n",
    "#                          bidir=bidir,\n",
    "#                          hidden_p=hidden_p,\n",
    "#                          input_p=input_p,\n",
    "#                          embed_p=embed_p,\n",
    "#                          weight_p=weight_p)\n",
    "\n",
    "# enc = multimodal_rnn.encoder if tie_weights else None\n",
    "# model = SequentialRNN(multimodal_rnn,\n",
    "#                       LinearDecoder(vocab_sz,\n",
    "#                                     emb_sz + audio_sz,\n",
    "#                                     output_p,\n",
    "#                                     tie_encoder=enc,\n",
    "#                                     bias=bias)).to(device)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = multimodal_rnn(x, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out[0][2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far, I'm able to get multimodal_rnn to work, but it doesnt work when used with SequentialRNN. Pretty sure this is because `forward` is not registered properly with SequentialRNN.\n",
    "\n",
    "Below, I attempt to include the decoder directly into the custom module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiModalRNN(\n",
       "  (encoder): Embedding(20000, 400, padding_idx=1)\n",
       "  (encoder_dp): EmbeddingDropout(\n",
       "    (emb): Embedding(20000, 400, padding_idx=1)\n",
       "  )\n",
       "  (rnns): None\n",
       "  (input_dp): RNNDropout()\n",
       "  (hidden_dps): ModuleList(\n",
       "    (0): RNNDropout()\n",
       "    (1): RNNDropout()\n",
       "    (2): RNNDropout()\n",
       "  )\n",
       "  (multimode): ModuleList(\n",
       "    (0): WeightDropout(\n",
       "      (module): LSTM(410, 1150)\n",
       "    )\n",
       "    (1): WeightDropout(\n",
       "      (module): LSTM(1150, 1150)\n",
       "    )\n",
       "    (2): WeightDropout(\n",
       "      (module): LSTM(1150, 410)\n",
       "    )\n",
       "  )\n",
       "  (multidecoder): LinearDecoder(\n",
       "    (decoder): Linear(in_features=410, out_features=20000, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiModalRNN(RNNCore):\n",
    "    def __init__(self, audio_sz, output_p, bias, **kwargs):\n",
    "        super(MultiModalRNN, self).__init__(**kwargs)\n",
    "        self.rnns = None\n",
    "        self.audio_sz = audio_sz\n",
    "        self.multimode = [nn.LSTM(emb_sz + audio_sz if l == 0 else n_hid,\n",
    "                                  (n_hid if l != n_layers - 1 else emb_sz + audio_sz)//self.ndir,\n",
    "                                  1, bidirectional=bidir) for l in range(n_layers)]\n",
    "        self.multimode = [WeightDropout(rnn, weight_p) for rnn in self.multimode]\n",
    "        self.multimode = torch.nn.ModuleList(self.multimode)\n",
    "        \n",
    "        self.multidecoder = LinearDecoder(vocab_sz,\n",
    "                                          emb_sz + audio_sz,\n",
    "                                          output_p,\n",
    "                                          tie_encoder=None,\n",
    "                                          bias=bias)\n",
    "        \n",
    "    def forward(self, input:LongTensor, input_audio:Tensor)->Tuple[Tensor,Tensor,Tensor]:\n",
    "        sl,bs = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        raw_output = self.input_dp(self.encoder_dp(input))\n",
    "        raw_output = torch.cat([raw_output, input_audio], dim=2)\n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.multimode, self.hidden_dps)):\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output)\n",
    "        self.hidden = to_detach(new_hidden)\n",
    "        \n",
    "        output = self.multidecoder.output_dp(outputs[-1])\n",
    "        decoded = self.multidecoder.decoder(output.view(output.size(0)*output.size(1),\n",
    "                                                        output.size(2)))\n",
    "        \n",
    "        return decoded, raw_outputs, outputs\n",
    "    \n",
    "    def _one_hidden(self, l:int)->Tensor:\n",
    "        \"Return one hidden state.\"\n",
    "        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz + self.audio_sz)//self.ndir\n",
    "        return self.weights.new(self.ndir, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states.\"\n",
    "        [r.reset() for r in self.multimode if hasattr(r, 'reset')]\n",
    "        self.weights = next(self.parameters()).data\n",
    "        if self.qrnn: self.hidden = [self._one_hidden(l) for l in range(self.n_layers)]\n",
    "        else: self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]\n",
    "    \n",
    "multimodal_rnn = MultiModalRNN(audio_sz=audio_sz,\n",
    "                              vocab_sz=vocab_sz,\n",
    "                              emb_sz=emb_sz,\n",
    "                              n_hid=n_hid,\n",
    "                              n_layers=n_layers,\n",
    "                              pad_token=pad_token,\n",
    "                              qrnn=qrnn,\n",
    "                              bidir=bidir,\n",
    "                              hidden_p=hidden_p,\n",
    "                              input_p=input_p,\n",
    "                              embed_p=embed_p,\n",
    "                              weight_p=weight_p,\n",
    "                              output_p=output_p,\n",
    "                              bias=bias).to(device)\n",
    "\n",
    "multimodal_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = multimodal_rnn(x, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4480, 20000])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should be able to train the data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9.90432357788086\n",
      "1 9.89766788482666\n",
      "2 9.891155242919922\n",
      "3 9.88436222076416\n",
      "4 9.87718677520752\n"
     ]
    }
   ],
   "source": [
    "multimodal_rnn.train()\n",
    "multimode\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# opt_params = multimodal_rnn.parameters()\n",
    "\n",
    "# this is a hack for now... not sure if this messes up the graph somewhere by doing this\n",
    "opt_params = [par for par in multimodal_rnn.parameters() if par.is_leaf]\n",
    "optimizer = torch.optim.Adam(opt_params, lr=learning_rate)\n",
    "\n",
    "y_pred = multimodal_rnn(x, z)\n",
    "\n",
    "for t in range(5):\n",
    "    y_pred = multimodal_rnn(x, z)[0]\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we get an error for those that are non-leaf nodes (tensors). Doing some investigating shows those that have a `grad_fn` are the culprits. These are most likely the tensors that are dropouts. Need to look at fastai and see how they handle these tensors...\n",
    "\n",
    "For now I used the code above to subset to `is_leaf == False`. I'm not sure if this \"is leaf\" is a symptom of something that's not working right or if this is, in fact, what we should do.\n",
    "\n",
    "In the original `RNNCore` class I'm a bit confused by what is happening for `self.rnns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0392, -0.0167,  0.0536,  ..., -0.0786,  0.0768, -0.0682],\n",
       "        [ 0.0976,  0.0382,  0.0584,  ...,  0.0168, -0.0589,  0.0689],\n",
       "        [-0.0717, -0.0809, -0.0149,  ...,  0.0354, -0.0986, -0.0407],\n",
       "        ...,\n",
       "        [-0.0886,  0.0420, -0.0151,  ..., -0.0790,  0.0397,  0.0285],\n",
       "        [ 0.0292,  0.0304,  0.0135,  ...,  0.0153, -0.0956, -0.0052],\n",
       "        [-0.0429, -0.0725, -0.0067,  ...,  0.0879,  0.0801,  0.0359]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = list(multimodal_rnn.parameters())[0]\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([20000, 400]) True True\n",
      "1 torch.Size([4600, 1150]) True True\n",
      "2 torch.Size([4600, 410]) True True\n",
      "3 torch.Size([4600, 1150]) False True\n",
      "4 torch.Size([4600]) True True\n",
      "5 torch.Size([4600]) True True\n",
      "6 torch.Size([4600, 1150]) True True\n",
      "7 torch.Size([4600, 1150]) True True\n",
      "8 torch.Size([4600, 1150]) False True\n",
      "9 torch.Size([4600]) True True\n",
      "10 torch.Size([4600]) True True\n",
      "11 torch.Size([1640, 410]) True True\n",
      "12 torch.Size([1640, 1150]) True True\n",
      "13 torch.Size([1640, 410]) False True\n",
      "14 torch.Size([1640]) True True\n",
      "15 torch.Size([1640]) True True\n",
      "16 torch.Size([20000, 410]) True True\n",
      "17 torch.Size([20000]) True True\n"
     ]
    }
   ],
   "source": [
    "for idx, thing in enumerate(multimodal_rnn.parameters()):\n",
    "    print(idx, thing.shape, thing.is_leaf, thing.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0504, -0.0437, -0.0204,  ...,  0.0562,  0.0344, -0.0210],\n",
       "        [ 0.0370,  0.0000, -0.0000,  ..., -0.0072,  0.0238,  0.0026],\n",
       "        [-0.0064,  0.0480, -0.0527,  ..., -0.0084,  0.0398, -0.0314],\n",
       "        ...,\n",
       "        [ 0.0469, -0.0468, -0.0339,  ...,  0.0566, -0.0047,  0.0132],\n",
       "        [ 0.0168,  0.0000, -0.0549,  ...,  0.0170,  0.0000,  0.0153],\n",
       "        [-0.0044,  0.0383, -0.0571,  ...,  0.0059, -0.0031,  0.0284]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = list(multimodal_rnn.parameters())[13]\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1640, 410])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multimodal_rnn.multimode.state_dict()['2.module.weight_hh_l0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder.weight': torch.Size([20000, 400]),\n",
       " 'encoder_dp.emb.weight': torch.Size([20000, 400]),\n",
       " 'multidecoder.decoder.bias': torch.Size([20000]),\n",
       " 'multidecoder.decoder.weight': torch.Size([20000, 410]),\n",
       " 'multimode.0.module.bias_hh_l0': torch.Size([4600]),\n",
       " 'multimode.0.module.bias_ih_l0': torch.Size([4600]),\n",
       " 'multimode.0.module.weight_hh_l0': torch.Size([4600, 1150]),\n",
       " 'multimode.0.module.weight_ih_l0': torch.Size([4600, 410]),\n",
       " 'multimode.0.weight_hh_l0_raw': torch.Size([4600, 1150]),\n",
       " 'multimode.1.module.bias_hh_l0': torch.Size([4600]),\n",
       " 'multimode.1.module.bias_ih_l0': torch.Size([4600]),\n",
       " 'multimode.1.module.weight_hh_l0': torch.Size([4600, 1150]),\n",
       " 'multimode.1.module.weight_ih_l0': torch.Size([4600, 1150]),\n",
       " 'multimode.1.weight_hh_l0_raw': torch.Size([4600, 1150]),\n",
       " 'multimode.2.module.bias_hh_l0': torch.Size([1640]),\n",
       " 'multimode.2.module.bias_ih_l0': torch.Size([1640]),\n",
       " 'multimode.2.module.weight_hh_l0': torch.Size([1640, 410]),\n",
       " 'multimode.2.module.weight_ih_l0': torch.Size([1640, 1150]),\n",
       " 'multimode.2.weight_hh_l0_raw': torch.Size([1640, 410])}"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multimodal_rnn.state_dict()\n",
    "{k:v.shape for (k,v) in multimodal_rnn.state_dict().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0504, -0.0437, -0.0204,  ...,  0.0562,  0.0344, -0.0210],\n",
       "        [ 0.0370,  0.0000, -0.0000,  ..., -0.0072,  0.0238,  0.0026],\n",
       "        [-0.0064,  0.0480, -0.0527,  ..., -0.0084,  0.0398, -0.0314],\n",
       "        ...,\n",
       "        [ 0.0469, -0.0468, -0.0339,  ...,  0.0566, -0.0047,  0.0132],\n",
       "        [ 0.0168,  0.0000, -0.0549,  ...,  0.0170,  0.0000,  0.0153],\n",
       "        [-0.0044,  0.0383, -0.0571,  ...,  0.0059, -0.0031,  0.0284]])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multimodal_rnn.state_dict()['multimode.2.module.weight_hh_l0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
