{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import utils, vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_DUMMY_DATA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_DUMMY_DATA:\n",
    "    nltk.download('brown') #sample corpus from nltk\n",
    "    corpus_object = nltk.corpus.brown\n",
    "    words = corpus_object.words() #singe list of words ['Friday','an','investigation','of',\"Atlanta's\",...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = pd.read_csv('../data/external/songdata.csv', usecols=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Look at her face, it's a wonderful face  \\nAnd...\n",
       "1  Take it easy with me, please  \\nTouch me gentl...\n",
       "2  I'll never know why I had to go  \\nWhy I had t...\n",
       "3  Making somebody happy is a question of give an...\n",
       "4  Making somebody happy is a question of give an..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = lyrics.text.str.cat()\n",
    "words = full_text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_full = pd.read_csv('../data/external/lyrics.csv', usecols=['lyrics'])\n",
    "lyrics_full.columns = ['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Oh baby, how you doing?\\nYou know I'm gonna cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>playin' everything so easy,\\nit's like you see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you search\\nFor tenderness\\nIt isn't hard t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oh oh oh I, oh oh oh I\\n[Verse 1:]\\nIf I wrote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Party the people, the people the party it's po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Oh baby, how you doing?\\nYou know I'm gonna cu...\n",
       "1  playin' everything so easy,\\nit's like you see...\n",
       "2  If you search\\nFor tenderness\\nIt isn't hard t...\n",
       "3  Oh oh oh I, oh oh oh I\\n[Verse 1:]\\nIf I wrote...\n",
       "4  Party the people, the people the party it's po..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = lyrics.text.str.cat()\n",
    "words = full_text.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load into vocab object for exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 246,660\n",
      "Most common unigrams:\n",
      "\"\": 2,254,106\n",
      "\"the\": 448,461\n",
      "\"you\": 329,508\n",
      "\"\n",
      "\": 298,900\n",
      "\"to\": 273,355\n",
      "\"i\": 257,440\n",
      "\"a\": 236,144\n",
      "\"me\": 172,838\n",
      "\"and\": 163,873\n",
      "\"\n",
      "i\": 154,413\n"
     ]
    }
   ],
   "source": [
    "# \"canonicalize_word\" performs a few tweaks to the token stream of\n",
    "# the corpus.  For example, it replaces digits with DG allowing numbers\n",
    "# to aggregate together when we count them below.\n",
    "# You can read the details in utils.py if you're really curious.\n",
    "token_feed = (utils.canonicalize_word(w) for w in words)\n",
    "\n",
    "# Collect counts of tokens and assign wordids.\n",
    "vocab = vocabulary.Vocabulary(token_feed)\n",
    "print(\"Vocabulary size: {:,}\".format(vocab.size))\n",
    "\n",
    "# Print out some (debugging) statistics to make sure everything went\n",
    "# as we expected.  (Unsurprisingly, you should see \"the\" as the most popular word.)\n",
    "print(\"Most common unigrams:\")\n",
    "for word, count in vocab.unigram_counts.most_common(10):\n",
    "    print(\"\\\"{:s}\\\": {:,}\".format(word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "def normalize_counter(c):\n",
    "    \"\"\"Given a dictionary of <item, counts>, return <item, fraction>.\"\"\"\n",
    "    total = sum(c.values())\n",
    "    return {w:float(c[w])/total for w in c}\n",
    "\n",
    "\n",
    "class SimpleTrigramLM(object):\n",
    "    def __init__(self, words, probas_file=None):\n",
    "        \"\"\"Build our simple trigram model.\"\"\"\n",
    "        #if pre-defined model is provided, use that as probabilities\n",
    "        if probas_file:\n",
    "            with open('{}.pkl'.format(file_name), 'rb') as main_dict:\n",
    "                self.probas = pickle.load(main_dict)\n",
    "        \n",
    "        else:\n",
    "            # Raw trigram counts over the corpus. \n",
    "            # c(w | w_1 w_2) = self.counts[(w_2,w_1)][w]\n",
    "            self.counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "\n",
    "            # Iterate through the word stream once.\n",
    "            w_1, w_2 = None, None\n",
    "            for word in words:\n",
    "                if w_1 is not None and w_2 is not None:\n",
    "                    # Increment trigram count.\n",
    "                    self.counts[(w_2,w_1)][word] += 1\n",
    "                # Shift context along the stream of words.\n",
    "                w_2 = w_1\n",
    "                w_1 = word\n",
    "            \n",
    "            # Normalize so that for each context we have a valid probability\n",
    "            # distribution (i.e. adds up to 1.0) of possible next tokens.\n",
    "            self.probas = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "            for context, ctr in self.counts.items():\n",
    "                self.probas[context] = normalize_counter(ctr)\n",
    "            \n",
    "    def next_word_proba(self, word, seq):\n",
    "        \"\"\"Compute p(word | seq)\"\"\"\n",
    "        context = tuple(seq[-2:])  # last two words\n",
    "        return self.probas[context].get(word, 0.0)\n",
    "    \n",
    "    def predict_next(self, seq):\n",
    "        \"\"\"Sample a word from the conditional distribution.\"\"\"\n",
    "        context = tuple(seq[-2:])  # last two words\n",
    "        pc = self.probas[context]  # conditional distribution\n",
    "        words, probs = zip(*pc.items())  # convert to list\n",
    "        return np.random.choice(words, p=probs)\n",
    "    \n",
    "    def score_seq(self, seq, verbose=False):\n",
    "        \"\"\"Compute log probability (base 2) of the given sequence.\"\"\"\n",
    "        score = 0.0\n",
    "        count = 0\n",
    "        # Start at third word, since we need a full context.\n",
    "        for i in range(2, len(seq)):\n",
    "            if (seq[i] == \"<s>\" or seq[i] == \"</s>\"):\n",
    "                continue  # Don't count special tokens in score.\n",
    "            s = np.log2(self.next_word_proba(seq[i], seq[i-2:i]))\n",
    "            score += s\n",
    "            count += 1\n",
    "            # DEBUG\n",
    "            if verbose:\n",
    "                print(\"log P({:s} | {:s}) = {.03f}\".format(seq[i], \" \".join(seq[i-2:i]), s))\n",
    "        return score, count\n",
    "    \n",
    "    def generate_text(self, max_length=40):\n",
    "        seq = [\"<s>\", \"<s>\"]\n",
    "        for i in range(max_length):\n",
    "            seq.append(self.predict_next(seq))\n",
    "            # Stop at end-of-sentence\n",
    "            if seq[-1] == \"</s>\": break\n",
    "        print(\" \".join(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(list(corpus_object.sents())) = array([list(['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'primary', 'election', 'produced', '``', 'no', 'evidence', '.']),\n",
    "                                        #        list(['The', 'jury', 'further', 'said', 'in', 'term-end', 'was', 'conducted', '.']))],\n",
    "                                        #        list([])...,\n",
    "                                        #    dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 57,650 sentences (1.52059e+07 tokens)\n",
      "Training set: 46,120 sentences (12,159,307 tokens)\n",
      "Test set: 11,530 sentences (3,046,572 tokens)\n"
     ]
    }
   ],
   "source": [
    "train_sents, test_sents = utils.get_train_test_sents(lyrics.text, split=0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set vocabulary: 190324 words\n"
     ]
    }
   ],
   "source": [
    "vocab = vocabulary.Vocabulary(utils.canonicalize_word(w) for w in utils.flatten(train_sents))\n",
    "print(\"Train set vocabulary: %d words\" % vocab.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sents_to_tokens(sents):\n",
    "    \"\"\"Returns a flattened list of the words in the sentences, with padding for a trigram model.\"\"\"\n",
    "    padded_sentences = ([\"<s>\", \"<s>\"] + s + [\"</s>\"] for s in sents)\n",
    "    # This will canonicalize words, and replace anything not in vocab with <unk>\n",
    "    return np.array([utils.canonicalize_word(w, wordset=vocab.wordset) \n",
    "                     for w in utils.flatten(padded_sentences)], dtype=object)\n",
    "\n",
    "train_tokens = sents_to_tokens(train_sents)\n",
    "#test_tokens = sents_to_tokens(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building trigram LM...\n",
      "Built trigram LM...\n"
     ]
    }
   ],
   "source": [
    "print(\"Building trigram LM...\",)\n",
    "lm = SimpleTrigramLM(train_tokens)\n",
    "print(\"Built trigram LM...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save trained probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='../data/models/trigram-weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}.pkl'.format(file_name), 'wb') as outfile:\n",
    "    pickle.dump(dict(lm.probas), outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in pre-trained probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}.pkl'.format(file_name), 'rb') as main_dict:\n",
    "    lm.probas = pickle.load(main_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building trigram LM...\n",
      "Built trigram LM...\n"
     ]
    }
   ],
   "source": [
    "print(\"Building trigram LM...\",)\n",
    "lm = SimpleTrigramLM(words=None, probas_file='../data/models/trigram-weights')\n",
    "print(\"Built trigram LM...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> there's a light where  \n",
      "he takes after each pause]  \n",
      "q.b., real niggas, bravehearts, c'mon  \n",
      "c'mon baby, cry baby)  \n",
      "oh black and white seed  \n",
      "  \n",
      "somebody bigger  \n",
      "anything you have to go  \n",
      "don't make waves don't make me see.  \n",
      "driving school,  \n",
      "his breath had stopped  \n",
      "for i will say we're gonna miss me  \n",
      "i sing, i've been waiting for a while, a while  \n",
      "you in white  \n",
      "well baby don't ya love lockdown  \n",
      "now they're draggin' her feet  \n",
      "eating from silver blue jewel  \n",
      "lord, they're coming to your way,  \n",
      "i'll take you through the sphere upon which i stand  \n",
      "  \n",
      "i can't say the words of guilt she picks on me  \n",
      "fingers down the highway  \n",
      "the stumble in the hands of the year  \n",
      "let's start a family that's gathered in the ice still on my knees  \n",
      "now's the time has come now in your face  \n",
      "it's addressed to your heart love)?  \n",
      "girl, you'll be there either way  \n",
      "  \n",
      "let's hear the choo-choo train that makes the world wasn't glowing  \n",
      "why is life this good for?  \n",
      "well this crazy world (crazy world)  \n",
      "we had it all again  \n",
      "  \n",
      "strange, strange, strange  \n",
      "but never really really near  \n",
      "and you're the one to believe in lovin' on my way  \n",
      "oh, i'll get along.  \n",
      "\n",
      " </s>\n"
     ]
    }
   ],
   "source": [
    "lm.generate_text(max_length=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
