{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.tokenize\n",
    "import itertools\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msd_id</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAADJU128F92F58E1</td>\n",
       "      <td>I hear you praying with your hands clasped ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAADQX128F422B4CF</td>\n",
       "      <td>If you ever make it back to Nashville\\nRemembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAAFTE128F429545F</td>\n",
       "      <td>Just when I thought I was safe\\nYou found me i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAAKAG128F4275D2A</td>\n",
       "      <td>Paroles de la chanson Sultao Das Matas :\\nSult...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAAMRO128F92F20D7</td>\n",
       "      <td>From What You Whispered\\n........................</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               msd_id                                             lyrics\n",
       "0  TRAADJU128F92F58E1  I hear you praying with your hands clasped ove...\n",
       "1  TRAADQX128F422B4CF  If you ever make it back to Nashville\\nRemembe...\n",
       "2  TRAAFTE128F429545F  Just when I thought I was safe\\nYou found me i...\n",
       "3  TRAAKAG128F4275D2A  Paroles de la chanson Sultao Das Matas :\\nSult...\n",
       "4  TRAAMRO128F92F20D7  From What You Whispered\\n........................"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://storage.googleapis.com/w210-capstone/data/lyrics-valid.csv', header=None, escapechar='\\\\', names=['msd_id', 'lyrics'])\n",
    "# drop lyrics >5000\n",
    "df = df[df.lyrics.str.len() < 5000]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "1. First consider each line its own \"sentence\", keeping track of blanklines\n",
    "2. Regexp Tokenizer with the following:  \n",
    " - Bracket enclosed texts (usually song part header)\n",
    " - All words\n",
    " - Any numeric -- keep commas and periods together\n",
    " - All other non-whitespace characters\n",
    "3. Wrap each line with `<s>` and `</s>` tokens\n",
    "4. Wrap each song with `<d>` and `</d>` tokens (documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_lyrics(lyrics):\n",
    "    tk = nltk.tokenize.LineTokenizer(blanklines='keep')\n",
    "    tokd = tk.tokenize(lyrics)\n",
    "    \n",
    "    re_tk = nltk.tokenize.RegexpTokenizer(r'\\[[^\\]]+\\]|\\w+|[\\d\\.,]+|\\S+',\n",
    "                                          discard_empty=False)\n",
    "    re_tokd = re_tk.tokenize_sents(tokd)\n",
    "    \n",
    "    [s.insert(0, f'xBOL') for s in re_tokd] # insert start token for each line\n",
    "    [s.append('xEOL') for s in re_tokd] # append end token for each line\n",
    "    \n",
    "    flat = list(itertools.chain(*re_tokd))\n",
    "    flat.insert(0, 'xBOS')\n",
    "    flat.append('xEOS')\n",
    "    # lower case and de-space\n",
    "    flat = [w.lower().replace(' ', '-') for w in flat]\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msd_id</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>tokd</th>\n",
       "      <th>tokd_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAADJU128F92F58E1</td>\n",
       "      <td>I hear you praying with your hands clasped ove...</td>\n",
       "      <td>[xbos, xbol, i, hear, you, praying, with, your...</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAADQX128F422B4CF</td>\n",
       "      <td>If you ever make it back to Nashville\\nRemembe...</td>\n",
       "      <td>[xbos, xbol, if, you, ever, make, it, back, to...</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAAFTE128F429545F</td>\n",
       "      <td>Just when I thought I was safe\\nYou found me i...</td>\n",
       "      <td>[xbos, xbol, just, when, i, thought, i, was, s...</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAAKAG128F4275D2A</td>\n",
       "      <td>Paroles de la chanson Sultao Das Matas :\\nSult...</td>\n",
       "      <td>[xbos, xbol, paroles, de, la, chanson, sultao,...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAAMRO128F92F20D7</td>\n",
       "      <td>From What You Whispered\\n........................</td>\n",
       "      <td>[xbos, xbol, from, what, you, whispered, xeol,...</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               msd_id                                             lyrics  \\\n",
       "0  TRAADJU128F92F58E1  I hear you praying with your hands clasped ove...   \n",
       "1  TRAADQX128F422B4CF  If you ever make it back to Nashville\\nRemembe...   \n",
       "2  TRAAFTE128F429545F  Just when I thought I was safe\\nYou found me i...   \n",
       "3  TRAAKAG128F4275D2A  Paroles de la chanson Sultao Das Matas :\\nSult...   \n",
       "4  TRAAMRO128F92F20D7  From What You Whispered\\n........................   \n",
       "\n",
       "                                                tokd  tokd_len  \n",
       "0  [xbos, xbol, i, hear, you, praying, with, your...       215  \n",
       "1  [xbos, xbol, if, you, ever, make, it, back, to...       196  \n",
       "2  [xbos, xbol, just, when, i, thought, i, was, s...       186  \n",
       "3  [xbos, xbol, paroles, de, la, chanson, sultao,...        58  \n",
       "4  [xbos, xbol, from, what, you, whispered, xeol,...       310  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokd'] = df.lyrics.apply(tokenize_lyrics)\n",
    "df['tokd_len'] = df.tokd.apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, just save both as both train and valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model file prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_title = '2.0-ULMFiT'\n",
    "MODEL_PATH = Path(f'../data/models/{model_title}')\n",
    "MODEL_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "TOKEN_PATH = MODEL_PATH/'tokens'\n",
    "TOKEN_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_TOKENS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_TOKENS:\n",
    "    tokens = np.array(df.tokd)\n",
    "    \n",
    "    np.save(MODEL_PATH/'train_tok.npy', tokens)\n",
    "    np.save(MODEL_PATH/'valid_tok.npy', tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ULMFiT Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_WIKITEXT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4097b3e9b5c348c199c87b53308c773d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=221972701), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf60a89bf1ef4dfa8460526bd8b8c9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1027972), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if DOWNLOAD_WIKITEXT:\n",
    "    url = 'http://files.fast.ai/models/wt103_v1/'\n",
    "    download_url(f'{url}lstm_wt103.pth', MODEL_PATH/'models/lstm_wt103.pth')\n",
    "    download_url(f'{url}itos_wt103.pkl', MODEL_PATH/'models/itos_wt103.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numericalizing train.\n",
      "Numericalizing valid.\n",
      "10002\n"
     ]
    }
   ],
   "source": [
    "data_lm = TextLMDataBunch.from_tokens(MODEL_PATH,\n",
    "                                      bs=128,\n",
    "                                      max_vocab=10000)\n",
    "\n",
    "print(data_lm.train_ds.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([95, 128]) torch.Size([12160])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xbos</td>\n",
       "      <td>xeol</td>\n",
       "      <td>'s</td>\n",
       "      <td>[outro]</td>\n",
       "      <td>will</td>\n",
       "      <td>xeol</td>\n",
       "      <td>so</td>\n",
       "      <td>xbol</td>\n",
       "      <td>won</td>\n",
       "      <td>my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xbol</td>\n",
       "      <td>xbol</td>\n",
       "      <td>sad</td>\n",
       "      <td>xeol</td>\n",
       "      <td>destroy</td>\n",
       "      <td>xbol</td>\n",
       "      <td>i</td>\n",
       "      <td>i</td>\n",
       "      <td>'t</td>\n",
       "      <td>soul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i</td>\n",
       "      <td>i</td>\n",
       "      <td>but</td>\n",
       "      <td>xbol</td>\n",
       "      <td>her</td>\n",
       "      <td>i</td>\n",
       "      <td>fight</td>\n",
       "      <td>said</td>\n",
       "      <td>deal</td>\n",
       "      <td>xeol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hear</td>\n",
       "      <td>'ll</td>\n",
       "      <td>you</td>\n",
       "      <td>i</td>\n",
       "      <td>xeol</td>\n",
       "      <td>'ve</td>\n",
       "      <td>xeol</td>\n",
       "      <td>,</td>\n",
       "      <td>xeol</td>\n",
       "      <td>xeos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you</td>\n",
       "      <td>do</td>\n",
       "      <td>seem</td>\n",
       "      <td>'ll</td>\n",
       "      <td>xbol</td>\n",
       "      <td>been</td>\n",
       "      <td>xbol</td>\n",
       "      <td>it</td>\n",
       "      <td>xbol</td>\n",
       "      <td>xbos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>praying</td>\n",
       "      <td>anything</td>\n",
       "      <td>better</td>\n",
       "      <td>never</td>\n",
       "      <td>so</td>\n",
       "      <td>takin</td>\n",
       "      <td>i</td>\n",
       "      <td>'s</td>\n",
       "      <td>i</td>\n",
       "      <td>xbol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>with</td>\n",
       "      <td>for</td>\n",
       "      <td>when</td>\n",
       "      <td>know</td>\n",
       "      <td>i</td>\n",
       "      <td>'</td>\n",
       "      <td>cannot</td>\n",
       "      <td>my</td>\n",
       "      <td>won</td>\n",
       "      <td>xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>your</td>\n",
       "      <td>you</td>\n",
       "      <td>i</td>\n",
       "      <td>xeol</td>\n",
       "      <td>'m</td>\n",
       "      <td>all</td>\n",
       "      <td>shake</td>\n",
       "      <td>life</td>\n",
       "      <td>'t</td>\n",
       "      <td>xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hands</td>\n",
       "      <td>,</td>\n",
       "      <td>'m</td>\n",
       "      <td>xbol</td>\n",
       "      <td>giving</td>\n",
       "      <td>the</td>\n",
       "      <td>from</td>\n",
       "      <td>xeol</td>\n",
       "      <td>change</td>\n",
       "      <td>em</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>xxunk</td>\n",
       "      <td>show</td>\n",
       "      <td>gone</td>\n",
       "      <td>i</td>\n",
       "      <td>up</td>\n",
       "      <td>blame</td>\n",
       "      <td>my</td>\n",
       "      <td>xbol</td>\n",
       "      <td>,</td>\n",
       "      <td>busca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>over</td>\n",
       "      <td>me</td>\n",
       "      <td>xeol</td>\n",
       "      <td>'ll</td>\n",
       "      <td>,</td>\n",
       "      <td>but</td>\n",
       "      <td>brain</td>\n",
       "      <td>you</td>\n",
       "      <td>i</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>your</td>\n",
       "      <td>xeol</td>\n",
       "      <td>xbol</td>\n",
       "      <td>never</td>\n",
       "      <td>giving</td>\n",
       "      <td>xeol</td>\n",
       "      <td>what</td>\n",
       "      <td>ain</td>\n",
       "      <td>'ll</td>\n",
       "      <td>xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>chest</td>\n",
       "      <td>xbol</td>\n",
       "      <td>xeol</td>\n",
       "      <td>know</td>\n",
       "      <td>up</td>\n",
       "      <td>xbol</td>\n",
       "      <td>i</td>\n",
       "      <td>'t</td>\n",
       "      <td>tough</td>\n",
       "      <td>xeol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>xeol</td>\n",
       "      <td>xeol</td>\n",
       "      <td>xbol</td>\n",
       "      <td>xeol</td>\n",
       "      <td>xeol</td>\n",
       "      <td>i</td>\n",
       "      <td>saw</td>\n",
       "      <td>gonna</td>\n",
       "      <td>it</td>\n",
       "      <td>xbol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>xbol</td>\n",
       "      <td>xbol</td>\n",
       "      <td>and</td>\n",
       "      <td>xeos</td>\n",
       "      <td>xbol</td>\n",
       "      <td>'ll</td>\n",
       "      <td>in</td>\n",
       "      <td>change</td>\n",
       "      <td>out</td>\n",
       "      <td>xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>i</td>\n",
       "      <td>oh</td>\n",
       "      <td>all</td>\n",
       "      <td>xbos</td>\n",
       "      <td>my</td>\n",
       "      <td>never</td>\n",
       "      <td>his</td>\n",
       "      <td>my</td>\n",
       "      <td>xeol</td>\n",
       "      <td>sua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hear</td>\n",
       "      <td>baby</td>\n",
       "      <td>of</td>\n",
       "      <td>xbol</td>\n",
       "      <td>pride</td>\n",
       "      <td>make</td>\n",
       "      <td>eyes</td>\n",
       "      <td>life</td>\n",
       "      <td>xbol</td>\n",
       "      <td>banda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>men</td>\n",
       "      <td>,</td>\n",
       "      <td>these</td>\n",
       "      <td>well</td>\n",
       "      <td>for</td>\n",
       "      <td>the</td>\n",
       "      <td>xeol</td>\n",
       "      <td>,</td>\n",
       "      <td>xxunk</td>\n",
       "      <td>pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>xxunk</td>\n",
       "      <td>come</td>\n",
       "      <td>words</td>\n",
       "      <td>,</td>\n",
       "      <td>you</td>\n",
       "      <td>same</td>\n",
       "      <td>xbol</td>\n",
       "      <td>oh</td>\n",
       "      <td>it</td>\n",
       "      <td>rio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>while</td>\n",
       "      <td>on</td>\n",
       "      <td>i</td>\n",
       "      <td>you</td>\n",
       "      <td>xeol</td>\n",
       "      <td>mistake</td>\n",
       "      <td>in</td>\n",
       "      <td>no</td>\n",
       "      <td>out</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1       2        3        4        5       6       7  \\\n",
       "0      xbos      xeol      's  [outro]     will     xeol      so    xbol   \n",
       "1      xbol      xbol     sad     xeol  destroy     xbol       i       i   \n",
       "2         i         i     but     xbol      her        i   fight    said   \n",
       "3      hear       'll     you        i     xeol      've    xeol       ,   \n",
       "4       you        do    seem      'll     xbol     been    xbol      it   \n",
       "5   praying  anything  better    never       so    takin       i      's   \n",
       "6      with       for    when     know        i        '  cannot      my   \n",
       "7      your       you       i     xeol       'm      all   shake    life   \n",
       "8     hands         ,      'm     xbol   giving      the    from    xeol   \n",
       "9     xxunk      show    gone        i       up    blame      my    xbol   \n",
       "10     over        me    xeol      'll        ,      but   brain     you   \n",
       "11     your      xeol    xbol    never   giving     xeol    what     ain   \n",
       "12    chest      xbol    xeol     know       up     xbol       i      't   \n",
       "13     xeol      xeol    xbol     xeol     xeol        i     saw   gonna   \n",
       "14     xbol      xbol     and     xeos     xbol      'll      in  change   \n",
       "15        i        oh     all     xbos       my    never     his      my   \n",
       "16     hear      baby      of     xbol    pride     make    eyes    life   \n",
       "17      men         ,   these     well      for      the    xeol       ,   \n",
       "18    xxunk      come   words        ,      you     same    xbol      oh   \n",
       "19    while        on       i      you     xeol  mistake      in      no   \n",
       "\n",
       "         8      9  \n",
       "0      won     my  \n",
       "1       't   soul  \n",
       "2     deal   xeol  \n",
       "3     xeol   xeos  \n",
       "4     xbol   xbos  \n",
       "5        i   xbol  \n",
       "6      won  xxunk  \n",
       "7       't  xxunk  \n",
       "8   change     em  \n",
       "9        ,  busca  \n",
       "10       i     de  \n",
       "11     'll  xxunk  \n",
       "12   tough   xeol  \n",
       "13      it   xbol  \n",
       "14     out  xxunk  \n",
       "15    xeol    sua  \n",
       "16    xbol  banda  \n",
       "17   xxunk    pro  \n",
       "18      it    rio  \n",
       "19     out     de  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = next(iter(data_lm.train_dl))\n",
    "example = x[:20,:10].cpu()\n",
    "texts = pd.DataFrame([data_lm.train_ds.vocab.textify(l).split(' ') for l in example])\n",
    "print(x.shape, y.shape)\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ULMFiT Model architecture and create and embedding matrix that includes the new words. The new words are initialized to the mean value of all prior vocab...\n",
    "\n",
    "TODO: maybe update the initialization points to the mean value of prior vocab that we keep in this model. e.g. average of the words that are in the lyrics corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): RNNCore(\n",
       "    (encoder): Embedding(10002, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(10002, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1150)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1150, 1150)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1150, 400)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=10002, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = RNNLearner.language_model(data_lm,\n",
    "                                  pretrained_fnames=['lstm_wt103', 'itos_wt103'],\n",
    "                                  drop_mult=0.5)\n",
    "\n",
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit one cycle, but keep all layers frozen except the linear encoder and decoder. Start with a realtively low learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b7892d3bf94d06baf7eb3d7c7b50db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, max=1), HTML(value='0.00% [0/1 00:00<00:00]'))), HTML(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, max=10), HTML(value='0.00% [0/10 00:00<00:00]'))), HTML(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 3:14:25\n",
      "epoch  train loss  valid loss  accuracy\n",
      "0      3.221238    3.089552    0.416633  (19:27)\n",
      "1      3.072464    2.944254    0.437782  (19:26)\n",
      "2      2.959016    2.841134    0.452242  (19:25)\n",
      "3      2.886021    2.753556    0.464477  (19:26)\n",
      "4      2.822960    2.687229    0.474104  (19:30)\n",
      "5      2.759549    2.625144    0.483686  (19:24)\n",
      "6      2.715732    2.583514    0.490242  (19:26)\n",
      "7      2.670489    2.546307    0.495953  (19:26)\n",
      "8      2.656668    2.527033    0.499007  (19:24)\n",
      "9      2.673330    2.524118    0.499475  (19:27)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'{model_title}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_load(self, name:PathOrStr):\n",
    "    \"Load model onto CPU that was trained on a GPU `name` from `self.model_dir`.\"\n",
    "    self.model.load_state_dict(torch.load(self.path/self.model_dir/f'{name}.pth', map_location=lambda storage, loc: storage))\n",
    "\n",
    "setattr(RNNLearner, 'cpu_load', cpu_load) #monkey patch onto our RNNLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not GPU:\n",
    "    learn.cpu_load(f'{model_title}')\n",
    "else:\n",
    "    learn.load(f'{model_title}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(learner, seed_text=['xbos'], max_len=500, GPU=False):\n",
    "    \"\"\"Generates text with a given learner and prints string to console.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    learner : RNNLearner Language Model (RNNLearner.language_model())\n",
    "        Fastai RNNLearner with tokenized language model data already loaded \n",
    "        \n",
    "    seed_text : list\n",
    "        List of strings where each item is a token. (e.g. ['the', 'cat'])\n",
    "\n",
    "    max_len : int\n",
    "        Number of words in generated sequence\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None : NoneType\n",
    "        Doesn't return anything, prints string to console\n",
    "    \"\"\"\n",
    "        \n",
    "    model = learner.model\n",
    "    \n",
    "    if GPU:\n",
    "        context = LongTensor(data_lm.train_ds.vocab.numericalize(seed_text)).view(-1,1).cuda()\n",
    "    else:\n",
    "        context = LongTensor(data_lm.train_ds.vocab.numericalize(seed_text)).view(-1,1).cpu()\n",
    "    \n",
    "    context = torch.autograd.Variable(context)\n",
    "    \n",
    "    # reset model's hidden state\n",
    "    # we don't want to carry over old contexts\n",
    "    model.reset()\n",
    "    model.eval()\n",
    "\n",
    "    #loop over max length of words\n",
    "    for _ in range(max_len):\n",
    "        # forward pass the \"context\" into the model\n",
    "        result, *_ = model(context)\n",
    "        result = result[-1]\n",
    "\n",
    "        # set unk and pad to 0 prob\n",
    "        # i.e. never pick unknown or pad\n",
    "        result[0] = -np.inf\n",
    "        result[1] = -np.inf\n",
    "\n",
    "        # softmax\n",
    "        probabilities = F.softmax(result, dim=0)\n",
    "        probabilities = np.asarray(probabilities.detach().cpu(), dtype=np.float)\n",
    "        probabilities /= np.sum(probabilities) # solve rounding issues for multinom function\n",
    "\n",
    "        # draw multinom and add to context\n",
    "        token_index = np.argmax(np.random.multinomial(1, probabilities))\n",
    "        \n",
    "        if GPU:\n",
    "            token_index = LongTensor([token_index]).view(-1, 1).cuda()\n",
    "        else:\n",
    "            token_index = LongTensor([token_index]).view(-1, 1).cpu()\n",
    "\n",
    "        context = torch.cat((context, token_index))    \n",
    "        \n",
    "        # print word\n",
    "        word = data_lm.valid_ds.vocab.textify([token_index])\n",
    "\n",
    "        if word == 'xeol':\n",
    "            word = '\\n'\n",
    "        elif 'xbol' in word:\n",
    "             continue\n",
    "        elif word == 'xeos': \n",
    "            break\n",
    "            \n",
    "        print(word, end=' ')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(learn, GPU=GPU, seed_text=['xbos', 'xbol'], max_len=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49085233, -0.25270373, -0.18936859, ..., -0.10786957,\n",
       "        -0.15040247,  0.08028701],\n",
       "       [ 0.38108253,  0.46519557, -0.04423388, ..., -0.32877025,\n",
       "         0.544967  , -0.6305809 ],\n",
       "       [ 0.2315303 ,  0.01689303,  0.5893272 , ..., -0.02504184,\n",
       "        -0.28303975,  0.18284719],\n",
       "       ...,\n",
       "       [ 0.21922407,  0.32938698,  0.04211523, ..., -0.2696808 ,\n",
       "         1.0012609 , -0.22526287],\n",
       "       [ 0.69269246,  0.845887  , -0.0607295 , ..., -0.33333674,\n",
       "         1.2671981 , -0.8235619 ],\n",
       "       [ 0.54437697,  1.2663454 , -0.09743147, ..., -0.84379476,\n",
       "         1.0860044 , -0.31181222]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = learn.model.state_dict().get('0.encoder.weight').cpu().numpy()\n",
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embed = pd.DataFrame(data=embed,\n",
    "                        index=data_lm.train_ds.vocab.itos)\n",
    "\n",
    "df_embed.to_csv('../data/models/embeddings.csv',\n",
    "                sep='\\t',\n",
    "                index=False,\n",
    "                header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.DataFrame(data=data_lm.train_ds.vocab.itos,\n",
    "                       columns=['token'])\n",
    "\n",
    "df_meta.to_csv('../data/models/embeddings_meta.csv',\n",
    "               sep='\\t',\n",
    "               header=False,\n",
    "               index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Embedding Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire semantic and contextual representations that the model truly learns cannot be visualized quite to the mindblowing extent that might be warranted. In order to make this figure, we collapse 400 dimensions of \"information\" into 3 dimensions so that we can view it in 3 dimensional space. In the process of reducing our dimensionalitye, we lose ~X% of our data's variance. Even with this loss of interpretability, very clear semantic understandings of the language model emerge in the 3D space below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import manifold\n",
    "from sklearn.utils import check_random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-03bed2b51f7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanifold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pca'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrans_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m         \"\"\"\n\u001b[0;32m--> 858\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    768\u001b[0m                           \u001b[0mX_embedded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_embedded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m                           \u001b[0mneighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneighbors_nn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m                           skip_num_points=skip_num_points)\n\u001b[0m\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_tsne\u001b[0;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[1;32m    825\u001b[0m             \u001b[0mopt_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_iter_without_progress'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_without_progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m             params, kl_divergence, it = _gradient_descent(obj_func, params,\n\u001b[0;32m--> 827\u001b[0;31m                                                           **opt_args)\n\u001b[0m\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0;31m# Save the final number of iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_gradient_descent\u001b[0;34m(objective, p0, it, n_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mgrad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_kl_divergence_bh\u001b[0;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose)\u001b[0m\n\u001b[1;32m    245\u001b[0m     error = _barnes_hut_tsne.gradient(val_P, X_embedded, neighbors, indptr,\n\u001b[1;32m    246\u001b[0m                                       \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                                       dof=degrees_of_freedom)\n\u001b[0m\u001b[1;32m    248\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdegrees_of_freedom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdegrees_of_freedom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tsne = manifold.TSNE(n_components=3, init='pca', random_state=0)\n",
    "trans_data = tsne.fit_transform(embed).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = go.Scatter3d(x=trans_data[0], y=trans_data[1], z=trans_data[2],\n",
    "                  mode='markers', \n",
    "                  marker=dict(color=x, \n",
    "                              colorscale=cmap,\n",
    "                              showscale=False,\n",
    "                              line=dict(color='black', width=1)))\n",
    "\n",
    "layout=dict(margin=dict(l=10, r=10,\n",
    "                        t=30, b=10)\n",
    "           )\n",
    "\n",
    "fig = go.Figure(data=[p1], layout=layout)\n",
    "\n",
    "py.iplot(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
