{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.tokenize\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msd_id</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAADJU128F92F58E1</td>\n",
       "      <td>I hear you praying with your hands clasped ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAADQX128F422B4CF</td>\n",
       "      <td>If you ever make it back to Nashville\\nRemembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAAFTE128F429545F</td>\n",
       "      <td>Just when I thought I was safe\\nYou found me i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAAKAG128F4275D2A</td>\n",
       "      <td>Paroles de la chanson Sultao Das Matas :\\nSult...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAAMRO128F92F20D7</td>\n",
       "      <td>From What You Whispered\\n........................</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               msd_id                                             lyrics\n",
       "0  TRAADJU128F92F58E1  I hear you praying with your hands clasped ove...\n",
       "1  TRAADQX128F422B4CF  If you ever make it back to Nashville\\nRemembe...\n",
       "2  TRAAFTE128F429545F  Just when I thought I was safe\\nYou found me i...\n",
       "3  TRAAKAG128F4275D2A  Paroles de la chanson Sultao Das Matas :\\nSult...\n",
       "4  TRAAMRO128F92F20D7  From What You Whispered\\n........................"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://storage.googleapis.com/w210-capstone/data/lyrics-valid.csv', header=None, escapechar='\\\\', names=['msd_id', 'lyrics'])\n",
    "# drop lyrics >5000\n",
    "df = df[df.lyrics.str.len() < 5000]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "1. First consider each line its own \"sentence\", keeping track of blanklines\n",
    "2. Regexp Tokenizer with the following:  \n",
    " - Bracket enclosed texts (usually song part header)\n",
    " - All words\n",
    " - Any numeric -- keep commas and periods together\n",
    " - All other non-whitespace characters\n",
    "3. Wrap each line with `<s>` and `</s>` tokens\n",
    "4. Wrap each song with `<d>` and `</d>` tokens (documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_lyrics(lyrics):\n",
    "    tk = nltk.tokenize.LineTokenizer(blanklines='keep')\n",
    "    tokd = tk.tokenize(lyrics)\n",
    "    \n",
    "    re_tk = nltk.tokenize.RegexpTokenizer(r'\\[[^\\]]+\\]|\\w+|[\\d\\.,]+|\\S+',\n",
    "                                          discard_empty=False)\n",
    "    re_tokd = re_tk.tokenize_sents(tokd)\n",
    "    \n",
    "    [s.insert(0, '<s>') for s in re_tokd] # insert start token for each line\n",
    "    [s.append('</s>') for s in re_tokd] # append end token for each line\n",
    "    \n",
    "    flat = list(itertools.chain(*re_tokd))\n",
    "    flat.insert(0, '<d>')\n",
    "    flat.append('</d>')\n",
    "    # lower case and de-space\n",
    "    flat = [w.lower().replace(' ', '-') for w in flat]\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msd_id</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>tokd</th>\n",
       "      <th>tokd_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAADJU128F92F58E1</td>\n",
       "      <td>I hear you praying with your hands clasped ove...</td>\n",
       "      <td>[&lt;d&gt;, &lt;s&gt;, i, hear, you, praying, with, your, ...</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAADQX128F422B4CF</td>\n",
       "      <td>If you ever make it back to Nashville\\nRemembe...</td>\n",
       "      <td>[&lt;d&gt;, &lt;s&gt;, if, you, ever, make, it, back, to, ...</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAAFTE128F429545F</td>\n",
       "      <td>Just when I thought I was safe\\nYou found me i...</td>\n",
       "      <td>[&lt;d&gt;, &lt;s&gt;, just, when, i, thought, i, was, saf...</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAAKAG128F4275D2A</td>\n",
       "      <td>Paroles de la chanson Sultao Das Matas :\\nSult...</td>\n",
       "      <td>[&lt;d&gt;, &lt;s&gt;, paroles, de, la, chanson, sultao, d...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAAMRO128F92F20D7</td>\n",
       "      <td>From What You Whispered\\n........................</td>\n",
       "      <td>[&lt;d&gt;, &lt;s&gt;, from, what, you, whispered, &lt;/s&gt;, &lt;...</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               msd_id                                             lyrics  \\\n",
       "0  TRAADJU128F92F58E1  I hear you praying with your hands clasped ove...   \n",
       "1  TRAADQX128F422B4CF  If you ever make it back to Nashville\\nRemembe...   \n",
       "2  TRAAFTE128F429545F  Just when I thought I was safe\\nYou found me i...   \n",
       "3  TRAAKAG128F4275D2A  Paroles de la chanson Sultao Das Matas :\\nSult...   \n",
       "4  TRAAMRO128F92F20D7  From What You Whispered\\n........................   \n",
       "\n",
       "                                                tokd  tokd_len  \n",
       "0  [<d>, <s>, i, hear, you, praying, with, your, ...       215  \n",
       "1  [<d>, <s>, if, you, ever, make, it, back, to, ...       196  \n",
       "2  [<d>, <s>, just, when, i, thought, i, was, saf...       186  \n",
       "3  [<d>, <s>, paroles, de, la, chanson, sultao, d...        58  \n",
       "4  [<d>, <s>, from, what, you, whispered, </s>, <...       310  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokd'] = df.lyrics.apply(tokenize_lyrics)\n",
    "df['tokd_len'] = df.tokd.apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, just save both as both train and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_path = '../data/interim/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_test = np.array(df.tokd)\n",
    "np.save(token_path+'/train_tok.npy', tok_test)\n",
    "np.save(token_path+'/valid_tok.npy', tok_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<d>',\n",
       " '<s>',\n",
       " 'i',\n",
       " 'hear',\n",
       " 'you',\n",
       " 'praying',\n",
       " 'with',\n",
       " 'your',\n",
       " 'hands',\n",
       " 'clasped',\n",
       " 'over',\n",
       " 'your',\n",
       " 'chest',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'i',\n",
       " 'hear',\n",
       " 'men',\n",
       " 'slaying',\n",
       " 'while',\n",
       " 'they',\n",
       " 'say',\n",
       " '\"keep',\n",
       " 'doing',\n",
       " 'your',\n",
       " 'best',\n",
       " '\"',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'i',\n",
       " 'hear',\n",
       " 'the',\n",
       " 'laughter',\n",
       " 'of',\n",
       " 'someone']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_test[0][0:35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ULMFiT Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cymem.cymem.Pool has the wrong size, try recompiling. Expected 64, got 48",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-af8334892808>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfastai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbasic_train\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"Provides basic training and validation with `Learner`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtorch_core\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/torch_core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"Utility functions to help deal with tensors\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimports\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mAffineMatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/imports/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/imports/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmimetypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mabc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabstractmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabstractproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0municode_literals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcli_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mglossary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexplain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mabout\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/spacy/cli/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlink\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpackage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/spacy/cli/download.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_messages\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMessages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlink\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_package_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/spacy/cli/link.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_messages\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMessages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msymlink_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath2str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/spacy/compat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneural\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/neural/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmsgpack_numpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumpyOps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCupyOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/thinc/neural/util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_literals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpreshed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaps\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreshMap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mcymem.pxd\u001b[0m in \u001b[0;36minit preshed.maps\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cymem.cymem.Pool has the wrong size, try recompiling. Expected 64, got 48"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = TextLMDataBunch.from_tokens(token_path,\n",
    "                                      bs=128,\n",
    "                                      max_vocab=10000)\n",
    "print(data_lm.train_ds.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([95, 128]) torch.Size([12160])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;d&gt;</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>'s</td>\n",
       "      <td>[outro]</td>\n",
       "      <td>will</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>so</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>won</td>\n",
       "      <td>my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>sad</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>destroy</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>i</td>\n",
       "      <td>'t</td>\n",
       "      <td>soul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i</td>\n",
       "      <td>i</td>\n",
       "      <td>but</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>her</td>\n",
       "      <td>i</td>\n",
       "      <td>fight</td>\n",
       "      <td>said</td>\n",
       "      <td>deal</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hear</td>\n",
       "      <td>'ll</td>\n",
       "      <td>you</td>\n",
       "      <td>i</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>'ve</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>,</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>&lt;/d&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you</td>\n",
       "      <td>do</td>\n",
       "      <td>seem</td>\n",
       "      <td>'ll</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>been</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>it</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>&lt;d&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>praying</td>\n",
       "      <td>anything</td>\n",
       "      <td>better</td>\n",
       "      <td>never</td>\n",
       "      <td>so</td>\n",
       "      <td>takin</td>\n",
       "      <td>i</td>\n",
       "      <td>'s</td>\n",
       "      <td>i</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>with</td>\n",
       "      <td>for</td>\n",
       "      <td>when</td>\n",
       "      <td>know</td>\n",
       "      <td>i</td>\n",
       "      <td>'</td>\n",
       "      <td>cannot</td>\n",
       "      <td>my</td>\n",
       "      <td>won</td>\n",
       "      <td>xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>your</td>\n",
       "      <td>you</td>\n",
       "      <td>i</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>'m</td>\n",
       "      <td>all</td>\n",
       "      <td>shake</td>\n",
       "      <td>life</td>\n",
       "      <td>'t</td>\n",
       "      <td>xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hands</td>\n",
       "      <td>,</td>\n",
       "      <td>'m</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>giving</td>\n",
       "      <td>the</td>\n",
       "      <td>from</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>change</td>\n",
       "      <td>em</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>xxunk</td>\n",
       "      <td>show</td>\n",
       "      <td>gone</td>\n",
       "      <td>i</td>\n",
       "      <td>up</td>\n",
       "      <td>blame</td>\n",
       "      <td>my</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>,</td>\n",
       "      <td>busca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>over</td>\n",
       "      <td>me</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>'ll</td>\n",
       "      <td>,</td>\n",
       "      <td>but</td>\n",
       "      <td>brain</td>\n",
       "      <td>you</td>\n",
       "      <td>i</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>your</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>never</td>\n",
       "      <td>giving</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>what</td>\n",
       "      <td>ain</td>\n",
       "      <td>'ll</td>\n",
       "      <td>xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>chest</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>know</td>\n",
       "      <td>up</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>'t</td>\n",
       "      <td>tough</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>saw</td>\n",
       "      <td>gonna</td>\n",
       "      <td>it</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>and</td>\n",
       "      <td>&lt;/d&gt;</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>'ll</td>\n",
       "      <td>in</td>\n",
       "      <td>change</td>\n",
       "      <td>out</td>\n",
       "      <td>xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>i</td>\n",
       "      <td>oh</td>\n",
       "      <td>all</td>\n",
       "      <td>&lt;d&gt;</td>\n",
       "      <td>my</td>\n",
       "      <td>never</td>\n",
       "      <td>his</td>\n",
       "      <td>my</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>sua</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hear</td>\n",
       "      <td>baby</td>\n",
       "      <td>of</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>pride</td>\n",
       "      <td>make</td>\n",
       "      <td>eyes</td>\n",
       "      <td>life</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>banda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>men</td>\n",
       "      <td>,</td>\n",
       "      <td>these</td>\n",
       "      <td>well</td>\n",
       "      <td>for</td>\n",
       "      <td>the</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>,</td>\n",
       "      <td>xxunk</td>\n",
       "      <td>pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>xxunk</td>\n",
       "      <td>come</td>\n",
       "      <td>words</td>\n",
       "      <td>,</td>\n",
       "      <td>you</td>\n",
       "      <td>same</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>oh</td>\n",
       "      <td>it</td>\n",
       "      <td>rio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>while</td>\n",
       "      <td>on</td>\n",
       "      <td>i</td>\n",
       "      <td>you</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>mistake</td>\n",
       "      <td>in</td>\n",
       "      <td>no</td>\n",
       "      <td>out</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1       2        3        4        5       6       7  \\\n",
       "0       <d>      </s>      's  [outro]     will     </s>      so     <s>   \n",
       "1       <s>       <s>     sad     </s>  destroy      <s>       i       i   \n",
       "2         i         i     but      <s>      her        i   fight    said   \n",
       "3      hear       'll     you        i     </s>      've    </s>       ,   \n",
       "4       you        do    seem      'll      <s>     been     <s>      it   \n",
       "5   praying  anything  better    never       so    takin       i      's   \n",
       "6      with       for    when     know        i        '  cannot      my   \n",
       "7      your       you       i     </s>       'm      all   shake    life   \n",
       "8     hands         ,      'm      <s>   giving      the    from    </s>   \n",
       "9     xxunk      show    gone        i       up    blame      my     <s>   \n",
       "10     over        me    </s>      'll        ,      but   brain     you   \n",
       "11     your      </s>     <s>    never   giving     </s>    what     ain   \n",
       "12    chest       <s>    </s>     know       up      <s>       i      't   \n",
       "13     </s>      </s>     <s>     </s>     </s>        i     saw   gonna   \n",
       "14      <s>       <s>     and     </d>      <s>      'll      in  change   \n",
       "15        i        oh     all      <d>       my    never     his      my   \n",
       "16     hear      baby      of      <s>    pride     make    eyes    life   \n",
       "17      men         ,   these     well      for      the    </s>       ,   \n",
       "18    xxunk      come   words        ,      you     same     <s>      oh   \n",
       "19    while        on       i      you     </s>  mistake      in      no   \n",
       "\n",
       "         8      9  \n",
       "0      won     my  \n",
       "1       't   soul  \n",
       "2     deal   </s>  \n",
       "3     </s>   </d>  \n",
       "4      <s>    <d>  \n",
       "5        i    <s>  \n",
       "6      won  xxunk  \n",
       "7       't  xxunk  \n",
       "8   change     em  \n",
       "9        ,  busca  \n",
       "10       i     de  \n",
       "11     'll  xxunk  \n",
       "12   tough   </s>  \n",
       "13      it    <s>  \n",
       "14     out  xxunk  \n",
       "15    </s>    sua  \n",
       "16     <s>  banda  \n",
       "17   xxunk    pro  \n",
       "18      it    rio  \n",
       "19     out     de  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = next(iter(data_lm.train_dl))\n",
    "example = x[:20,:10].cpu()\n",
    "texts = pd.DataFrame([data_lm.train_ds.vocab.textify(l).split(' ') for l in example])\n",
    "print(x.shape, y.shape)\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ULMFiT Model architecture and create and embedding matrix that includes the new words. The new words are initialized to the mean value of all prior vocab...\n",
    "\n",
    "TODO: maybe update the initialization points to the mean value of prior vocab that we keep in this model. e.g. average of the words that are in the lyrics corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): RNNCore(\n",
       "    (encoder): Embedding(10002, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(10002, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1150)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1150, 1150)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1150, 400)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=10002, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = RNNLearner.language_model(data_lm,\n",
    "                                  #pretrained_fnames=['lstm_wt103', 'itos_wt103'],\n",
    "                                  drop_mult=0.5)\n",
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a glance at the new words that we've added to our vocabularly -- we add quite a bit. This is expected because this is such a specialized corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/imdb_sample/models/itos_wt103.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4bcaaec70a0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/imdb_sample/models/itos_wt103.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0maa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_lm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnew_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maa\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/imdb_sample/models/itos_wt103.pkl'"
     ]
    }
   ],
   "source": [
    "with open('../data/imdb_sample/models/itos_wt103.pkl', 'rb') as f:\n",
    "    aa = pickle.load(f)\n",
    "aa = set(aa)\n",
    "bb = data_lm.train_ds.vocab.itos\n",
    "new_words = [w for w in bb if w not in aa]\n",
    "new_words_id = data_lm.train_ds.vocab.numericalize(new_words)\n",
    "print(\"New Vocab: \", len(new_words))\n",
    "print(new_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0314, -0.0846, -0.0673,  ..., -0.0922, -0.0721, -0.0338],\n",
       "        [ 0.0595, -0.0232,  0.0071,  ..., -0.0663, -0.0444,  0.0865],\n",
       "        [-0.0644,  0.0233, -0.0846,  ..., -0.0684, -0.0425,  0.0267],\n",
       "        ...,\n",
       "        [-0.0347,  0.0805, -0.0097,  ...,  0.0473,  0.0058, -0.0504],\n",
       "        [ 0.0009,  0.0383, -0.0344,  ...,  0.0696, -0.0689,  0.0592],\n",
       "        [ 0.0415,  0.0318, -0.0593,  ...,  0.0845, -0.0466, -0.0619]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model.state_dict().get('0.encoder.weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit one cycle, but keep all layers frozen except the linear encoder and decoder. Start with a realtively low learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, max=1), HTML(value='0.00% [0/1 00:00<00:00]'))), HTML(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 19:12\n",
      "epoch  train loss  valid loss  accuracy\n",
      "0      3.142067    3.021067    0.429976  (19:12)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2809,  0.0228,  0.0205,  ...,  0.0531, -0.2897,  0.5122],\n",
       "        [ 0.3141,  0.1556, -1.2372,  ...,  0.8679, -0.3641,  0.7379],\n",
       "        [-1.3342,  0.1629, -0.1386,  ...,  0.7200, -0.1327,  0.5287],\n",
       "        ...,\n",
       "        [-0.0839, -0.2861, -1.0583,  ...,  0.1260, -0.8269,  1.5154],\n",
       "        [ 0.4406, -0.5391, -1.1914,  ..., -0.2362, -2.1606,  1.1331],\n",
       "        [ 0.9451,  0.8559, -2.0430,  ..., -0.0076, -1.3519,  1.7898]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model.state_dict().get('0.encoder.weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68dfea05b65b41e48aebb542b1919bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, max=10), HTML(value='0.00% [0/10 00:00<00:00]'))), HTML(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('100k-custom-tok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('100k-custom-tok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/j_rosen_1392/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(learner, seed_text=['<d>', '<s>', '</s>', '<s>', '</s>','<s>', '[verse]', '</s>'], max_len=2500):\n",
    "    model = learner.model\n",
    "    tokens = LongTensor(data_lm.train_ds.vocab.numericalize(seed_text)).view(-1,1).cuda()\n",
    "    token_seq = torch.autograd.Variable(tokens)\n",
    "    \n",
    "    #reset hidden state\n",
    "    model.reset()\n",
    "    model.eval()\n",
    "\n",
    "    #loop over each max length of words\n",
    "    for _ in range(max_len):\n",
    "        # forward pass\n",
    "        result, *_ = model(token_seq)\n",
    "        r = result[-1]\n",
    "\n",
    "        # set unk and pad to 0 prob\n",
    "        # i.e. never pick unknown or pad\n",
    "        r[0] = -np.inf\n",
    "        r[1] = -np.inf\n",
    "\n",
    "        # softmax\n",
    "        r2 = F.softmax(r, dim=0)\n",
    "        r2 = np.asarray(r2.detach().cpu(), dtype=np.float)\n",
    "        r2 /= np.sum(r2) # solve rounding issues for multinom function\n",
    "\n",
    "        # draw multinom\n",
    "        token_index = np.argmax(np.random.multinomial(1, r2))\n",
    "\n",
    "        word = data_lm.valid_ds.vocab.textify([token_index])\n",
    "        token_index = LongTensor([token_index]).view(-1, 1).cuda()\n",
    "        token_seq = torch.cat((token_seq, token_index))    \n",
    "        if word == '</s>':\n",
    "            word = '\\n'\n",
    "        print(word, end=' ')\n",
    "        if word == '</d>': break    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> and i will move \n",
      " <s> far from left and giving up \n",
      " <s> it was that for sure \n",
      " <s> and i , that you believe \n",
      " <s> we 're like a bird \n",
      " <s> and there 's something \n",
      " <s> you 've seen that \n",
      " <s> the best things \n",
      " <s> are true \n",
      " <s> if i could \n",
      " <s> i would have listened \n",
      " <s> erase the regret \n",
      " <s> \n",
      " <s> i had some ways \n",
      " <s> for the oh , the man \n",
      " <s> that left 're through \n",
      " <s> so , let 's run \n",
      " <s> \n",
      " <s> back again \n",
      " <s> pushed away \n",
      " <s> i 'm out of time \n",
      " <s> out of my thoughts \n",
      " <s> you said i told you \n",
      " <s> still saying \n",
      " <s> \n",
      " <s> [chorus] \n",
      " <s> never fall so down \n",
      " <s> i 'll never be away to losing you \n",
      " <s> so glad that i found you \n",
      " <s> all you ever wanted was you \n",
      " <s> only in my dreams through the miles \n",
      " <s> could it be what you wanted ? \n",
      " <s> all i love was tenderly \n",
      " <s> not now , now , now \n",
      " <s> \n",
      " <s> i do ’d like to know \n",
      " <s> if i called it \n",
      " <s> i tell that you would \n",
      " <s> i wish that i wasn 't meant to \n",
      " <s> yeah , the feelings and the days i 'd change \n",
      " <s> \n",
      " <s> [chorus] \n",
      " <s> because you know i can 't leave \n",
      " <s> you 're like i must leave \n",
      " <s> under the sea \n",
      " <s> babe , i would die \n",
      " <s> out on my mind \n",
      " <s> \n",
      " <s> [verse-2] \n",
      " <s> so hard to keep away \n",
      " <s> an honest apple to go \n",
      " <s> but i 'll be there forever \n",
      " <s> but i nearly won 't know what i 'm at \n",
      " <s> i 'm not listening to your brain \n",
      " <s> i could be here to keep you strong \n",
      " <s> and i 'll be a thief in my soul \n",
      " <s> after all that i done love you and not switch \n",
      " <s> i will cover you \n",
      " <s> i shall reply we 're all alone \n",
      " <s> until we hide the dark \n",
      " <s> \n",
      " <s> [chorus] \n",
      " <s> in the deepest highways \n",
      " <s> today will never end \n",
      " <s> arise \n",
      " <s> to no one \n",
      " <s> to only cry \n",
      " <s> you ´ll die \n",
      " </d> "
     ]
    }
   ],
   "source": [
    "generate_text(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.27236950e-01,  7.88899288e-02,  1.41409770e-01, ...,\n",
       "         1.18825608e-03, -2.78300911e-01,  2.75513470e-01],\n",
       "       [ 3.01054031e-01,  2.12108001e-01, -1.18636227e+00, ...,\n",
       "         8.79542947e-01, -3.23059350e-01,  7.22483337e-01],\n",
       "       [-1.08073759e+00,  1.05993815e-01,  3.97628546e-02, ...,\n",
       "         7.56273389e-01, -1.93531737e-01,  4.99946654e-01],\n",
       "       ...,\n",
       "       [ 1.91946849e-02,  8.63057375e-03, -5.99980175e-01, ...,\n",
       "        -1.33894552e-02, -6.13913774e-01,  8.61503780e-01],\n",
       "       [ 2.98022062e-01, -1.85456827e-01, -8.70638669e-01, ...,\n",
       "         7.70014673e-02, -1.51546502e+00,  8.48747432e-01],\n",
       "       [ 6.35387242e-01,  5.96060991e-01, -1.86128807e+00, ...,\n",
       "        -1.65299833e-01, -1.06414533e+00,  1.37599111e+00]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = learn.model.state_dict().get('0.encoder.weight').cpu().numpy()\n",
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embed = pd.DataFrame(data=embed,\n",
    "                        index=data_lm.train_ds.vocab.itos)\n",
    "df_embed.to_csv('../data/models/embeddings.csv',\n",
    "                sep='\\t',\n",
    "                index=False,\n",
    "                header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.DataFrame(data=data_lm.train_ds.vocab.itos,\n",
    "                       columns=['token'])\n",
    "df_meta.to_csv('../data/models/embeddings_meta.csv',\n",
    "               sep='\\t',\n",
    "               header=False,\n",
    "               index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
