{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning PyTorch for Multimodality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Model of Sequential Modules\n",
    "\n",
    "Example from [here](https://github.com/jcjohnson/pytorch-examples#pytorch-optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 636.4593505859375\n",
      "1 623.7313232421875\n",
      "2 611.2108764648438\n",
      "3 598.9217529296875\n",
      "4 586.8594360351562\n",
      "5 575.022705078125\n",
      "6 563.3897705078125\n",
      "7 551.9767456054688\n",
      "8 540.7965698242188\n",
      "9 529.8015747070312\n"
     ]
    }
   ],
   "source": [
    "N, D_in, Z_in, H, H2, D_out = 64, 400, 20, 300, 200, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "z = torch.randn(N, Z_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(10):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['0.weight', '0.bias', '2.weight', '2.bias'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().get('0.bias').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding to \"Multi-Modal\" with Custom Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So rather than stringing together modules like above, we have to create a custom class for our model. Mainly, this is because we are concatenating multiple inputs.\n",
    "\n",
    "This still hasn't been tested on real data, and I don't know all the \"gotchas\" of these modules yet, but this works as expected thus far, and creates weight matrices as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multimodal(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, Z_in, H2, D_out):\n",
    "        super(Multimodal, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.multi = torch.nn.Linear(H + Z_in, H2)\n",
    "        self.linear2 = torch.nn.Linear(H2, D_out)\n",
    "        \n",
    "    def forward(self, x, z):\n",
    "        h1 = self.linear1(x)\n",
    "        h1_z = torch.cat([h1, z], dim=1)\n",
    "        h2 = self.multi(h1_z)\n",
    "        out = self.linear2(h2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of backend CPU but got backend CUDA for argument #4 'mat1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-22fb5c6d4a4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-f190b67c0da2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, z)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mh1_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh1_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of backend CPU but got backend CUDA for argument #4 'mat1'"
     ]
    }
   ],
   "source": [
    "N, D_in, Z_in, H, H2, D_out = 64, 400, 20, 300, 200, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "z = torch.randn(N, Z_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "model = Multimodal(D_in, H, Z_in, H2, D_out)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(10):\n",
    "    y_pred = model(x, z)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the \"multi\" weights have input weights of length 320 because we take 300 from the output of `linear1` + 20 from the auxillary input (e.g. z) and then we map this to the output of 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['linear1.weight', 'linear1.bias', 'multi.weight', 'multi.bias', 'linear2.weight', 'linear2.bias'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 320])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().get('multi.weight').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multimodal(\n",
      "  (linear1): Linear(in_features=400, out_features=300, bias=True)\n",
      "  (multi): Linear(in_features=320, out_features=200, bias=True)\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual run of FastAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bring in some sample data using fastai imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([72, 64, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "data_lm = TextLMDataBunch.from_csv(path)\n",
    "\n",
    "for x, y in list(data_lm.train_dl): # just testing with one batch\n",
    "    x, y\n",
    "    \n",
    "z = torch.randn(x.shape[0], x.shape[1], 10,\n",
    "                device=device, requires_grad=True) # making up 10 \"audio features\"\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.encoder.weight': torch.Size([5999, 400]),\n",
       " '0.encoder_dp.emb.weight': torch.Size([5999, 400]),\n",
       " '0.rnns.0.weight_hh_l0_raw': torch.Size([4600, 1150]),\n",
       " '0.rnns.0.module.weight_ih_l0': torch.Size([4600, 400]),\n",
       " '0.rnns.0.module.weight_hh_l0': torch.Size([4600, 1150]),\n",
       " '0.rnns.0.module.bias_ih_l0': torch.Size([4600]),\n",
       " '0.rnns.0.module.bias_hh_l0': torch.Size([4600]),\n",
       " '0.rnns.1.weight_hh_l0_raw': torch.Size([4600, 1150]),\n",
       " '0.rnns.1.module.weight_ih_l0': torch.Size([4600, 1150]),\n",
       " '0.rnns.1.module.weight_hh_l0': torch.Size([4600, 1150]),\n",
       " '0.rnns.1.module.bias_ih_l0': torch.Size([4600]),\n",
       " '0.rnns.1.module.bias_hh_l0': torch.Size([4600]),\n",
       " '0.rnns.2.weight_hh_l0_raw': torch.Size([1600, 400]),\n",
       " '0.rnns.2.module.weight_ih_l0': torch.Size([1600, 1150]),\n",
       " '0.rnns.2.module.weight_hh_l0': torch.Size([1600, 400]),\n",
       " '0.rnns.2.module.bias_ih_l0': torch.Size([1600]),\n",
       " '0.rnns.2.module.bias_hh_l0': torch.Size([1600]),\n",
       " '1.decoder.weight': torch.Size([5999, 400]),\n",
       " '1.decoder.bias': torch.Size([5999])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_learn = RNNLearner.language_model(data_lm)\n",
    "{k:v.shape for (k,v) in original_learn.model.state_dict().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_learn.save('imdb-model-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(original_learn.path/original_learn.model_dir/'imdb-itos.pkl', 'wb') as f:\n",
    "    pickle.dump(original_learn.data.train_ds.vocab.itos, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure we can run the same model as fastai manually..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sz = 5999\n",
    "emb_sz = 400\n",
    "n_hid = 1150\n",
    "n_layers = 3\n",
    "pad_token= 1\n",
    "qrnn = False\n",
    "bidir = False\n",
    "\n",
    "dps = np.array([0.25, 0.1, 0.2, 0.02, 0.15])\n",
    "\n",
    "hidden_p = dps[4]\n",
    "input_p = dps[0]\n",
    "embed_p = dps[3]\n",
    "weight_p = dps[2]\n",
    "\n",
    "tie_weights = True\n",
    "output_p = dps[1]\n",
    "bias = True\n",
    "\n",
    "audio_sz = 10\n",
    "\n",
    "# Create a full AWD-LSTM.\n",
    "rnn_enc = RNNCore(vocab_sz=vocab_sz,\n",
    "                  emb_sz=emb_sz,\n",
    "                  n_hid=n_hid,\n",
    "                  n_layers=n_layers,\n",
    "                  pad_token=pad_token,\n",
    "                  qrnn=qrnn,\n",
    "                  bidir=bidir,\n",
    "                  hidden_p=hidden_p,\n",
    "                  input_p=input_p,\n",
    "                  embed_p=embed_p,\n",
    "                  weight_p=weight_p)\n",
    "\n",
    "enc = rnn_enc.encoder if tie_weights else None\n",
    "model = SequentialRNN(rnn_enc, LinearDecoder(vocab_sz, emb_sz, output_p, tie_encoder=enc, bias=bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of backend CPU but got backend CUDA for argument #3 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2481cb3b5a08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/text/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mraw_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_dp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mnew_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mraw_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhid_dp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/text/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, words, scale)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmasked_embed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n\u001b[0;32m---> 70\u001b[0;31m                            self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m#def _repackage_var(h:Tensors)->Tensors:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1205\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of backend CPU but got backend CUDA for argument #3 'index'"
     ]
    }
   ],
   "source": [
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8.698554992675781\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "model.reset()\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "opt_params = model.parameters()\n",
    "\n",
    "# this is a hack for now... not sure if this messes up the graph somewhere by doing this\n",
    "# opt_params = [par for par in model.parameters() if par.is_leaf]\n",
    "optimizer = torch.optim.Adam(opt_params, lr=learning_rate)\n",
    "\n",
    "y_pred = model(x)\n",
    "\n",
    "for t in range(1):\n",
    "    y_pred = model(x)[0]\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, try porting this into the fastai learner object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.reset()\n",
    "# learn = RNNLearner(data_lm, model)\n",
    "# learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Multimodality to FastAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far, I'm able to get multimodal_rnn to work, but it doesnt work when used with SequentialRNN. Pretty sure this is because `forward` is not registered properly with SequentialRNN.\n",
    "\n",
    "Below, I attempt to include the decoder directly into the custom module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiModalRNN(\n",
       "  (encoder): Embedding(5999, 400, padding_idx=1)\n",
       "  (encoder_dp): EmbeddingDropout(\n",
       "    (emb): Embedding(5999, 400, padding_idx=1)\n",
       "  )\n",
       "  (rnns): None\n",
       "  (input_dp): RNNDropout()\n",
       "  (hidden_dps): ModuleList(\n",
       "    (0): RNNDropout()\n",
       "    (1): RNNDropout()\n",
       "    (2): RNNDropout()\n",
       "  )\n",
       "  (multimode): ModuleList(\n",
       "    (0): WeightDropout(\n",
       "      (module): LSTM(410, 1150)\n",
       "    )\n",
       "    (1): WeightDropout(\n",
       "      (module): LSTM(1150, 1150)\n",
       "    )\n",
       "    (2): WeightDropout(\n",
       "      (module): LSTM(1150, 400)\n",
       "    )\n",
       "  )\n",
       "  (multidecoder): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=5999, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiModalRNN(RNNCore):\n",
    "    def __init__(self, audio_sz, output_p, bias, tie_encoder:bool=True, **kwargs):\n",
    "        super(MultiModalRNN, self).__init__(**kwargs)\n",
    "        self.rnns = None\n",
    "        self.audio_sz = audio_sz\n",
    "        self.multimode = [nn.LSTM(emb_sz + audio_sz if l == 0 else n_hid,\n",
    "                                  (n_hid if l != n_layers - 1 else emb_sz)//self.ndir,\n",
    "                                  1, bidirectional=bidir) for l in range(n_layers)]\n",
    "        self.multimode = [WeightDropout(rnn, weight_p) for rnn in self.multimode]\n",
    "        self.multimode = torch.nn.ModuleList(self.multimode)\n",
    "        \n",
    "        if tie_encoder:\n",
    "            enc = self.encoder\n",
    "        else:\n",
    "            enc = None\n",
    "        \n",
    "        self.multidecoder = LinearDecoder(vocab_sz,\n",
    "                                          emb_sz,\n",
    "                                          output_p,\n",
    "                                          tie_encoder=enc,\n",
    "                                          bias=bias)\n",
    "        \n",
    "    def forward(self, input:LongTensor, input_audio:Tensor)->Tuple[Tensor,Tensor,Tensor]:\n",
    "        sl,bs = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        raw_output = self.input_dp(self.encoder_dp(input))\n",
    "        raw_output = torch.cat([raw_output, input_audio], dim=2)\n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.multimode, self.hidden_dps)):\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output)\n",
    "        self.hidden = to_detach(new_hidden)\n",
    "        \n",
    "        output = self.multidecoder.output_dp(outputs[-1])\n",
    "        decoded = self.multidecoder.decoder(output.view(output.size(0)*output.size(1),\n",
    "                                                        output.size(2)))\n",
    "        \n",
    "        return decoded, raw_outputs, outputs\n",
    "    \n",
    "    def _one_hidden(self, l:int)->Tensor:\n",
    "        \"Return one hidden state.\"\n",
    "        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz)//self.ndir\n",
    "        return self.weights.new(self.ndir, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states.\"\n",
    "        [r.reset() for r in self.multimode if hasattr(r, 'reset')]\n",
    "        self.weights = next(self.parameters()).data\n",
    "        if self.qrnn: self.hidden = [self._one_hidden(l) for l in range(self.n_layers)]\n",
    "        else: self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]\n",
    "    \n",
    "multimodal_rnn = MultiModalRNN(audio_sz=audio_sz,\n",
    "                              vocab_sz=vocab_sz,\n",
    "                              emb_sz=emb_sz,\n",
    "                              n_hid=n_hid,\n",
    "                              n_layers=n_layers,\n",
    "                              pad_token=pad_token,\n",
    "                              qrnn=qrnn,\n",
    "                              bidir=bidir,\n",
    "                              hidden_p=hidden_p,\n",
    "                              input_p=input_p,\n",
    "                              embed_p=embed_p,\n",
    "                              weight_p=weight_p,\n",
    "                              output_p=output_p,\n",
    "                              bias=bias,\n",
    "                              tie_encoder=tie_weights).to(device)\n",
    "\n",
    "multimodal_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = multimodal_rnn(x, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLinearDecoder(nn.Module):\n",
    "    \"To go on top of a RNNCore module and create a Language Model.\"\n",
    "\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, n_out:int, n_hid:int, audio_sz:int,\n",
    "                 output_p:float, tie_encoder:nn.Module=None,\n",
    "                 bias:bool=True):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Linear(n_hid + audio_sz, n_out, bias=bias)\n",
    "        self.decoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "\n",
    "    def forward(self, input:Tuple[Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "        raw_outputs, outputs = input\n",
    "        output = self.output_dp(outputs[-1])\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded, raw_outputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiModalPostRNN(\n",
       "  (encoder): Embedding(5999, 400, padding_idx=1)\n",
       "  (encoder_dp): EmbeddingDropout(\n",
       "    (emb): Embedding(5999, 400, padding_idx=1)\n",
       "  )\n",
       "  (rnns): None\n",
       "  (input_dp): RNNDropout()\n",
       "  (hidden_dps): ModuleList(\n",
       "    (0): RNNDropout()\n",
       "    (1): RNNDropout()\n",
       "    (2): RNNDropout()\n",
       "  )\n",
       "  (multimode): ModuleList(\n",
       "    (0): WeightDropout(\n",
       "      (module): LSTM(400, 1150)\n",
       "    )\n",
       "    (1): WeightDropout(\n",
       "      (module): LSTM(1150, 1150)\n",
       "    )\n",
       "    (2): WeightDropout(\n",
       "      (module): LSTM(1150, 400)\n",
       "    )\n",
       "  )\n",
       "  (multidecoder): MultiLinearDecoder(\n",
       "    (decoder): Linear(in_features=410, out_features=5999, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiModalPostRNN(RNNCore):\n",
    "    def __init__(self, audio_sz, output_p, bias, tie_encoder:bool=False, **kwargs):\n",
    "        super(MultiModalPostRNN, self).__init__(**kwargs)\n",
    "        self.rnns = None\n",
    "        self.multimode = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz)//self.ndir,\n",
    "            1, bidirectional=bidir) for l in range(n_layers)]\n",
    "        self.multimode = [WeightDropout(rnn, weight_p) for rnn in self.multimode]\n",
    "        self.multimode = torch.nn.ModuleList(self.multimode)\n",
    "        self.audio_sz = audio_sz\n",
    "#         self.multimode = [nn.LSTM(emb_sz + audio_sz if l == 0 else n_hid,\n",
    "#                                   (n_hid if l != n_layers - 1 else emb_sz)//self.ndir,\n",
    "#                                   1, bidirectional=bidir) for l in range(n_layers)]\n",
    "#         self.multimode = [WeightDropout(rnn, weight_p) for rnn in self.multimode]\n",
    "#         self.multimode = torch.nn.ModuleList(self.multimode)\n",
    "        \n",
    "        if tie_encoder:\n",
    "            enc = self.encoder\n",
    "        else:\n",
    "            enc = None\n",
    "        \n",
    "        self.multidecoder = MultiLinearDecoder(vocab_sz,\n",
    "                                               emb_sz,\n",
    "                                               audio_sz,\n",
    "                                               output_p,\n",
    "                                               tie_encoder=enc,\n",
    "                                               bias=bias)\n",
    "        \n",
    "    def forward(self, input:LongTensor, input_audio:Tensor)->Tuple[Tensor,Tensor,Tensor]:\n",
    "        sl,bs = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        raw_output = self.input_dp(self.encoder_dp(input))\n",
    "#         raw_output = torch.cat([raw_output, input_audio], dim=2)\n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.multimode, self.hidden_dps)):\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output)\n",
    "        self.hidden = to_detach(new_hidden)\n",
    "        \n",
    "        output = self.multidecoder.output_dp(outputs[-1])\n",
    "        output = torch.cat([raw_output, input_audio], dim=2)\n",
    "        decoded = self.multidecoder.decoder(output.view(output.size(0)*output.size(1),\n",
    "                                                        output.size(2)))\n",
    "        \n",
    "        return decoded, raw_outputs, outputs\n",
    "    \n",
    "    def _one_hidden(self, l:int)->Tensor:\n",
    "        \"Return one hidden state.\"\n",
    "        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz)//self.ndir\n",
    "        return self.weights.new(self.ndir, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states.\"\n",
    "        [r.reset() for r in self.multimode if hasattr(r, 'reset')]\n",
    "        self.weights = next(self.parameters()).data\n",
    "        if self.qrnn: self.hidden = [self._one_hidden(l) for l in range(self.n_layers)]\n",
    "        else: self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]\n",
    "    \n",
    "multimodal_rnn = MultiModalPostRNN(audio_sz=audio_sz,\n",
    "                              vocab_sz=vocab_sz,\n",
    "                              emb_sz=emb_sz,\n",
    "                              n_hid=n_hid,\n",
    "                              n_layers=n_layers,\n",
    "                              pad_token=pad_token,\n",
    "                              qrnn=qrnn,\n",
    "                              bidir=bidir,\n",
    "                              hidden_p=hidden_p,\n",
    "                              input_p=input_p,\n",
    "                              embed_p=embed_p,\n",
    "                              weight_p=weight_p,\n",
    "                              output_p=output_p,\n",
    "                              bias=bias,\n",
    "                              tie_encoder=False).to(device)\n",
    "\n",
    "multimodal_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = multimodal_rnn(x, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0027,  0.2645,  0.2557,  ..., -0.0856, -0.0201, -0.1912],\n",
       "        [ 0.1005, -0.0032,  0.2590,  ...,  0.2249,  0.1127, -0.0132],\n",
       "        [-0.2534, -0.3319, -0.3910,  ...,  0.0995,  0.1720,  0.3377],\n",
       "        ...,\n",
       "        [-0.2927, -0.1935, -0.2345,  ...,  0.1611, -0.1844,  0.1487],\n",
       "        [-0.0127,  0.0996, -0.0252,  ..., -0.1012,  0.0079, -0.2184],\n",
       "        [-0.1856, -0.0598, -0.1102,  ...,  0.0976,  0.1709,  0.3871]],\n",
       "       device='cuda:0', grad_fn=<ThAddmmBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = multimodal_rnn(x, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4160, 5999])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should be able to train the data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 36194.5390625\n"
     ]
    }
   ],
   "source": [
    "multimodal_rnn.train()\n",
    "multimodal_rnn.reset()\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "opt_params = multimodal_rnn.parameters()\n",
    "\n",
    "# this is a hack for now... not sure if this messes up the graph somewhere by doing this\n",
    "# opt_params = [par for par in multimodal_rnn.parameters() if par.is_leaf]\n",
    "optimizer = torch.optim.Adam(opt_params, lr=learning_rate)\n",
    "\n",
    "y_pred = multimodal_rnn(x, z)\n",
    "\n",
    "for t in range(1):\n",
    "    y_pred = multimodal_rnn(x, z)[0]\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~So we get an error for those that are non-leaf nodes (tensors). Doing some investigating shows those that have a `grad_fn` are the culprits. These are most likely the tensors that are dropouts. Need to look at fastai and see how they handle these tensors...~~\n",
    "\n",
    "~~For now I used the code above to subset to `is_leaf == False`. I'm not sure if this \"is leaf\" is a symptom of something that's not working right or if this is, in fact, what we should do.~~\n",
    "\n",
    "Using `model.reset()` solves the above issue.\n",
    "\n",
    "In the original `RNNCore` class I'm a bit confused by what is happening for `self.rnns` still.. but assuming this is fine for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make training happen with the learner framework... First step is data.\n",
    "\n",
    "The following is a new class called `AudioDataset`. To initialize, we have to link it to a `TextDataset` instance, and the two will be used together to initialize `MultimodalDataLoader`. The reason the Datesets must align, is so that when we create bptt batches in the DataLoader object, the audio attributes remain the same for songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    PyTorch Dataset for Aggregate Audio features from MSD\n",
    "    '''\n",
    "    def __init__(self, data:np.ndarray, text_link:TextDataset):\n",
    "        '''\n",
    "        Args\n",
    "        ----\n",
    "          data (2-darray) : a 2-d array of (n, features). n must be the same\n",
    "                            size as the length of `text_link`\n",
    "          text_link       : an instance of `TextDataset` from fastai to link\n",
    "                            with. This is necessary to create a dataloader\n",
    "                            of language and corresponding audio features\n",
    "        '''\n",
    "        # audio features\n",
    "        self.data = data\n",
    "        \n",
    "        # linked song lyrics\n",
    "        self.text_link = text_link\n",
    "        assert len(self.text_link) == len(self.data),\\\n",
    "        \"Number of examples must be the same.\"\n",
    "        \n",
    "        # return the length of each song\n",
    "        self.text_length = self._get_text_length()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx:int):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def _get_text_length(self):\n",
    "        '''\n",
    "        An list of size n containing the length of lyrics for each song in `text_link`\n",
    "        '''\n",
    "        return [len(song[0]) for song in self.text_link]\n",
    "    \n",
    "    @property\n",
    "    def feature_size(self):\n",
    "        return self.data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts = torch.load(original_learn.path/original_learn.model_dir/'imdb-model-test.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "text_data = copy.copy(data_lm.train_ds)\n",
    "audio_features = np.random.normal(size=(800,audio_sz))\n",
    "\n",
    "data_audio = AudioDataset(audio_features, text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataLoader(LanguageModelLoader):\n",
    "    \"\"\"\n",
    "    A data loader for language model with bptt with augmented audio features\n",
    "    \"\"\"\n",
    "    def __init__(self, audio_dataset, *args, **kwargs):\n",
    "        super(MultimodalDataLoader, self).__init__(*args, **kwargs)\n",
    "        self.audio_dataset = audio_dataset\n",
    "        self.audio_data = self.batchify_audio(audio_dataset)\n",
    "        \n",
    "        \n",
    "    def batchify_audio(self, audio_data):\n",
    "        '''\n",
    "        Repeats audio features for each word in song and batchifies\n",
    "        '''\n",
    "        nb = np.sum(audio_data.text_length) // self.bs # words per batch\n",
    "        \n",
    "        # repeat audio features\n",
    "        repeated_audio = np.empty((0, self.audio_dataset.feature_size))\n",
    "        for song in zip(audio_data.data, audio_data.text_length):\n",
    "            features, song_length = song\n",
    "            repeated_audio = np.append(repeated_audio,\n",
    "                                       np.tile(features, (song_length, 1)),\n",
    "                                       axis=0)\n",
    "        \n",
    "        # reshape to (nb * bs)\n",
    "        audio = repeated_audio[:nb*self.bs]\\\n",
    "            .reshape(-1, self.bs, self.audio_dataset.feature_size)\n",
    "        return Tensor(audio)\n",
    "    \n",
    "    def get_batch(self, i:int, seq_len:int) -> Tuple[LongTensor, Tensor, LongTensor]:\n",
    "        \"Create a batch at `i` of a given `seq_len`.\"\n",
    "        seq_len = min(seq_len, len(self.data) - 1 - i)\n",
    "        return ((self.data[i:i+seq_len],\n",
    "                self.audio_data[i:i+seq_len]),\n",
    "                self.data[i+1:i+1+seq_len].contiguous().view(-1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_data = MultimodalDataLoader(audio_dataset=data_audio, dataset=text_data)\n",
    "# just doubling up the training data for now\n",
    "multi_db = DataBunch(multi_data, multi_data, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1 = next(iter(multi_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([65, 64, 15])"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see what we can do about training... Bringing in some lower-level fastai functions and modifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_batch(model:nn.Module, xb:Tensor, yb:Tensor, loss_func:OptLossFunc=None, opt:OptOptimizer=None,\n",
    "#                cb_handler:Optional[CallbackHandler]=None)->Tuple[Union[Tensor,int,float,str]]:\n",
    "#     \"Calculate loss and metrics for a batch, call out to callbacks as necessary.\"\n",
    "#     cb_handler = ifnone(cb_handler, CallbackHandler())\n",
    "#     if not is_listy(xb): xb = [xb]\n",
    "#     if not is_listy(yb): yb = [yb]\n",
    "#     out = model(*xb)\n",
    "#     out = cb_handler.on_loss_begin(out)\n",
    "\n",
    "#     if not loss_func: return to_detach(out), yb[0].detach()\n",
    "#     loss = loss_func(out, *yb)\n",
    "\n",
    "#     if opt is not None:\n",
    "#         loss = cb_handler.on_backward_begin(loss)\n",
    "#         loss.backward()\n",
    "#         cb_handler.on_backward_end()\n",
    "#         opt.step()\n",
    "#         cb_handler.on_step_end()\n",
    "#         opt.zero_grad()\n",
    "\n",
    "#     return loss.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multimodal_rnn.train()\n",
    "# multimodal_rnn.reset()\n",
    "\n",
    "# loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "# learning_rate = 1e-4\n",
    "# opt_params = multimodal_rnn.parameters()\n",
    "# optimizer = torch.optim.Adam(opt_params, lr=learning_rate)\n",
    "\n",
    "# xb, yb = next(iter(multi_data))\n",
    "\n",
    "# # loss_batch(multimodal_rnn, xb, yb, loss_fn, optimizer)\n",
    "# xxx = multimodal_rnn(*xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multimodal_rnn.train()\n",
    "# multimodal_rnn.reset()\n",
    "# loss_fn = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "# learning_rate = 1e-4\n",
    "# opt_params = multimodal_rnn.parameters()\n",
    "\n",
    "# optimizer = torch.optim.Adam(opt_params, lr=learning_rate)\n",
    "\n",
    "learn = RNNLearner(multi_db, multimodal_rnn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "        \t/* Turns off some styling */\n",
       "        \tprogress {\n",
       "\n",
       "            \t/* gets rid of default border in Firefox and Opera. */\n",
       "            \tborder: none;\n",
       "\n",
       "            \t/* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "            \tbackground-size: auto;\n",
       "            }\n",
       "\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "\n",
       "  </tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "        \t/* Turns off some styling */\n",
       "        \tprogress {\n",
       "\n",
       "            \t/* gets rid of default border in Firefox and Opera. */\n",
       "            \tborder: none;\n",
       "\n",
       "            \t/* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "            \tbackground-size: auto;\n",
       "            }\n",
       "\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='progress-bar-interrupted' max='51', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      Interrupted\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-367-8f7ad3f8da02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         fit(epochs, self.model, self.loss_func, opt=self.opt, data=self.data, metrics=self.metrics,\n\u001b[0;32m--> 162\u001b[0;31m             callbacks=self.callbacks+callbacks)\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, model, loss_func, opt, data, callbacks, metrics)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_loss_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-a038e083dd8b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, input_audio)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mnew_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mraw_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhid_dp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultimode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mraw_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mnew_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mraw_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/text/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m#To avoid the warning that comes because the weights aren't flattened.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoder.weight': torch.Size([5999, 400]),\n",
       " 'encoder_dp.emb.weight': torch.Size([5999, 400]),\n",
       " 'multidecoder.decoder.bias': torch.Size([5999]),\n",
       " 'multidecoder.decoder.weight': torch.Size([5999, 400]),\n",
       " 'multimode.0.module.bias_hh_l0': torch.Size([4600]),\n",
       " 'multimode.0.module.bias_ih_l0': torch.Size([4600]),\n",
       " 'multimode.0.module.weight_hh_l0': torch.Size([4600, 1150]),\n",
       " 'multimode.0.module.weight_ih_l0': torch.Size([4600, 410]),\n",
       " 'multimode.0.weight_hh_l0_raw': torch.Size([4600, 1150]),\n",
       " 'multimode.1.module.bias_hh_l0': torch.Size([4600]),\n",
       " 'multimode.1.module.bias_ih_l0': torch.Size([4600]),\n",
       " 'multimode.1.module.weight_hh_l0': torch.Size([4600, 1150]),\n",
       " 'multimode.1.module.weight_ih_l0': torch.Size([4600, 1150]),\n",
       " 'multimode.1.weight_hh_l0_raw': torch.Size([4600, 1150]),\n",
       " 'multimode.2.module.bias_hh_l0': torch.Size([1600]),\n",
       " 'multimode.2.module.bias_ih_l0': torch.Size([1600]),\n",
       " 'multimode.2.module.weight_hh_l0': torch.Size([1600, 400]),\n",
       " 'multimode.2.module.weight_ih_l0': torch.Size([1600, 1150]),\n",
       " 'multimode.2.weight_hh_l0_raw': torch.Size([1600, 400])}"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multimodal_rnn.state_dict()\n",
    "{k:v.shape for (k,v) in learn.model.state_dict().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_TO_MULTI = {\n",
    "    '0.encoder.weight': 'encoder.weight',\n",
    "    '0.encoder_dp.emb.weight': 'encoder_dp.emb.weight',\n",
    "    '0.rnns.0.weight_hh_l0_raw': 'multimode.0.weight_hh_l0_raw',\n",
    "    '0.rnns.0.module.weight_ih_l0': 'multimode.0.module.weight_ih_l0',\n",
    "    '0.rnns.0.module.weight_hh_l0': 'multimode.0.module.weight_hh_l0',\n",
    "    '0.rnns.0.module.bias_ih_l0': 'multimode.0.module.bias_ih_l0',\n",
    "    '0.rnns.0.module.bias_hh_l0': 'multimode.0.module.bias_hh_l0',\n",
    "    '0.rnns.1.weight_hh_l0_raw': 'multimode.1.weight_hh_l0_raw',\n",
    "    '0.rnns.1.module.weight_ih_l0': 'multimode.1.module.weight_ih_l0',\n",
    "    '0.rnns.1.module.weight_hh_l0': 'multimode.1.module.weight_hh_l0',\n",
    "    '0.rnns.1.module.bias_ih_l0': 'multimode.1.module.bias_ih_l0',\n",
    "    '0.rnns.1.module.bias_hh_l0': 'multimode.1.module.bias_hh_l0', \n",
    "    '0.rnns.2.weight_hh_l0_raw': 'multimode.2.weight_hh_l0_raw', \n",
    "    '0.rnns.2.module.weight_ih_l0': 'multimode.2.module.weight_ih_l0',\n",
    "    '0.rnns.2.module.weight_hh_l0': 'multimode.2.module.weight_hh_l0',\n",
    "    '0.rnns.2.module.bias_ih_l0': 'multimode.2.module.bias_ih_l0',\n",
    "    '0.rnns.2.module.bias_hh_l0': 'multimode.2.module.bias_hh_l0',\n",
    "    '1.decoder.weight': 'multidecoder.decoder.weight',\n",
    "    '1.decoder.bias': 'multidecoder.decoder.bias'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.encoder.weight encoder.weight\n",
      "0.encoder_dp.emb.weight encoder_dp.emb.weight\n",
      "0.rnns.0.weight_hh_l0_raw multimode.0.weight_hh_l0_raw\n",
      "0.rnns.0.module.weight_ih_l0 multimode.0.module.weight_ih_l0\n",
      "0.rnns.0.module.weight_hh_l0 multimode.0.module.weight_hh_l0\n",
      "0.rnns.0.module.bias_ih_l0 multimode.0.module.bias_ih_l0\n",
      "0.rnns.0.module.bias_hh_l0 multimode.0.module.bias_hh_l0\n",
      "0.rnns.1.weight_hh_l0_raw multimode.1.weight_hh_l0_raw\n",
      "0.rnns.1.module.weight_ih_l0 multimode.1.module.weight_ih_l0\n",
      "0.rnns.1.module.weight_hh_l0 multimode.1.module.weight_hh_l0\n",
      "0.rnns.1.module.bias_ih_l0 multimode.1.module.bias_ih_l0\n",
      "0.rnns.1.module.bias_hh_l0 multimode.1.module.bias_hh_l0\n",
      "0.rnns.2.weight_hh_l0_raw multimode.2.weight_hh_l0_raw\n",
      "0.rnns.2.module.weight_ih_l0 multimode.2.module.weight_ih_l0\n",
      "0.rnns.2.module.weight_hh_l0 multimode.2.module.weight_hh_l0\n",
      "0.rnns.2.module.bias_ih_l0 multimode.2.module.bias_ih_l0\n",
      "0.rnns.2.module.bias_hh_l0 multimode.2.module.bias_hh_l0\n",
      "1.decoder.weight multidecoder.decoder.weight\n",
      "1.decoder.bias multidecoder.decoder.bias\n"
     ]
    }
   ],
   "source": [
    "for k, v in PRETRAINED_TO_MULTI.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_weights(learner, pretrained_weights, pretrained_vocab, pretrained_to_multi):\n",
    "    \"\"\"\n",
    "    Special case of loading pretrained weights for multimodal model. \n",
    "    Does the following:\n",
    "        1. Convert encoder/decoder vocab weights\n",
    "        2. Map the names of the weights from the pretrained wikitext103 to multimodal\n",
    "        3. For the multimodal weights, random initialize and concatenate\n",
    "        4. Load Weights\n",
    "    \"\"\"\n",
    "    # 1)\n",
    "    old_itos = pickle.load(open(pretrained_vocab, 'rb'))\n",
    "    old_stoi = {v:k for k,v in enumerate(old_itos)}\n",
    "    wgts = torch.load(pretrained_weights, map_location=lambda storage, loc: storage)\n",
    "    wgts = convert_weights(wgts, old_stoi, learner.data.train_ds.vocab.itos)\n",
    "    \n",
    "    #2)\n",
    "    for k, v in pretrained_to_multi.items():\n",
    "        wgts[v] = wgts.pop(k)\n",
    "        \n",
    "    #3)\n",
    "    h1_dim = wgts['multimode.0.module.weight_ih_l0'].shape[0]\n",
    "    random_init = torch.randn(h1_dim, learner.model.audio_sz)\n",
    "    h1_z1 = torch.cat([wgts['multimode.0.module.weight_ih_l0'],\n",
    "                       random_init], dim=1)\n",
    "    wgts['multimode.0.module.weight_ih_l0'] = h1_z1\n",
    "    \n",
    "    #4)\n",
    "    learner.model.load_state_dict(wgts)\n",
    "    \n",
    "#     return random_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = map_weights(learn,\n",
    "            original_learn.path/original_learn.model_dir/'imdb-model-test.pth',\n",
    "            original_learn.path/original_learn.model_dir/'imdb-itos.pkl',\n",
    "            PRETRAINED_TO_MULTI)\n",
    "zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 15:11\n",
      "epoch  train_loss  valid_loss  accuracy\n",
      "1      6.305592    6.226836    0.058895  (15:11)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-550-2afe5836a35b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "{k:v.shape for (k,v) in zz.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4600, 400])"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz['multimode.0.module.weight_ih_l0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = next(multimodal_rnn.named_parameters())[1]\n",
    "zz.data = torch.zeros(20000, 400, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz = next(multimodal_rnn.named_parameters())[1]\n",
    "zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  42, 2435,    0,  ...,   16,   21,   27],\n",
       "         [  39,    9,   21,  ...,  819,    0,   11],\n",
       "         [4612,    0,    6,  ...,   13,   16,   80],\n",
       "         ...,\n",
       "         [   9,    8,    3,  ...,   22,    2,  727],\n",
       "         [   6,  404,   24,  ...,    5, 1913,   61],\n",
       "         [ 595,   10,   13,  ..., 1239,  219,   30]]),\n",
       " tensor([[[-0.8940, -0.9863,  1.0023,  ..., -0.0324, -1.4640,  1.0113],\n",
       "          [-0.8940, -0.9863,  1.0023,  ..., -0.0324, -1.4640,  1.0113],\n",
       "          [-0.8940, -0.9863,  1.0023,  ..., -0.0324, -1.4640,  1.0113],\n",
       "          ...,\n",
       "          [-0.8940, -0.9863,  1.0023,  ..., -0.0324, -1.4640,  1.0113],\n",
       "          [-0.8940, -0.9863,  1.0023,  ..., -0.0324, -1.4640,  1.0113],\n",
       "          [-0.8940, -0.9863,  1.0023,  ..., -0.0324, -1.4640,  1.0113]],\n",
       " \n",
       "         [[-0.8940, -0.9863,  1.0023,  ..., -0.0324, -1.4640,  1.0113],\n",
       "          [-0.8940, -0.9863,  1.0023,  ..., -0.0324, -1.4640,  1.0113],\n",
       "          [-0.8940, -0.9863,  1.0023,  ..., -0.0324, -1.4640,  1.0113],\n",
       "          ...,\n",
       "          [ 0.3504,  1.0622, -0.4503,  ..., -1.8137,  1.0830, -0.7404],\n",
       "          [ 0.3504,  1.0622, -0.4503,  ..., -1.8137,  1.0830, -0.7404],\n",
       "          [ 0.3504,  1.0622, -0.4503,  ..., -1.8137,  1.0830, -0.7404]],\n",
       " \n",
       "         [[ 0.3504,  1.0622, -0.4503,  ..., -1.8137,  1.0830, -0.7404],\n",
       "          [ 0.3504,  1.0622, -0.4503,  ..., -1.8137,  1.0830, -0.7404],\n",
       "          [ 0.3504,  1.0622, -0.4503,  ..., -1.8137,  1.0830, -0.7404],\n",
       "          ...,\n",
       "          [ 0.3504,  1.0622, -0.4503,  ..., -1.8137,  1.0830, -0.7404],\n",
       "          [ 0.3504,  1.0622, -0.4503,  ..., -1.8137,  1.0830, -0.7404],\n",
       "          [ 0.3504,  1.0622, -0.4503,  ..., -1.8137,  1.0830, -0.7404]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 1.0967,  1.5206,  2.3490,  ..., -0.1670,  0.4186, -0.0851],\n",
       "          [ 1.0967,  1.5206,  2.3490,  ..., -0.1670,  0.4186, -0.0851],\n",
       "          [ 1.0967,  1.5206,  2.3490,  ..., -0.1670,  0.4186, -0.0851],\n",
       "          ...,\n",
       "          [ 1.0967,  1.5206,  2.3490,  ..., -0.1670,  0.4186, -0.0851],\n",
       "          [ 1.0967,  1.5206,  2.3490,  ..., -0.1670,  0.4186, -0.0851],\n",
       "          [ 1.0967,  1.5206,  2.3490,  ..., -0.1670,  0.4186, -0.0851]],\n",
       " \n",
       "         [[ 1.0967,  1.5206,  2.3490,  ..., -0.1670,  0.4186, -0.0851],\n",
       "          [ 1.0967,  1.5206,  2.3490,  ..., -0.1670,  0.4186, -0.0851],\n",
       "          [ 1.0967,  1.5206,  2.3490,  ..., -0.1670,  0.4186, -0.0851],\n",
       "          ...,\n",
       "          [-0.6606, -3.4291,  1.1188,  ..., -0.0044,  0.4027,  2.3142],\n",
       "          [-0.6606, -3.4291,  1.1188,  ..., -0.0044,  0.4027,  2.3142],\n",
       "          [-0.6606, -3.4291,  1.1188,  ..., -0.0044,  0.4027,  2.3142]],\n",
       " \n",
       "         [[-0.6606, -3.4291,  1.1188,  ..., -0.0044,  0.4027,  2.3142],\n",
       "          [-0.6606, -3.4291,  1.1188,  ..., -0.0044,  0.4027,  2.3142],\n",
       "          [-0.6606, -3.4291,  1.1188,  ..., -0.0044,  0.4027,  2.3142],\n",
       "          ...,\n",
       "          [-0.6606, -3.4291,  1.1188,  ..., -0.0044,  0.4027,  2.3142],\n",
       "          [-0.6606, -3.4291,  1.1188,  ..., -0.0044,  0.4027,  2.3142],\n",
       "          [-0.6606, -3.4291,  1.1188,  ..., -0.0044,  0.4027,  2.3142]]]),\n",
       " tensor([  39,    9,   21,  ...,   10, 1182, 4604]))"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(multi_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[    1,     2,     3,     4, ...,     7,     8,     9,    10],\n",
       "        [   11,    12,    13,    14, ...,    17,    18,    19,    20],\n",
       "        [   21,    22,    23,    24, ...,    27,    28,    29,    30],\n",
       "        [   31,    32,    33,    34, ...,    37,    38,    39,    40],\n",
       "        ...,\n",
       "        [  601,   602,   603,   604, ...,   607,   608,   609,   610],\n",
       "        [  611,   612,   613,   614, ...,   617,   618,   619,   620],\n",
       "        [  621,   622,   623,   624, ...,   627,   628,   629,   630],\n",
       "        [  631,   632,   633,   634, ...,   637,   638,   639,   640]],\n",
       "\n",
       "       [[  641,   642,   643,   644, ...,   647,   648,   649,   650],\n",
       "        [  651,   652,   653,   654, ...,   657,   658,   659,   660],\n",
       "        [  661,   662,   663,   664, ...,   667,   668,   669,   670],\n",
       "        [  671,   672,   673,   674, ...,   677,   678,   679,   680],\n",
       "        ...,\n",
       "        [ 1241,  1242,  1243,  1244, ...,  1247,  1248,  1249,  1250],\n",
       "        [ 1251,  1252,  1253,  1254, ...,  1257,  1258,  1259,  1260],\n",
       "        [ 1261,  1262,  1263,  1264, ...,  1267,  1268,  1269,  1270],\n",
       "        [ 1271,  1272,  1273,  1274, ...,  1277,  1278,  1279,  1280]],\n",
       "\n",
       "       [[ 1281,  1282,  1283,  1284, ...,  1287,  1288,  1289,  1290],\n",
       "        [ 1291,  1292,  1293,  1294, ...,  1297,  1298,  1299,  1300],\n",
       "        [ 1301,  1302,  1303,  1304, ...,  1307,  1308,  1309,  1310],\n",
       "        [ 1311,  1312,  1313,  1314, ...,  1317,  1318,  1319,  1320],\n",
       "        ...,\n",
       "        [ 1881,  1882,  1883,  1884, ...,  1887,  1888,  1889,  1890],\n",
       "        [ 1891,  1892,  1893,  1894, ...,  1897,  1898,  1899,  1900],\n",
       "        [ 1901,  1902,  1903,  1904, ...,  1907,  1908,  1909,  1910],\n",
       "        [ 1911,  1912,  1913,  1914, ...,  1917,  1918,  1919,  1920]],\n",
       "\n",
       "       [[ 1921,  1922,  1923,  1924, ...,  1927,  1928,  1929,  1930],\n",
       "        [ 1931,  1932,  1933,  1934, ...,  1937,  1938,  1939,  1940],\n",
       "        [ 1941,  1942,  1943,  1944, ...,  1947,  1948,  1949,  1950],\n",
       "        [ 1951,  1952,  1953,  1954, ...,  1957,  1958,  1959,  1960],\n",
       "        ...,\n",
       "        [ 2521,  2522,  2523,  2524, ...,  2527,  2528,  2529,  2530],\n",
       "        [ 2531,  2532,  2533,  2534, ...,  2537,  2538,  2539,  2540],\n",
       "        [ 2541,  2542,  2543,  2544, ...,  2547,  2548,  2549,  2550],\n",
       "        [ 2551,  2552,  2553,  2554, ...,  2557,  2558,  2559,  2560]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[10241, 10242, 10243, 10244, ..., 10247, 10248, 10249, 10250],\n",
       "        [10251, 10252, 10253, 10254, ..., 10257, 10258, 10259, 10260],\n",
       "        [10261, 10262, 10263, 10264, ..., 10267, 10268, 10269, 10270],\n",
       "        [10271, 10272, 10273, 10274, ..., 10277, 10278, 10279, 10280],\n",
       "        ...,\n",
       "        [10841, 10842, 10843, 10844, ..., 10847, 10848, 10849, 10850],\n",
       "        [10851, 10852, 10853, 10854, ..., 10857, 10858, 10859, 10860],\n",
       "        [10861, 10862, 10863, 10864, ..., 10867, 10868, 10869, 10870],\n",
       "        [10871, 10872, 10873, 10874, ..., 10877, 10878, 10879, 10880]],\n",
       "\n",
       "       [[10881, 10882, 10883, 10884, ..., 10887, 10888, 10889, 10890],\n",
       "        [10891, 10892, 10893, 10894, ..., 10897, 10898, 10899, 10900],\n",
       "        [10901, 10902, 10903, 10904, ..., 10907, 10908, 10909, 10910],\n",
       "        [10911, 10912, 10913, 10914, ..., 10917, 10918, 10919, 10920],\n",
       "        ...,\n",
       "        [11481, 11482, 11483, 11484, ..., 11487, 11488, 11489, 11490],\n",
       "        [11491, 11492, 11493, 11494, ..., 11497, 11498, 11499, 11500],\n",
       "        [11501, 11502, 11503, 11504, ..., 11507, 11508, 11509, 11510],\n",
       "        [11511, 11512, 11513, 11514, ..., 11517, 11518, 11519, 11520]],\n",
       "\n",
       "       [[11521, 11522, 11523, 11524, ..., 11527, 11528, 11529, 11530],\n",
       "        [11531, 11532, 11533, 11534, ..., 11537, 11538, 11539, 11540],\n",
       "        [11541, 11542, 11543, 11544, ..., 11547, 11548, 11549, 11550],\n",
       "        [11551, 11552, 11553, 11554, ..., 11557, 11558, 11559, 11560],\n",
       "        ...,\n",
       "        [12121, 12122, 12123, 12124, ..., 12127, 12128, 12129, 12130],\n",
       "        [12131, 12132, 12133, 12134, ..., 12137, 12138, 12139, 12140],\n",
       "        [12141, 12142, 12143, 12144, ..., 12147, 12148, 12149, 12150],\n",
       "        [12151, 12152, 12153, 12154, ..., 12157, 12158, 12159, 12160]],\n",
       "\n",
       "       [[12161, 12162, 12163, 12164, ..., 12167, 12168, 12169, 12170],\n",
       "        [12171, 12172, 12173, 12174, ..., 12177, 12178, 12179, 12180],\n",
       "        [12181, 12182, 12183, 12184, ..., 12187, 12188, 12189, 12190],\n",
       "        [12191, 12192, 12193, 12194, ..., 12197, 12198, 12199, 12200],\n",
       "        ...,\n",
       "        [12761, 12762, 12763, 12764, ..., 12767, 12768, 12769, 12770],\n",
       "        [12771, 12772, 12773, 12774, ..., 12777, 12778, 12779, 12780],\n",
       "        [12781, 12782, 12783, 12784, ..., 12787, 12788, 12789, 12790],\n",
       "        [12791, 12792, 12793, 12794, ..., 12797, 12798, 12799, 12800]]])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = np.arange(1, (300*10*64)+1).reshape(300*64, 10)\n",
    "aa = aa.reshape(-1, 64, 10)\n",
    "aa[0:20, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(232543, 10)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new_array = np.empty((232543, 10))\n",
    "new_array = np.empty((0, 10))\n",
    "for song in zip(data_audio.data, tl):\n",
    "    features, song_length = song\n",
    "    new_array = np.append(new_array, np.tile(features, (song_length,1)), axis=0)\n",
    "\n",
    "new_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.893972, -0.986326,  1.002337,  1.44418 , ..., -0.701897, -0.032365, -1.464049,  1.011268],\n",
       "       [-0.893972, -0.986326,  1.002337,  1.44418 , ..., -0.701897, -0.032365, -1.464049,  1.011268],\n",
       "       [-0.893972, -0.986326,  1.002337,  1.44418 , ..., -0.701897, -0.032365, -1.464049,  1.011268],\n",
       "       [-0.893972, -0.986326,  1.002337,  1.44418 , ..., -0.701897, -0.032365, -1.464049,  1.011268],\n",
       "       ...,\n",
       "       [-0.893972, -0.986326,  1.002337,  1.44418 , ..., -0.701897, -0.032365, -1.464049,  1.011268],\n",
       "       [-0.893972, -0.986326,  1.002337,  1.44418 , ..., -0.701897, -0.032365, -1.464049,  1.011268],\n",
       "       [-0.893972, -0.986326,  1.002337,  1.44418 , ..., -0.701897, -0.032365, -1.464049,  1.011268],\n",
       "       [ 0.35043 ,  1.06222 , -0.450322, -0.047755, ..., -0.18382 , -1.813732,  1.08301 , -0.740351]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_array[:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.939716e-01, -4.775463e-02, -1.150684e+00, -2.796405e-01, ...,  8.273797e-01,  9.500533e-01, -8.594624e-01,\n",
       "        -1.653091e-01],\n",
       "       [-9.863259e-01, -4.900306e-01, -1.747391e+00, -5.196730e-01, ..., -2.519979e-01,  2.382013e+00,  4.387546e-01,\n",
       "        -4.724991e-01],\n",
       "       [ 1.002337e+00, -1.582357e-01,  7.213377e-01,  1.400212e+00, ..., -1.941280e+00,  1.537789e-01,  4.615265e-01,\n",
       "        -1.079530e-01],\n",
       "       [ 1.444180e+00, -1.838197e-01, -1.130803e-01,  4.847791e-01, ..., -9.453367e-01,  1.841860e+00,  1.183999e-01,\n",
       "        -2.690129e-01],\n",
       "       ...,\n",
       "       [-7.403514e-01,  3.171212e-01,  5.168427e-01, -1.010780e+00, ...,  1.093102e+00,  6.304512e-01, -5.211421e-01,\n",
       "        -4.872220e-01],\n",
       "       [ 3.504297e-01,  1.105425e+00,  4.489977e-01, -1.510797e+00, ...,  2.343389e-01, -8.119963e-01, -2.440054e+00,\n",
       "         1.993736e-01],\n",
       "       [ 1.062220e+00,  8.832193e-01, -2.157157e+00,  3.181909e-01, ..., -8.850293e-01,  1.482015e+00,  1.277619e+00,\n",
       "         1.404804e+00],\n",
       "       [-4.503223e-01, -1.416715e+00,  5.557092e-01,  1.243965e+00, ..., -4.281627e-01,  1.651247e+00, -4.945371e-01,\n",
       "         1.832474e-03]])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_array[:3633*64].reshape(64, 10, -1).T[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3633"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(data_audio.text_length) // 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = np.concatenate(text_data.ids)\n",
    "nb = dd.shape[0] // 64\n",
    "dt = np.array(dd[:nb*64]).reshape(64, -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(dt)-1) // 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  42,   39, 4612,   18, ...,  489,  243,  146,    2])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  42, 2435,    0,   85, ...,   82,   16,   21,   27],\n",
       "       [  39,    9,   21,   14, ...,    6,  819,    0,   11],\n",
       "       [4612,    0,    6,   25, ...,   22,   13,   16,   80],\n",
       "       [  18,   59, 1581,    4, ...,    3,   23, 2692,    0],\n",
       "       ...,\n",
       "       [ 489,  100,   12,  849, ...,   10,   50,  905, 1216],\n",
       "       [ 243,   36,  163,  140, ...,    3,   17,   61,   47],\n",
       "       [ 146,   99,   17,   13, ...,   19,   93,   30,    6],\n",
       "       [   2,    2,  381,   32, ...,   10, 1336,    0,  979]])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-b599941f46fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'text_length' is not defined"
     ]
    }
   ],
   "source": [
    "text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([10, 20, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
