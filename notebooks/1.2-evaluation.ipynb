{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.translate import bleu_score\n",
    "import nltk.tokenize\n",
    "\n",
    "#from fastai import *\n",
    "#from fastai.text import *\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import pronouncing\n",
    "import glob\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "import requests\n",
    "\n",
    "from copy import copy, deepcopy\n",
    "from enum import Enum\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bleu_reference():\n",
    "    \"\"\"\n",
    "    Retrieve preprocessor from google cloud storage\n",
    "    \"\"\"\n",
    "    REFERENCE_URL = 'https://storage.googleapis.com/w210-capstone/lyrics/reference/4.1-LM-108k-lines-validation-tokens_100.pkl'\n",
    "    ref = requests.get(REFERENCE_URL)\n",
    "    ref = ref.content\n",
    "    ref = pickle.loads(ref)\n",
    "    return ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def combine_contraction(token_list, sign=\"'\"):\n",
    "    \"\"\"\n",
    "    combine sequent items in a list that compose a single contraction. By default look for apostrophe as signal\n",
    "    \n",
    "    Input Example:\n",
    "    --------------\n",
    "    ['xbos', 'xgenre', 'death', 'metal', 'xtitle', 'and', 'i', 'don', \"'t\",\n",
    "     'think', 'that', 'xbol-1', 'today', 'is', 'the', 'greatest', 'day', 'ever', 'xeol',\n",
    "     'xbol-2', 'so', 'what', 'never', 'xeol', 'xeos']\n",
    "     \n",
    "    Output Example:\n",
    "    ---------------\n",
    "    ['xbos', 'xgenre', 'death', 'metal', 'xtitle', 'and', 'i', \"don't\",\n",
    "     'think', 'that', 'xbol-1', 'today', 'is', 'the', 'greatest', 'day', 'ever', 'xeol',\n",
    "     'xbol-2', 'so', 'what', 'never', 'xeol', 'xeos']\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        newList= []\n",
    "        for token in token_list:\n",
    "            if not newList:\n",
    "                newList.append(token)\n",
    "            elif not str(token).startswith(sign):\n",
    "                newList.append(token)\n",
    "            else:\n",
    "                prior = newList.pop()\n",
    "                newList.append(prior+token)\n",
    "    except:\n",
    "        newList = token_list\n",
    "    return newList\n",
    "\n",
    "\n",
    "def parse_tokens(tokens, lines=True, tags=False, contraction=False):\n",
    "    \"\"\"\n",
    "    Parses tokens with various options for evaluation methods.\n",
    "    Assumes `xbol-1` tag as first line of actual lyrics.\n",
    "    \n",
    "    Input Example:\n",
    "    --------------\n",
    "    ['xbos', 'xgenre', 'death', 'metal', 'xtitle', 'and', 'i', 'don', \"'t\",\n",
    "     'think', 'that', 'xbol-1', 'today', 'is', 'the', 'greatest', 'day', 'ever', 'xeol',\n",
    "     'xbol-2', 'so', 'what', 'never', 'xeol', 'xeos']\n",
    "          \n",
    "    Output Example:\n",
    "    ---------------\n",
    "    `lines=True` and `tags=False`:\n",
    "        [['today', 'is', 'the', 'greatest', 'day', 'ever'],\n",
    "         ['so', 'what', 'never']]\n",
    "         \n",
    "          \n",
    "    \"\"\"\n",
    "    # lines and no tags\n",
    "    if contraction:\n",
    "        tokens = combine_contraction(tokens)\n",
    "    \n",
    "    if tokens and not tokens[-1] == 'xeos':\n",
    "        tokens.append('xeos')\n",
    "    \n",
    "    if lines and not tags:\n",
    "        reached_bol = False\n",
    "        parsed_tokens = []\n",
    "        for w in tokens:\n",
    "            if w == 'xbol-1':\n",
    "                reached_bol = True\n",
    "                current_line = []\n",
    "                continue\n",
    "\n",
    "            if not reached_bol:\n",
    "                continue\n",
    "\n",
    "            if 'xbol' in w:\n",
    "                parsed_tokens.append(current_line)\n",
    "                current_line = []\n",
    "                continue\n",
    "\n",
    "            elif w not in ['xeol', 'xeos']:\n",
    "                current_line.append(w)\n",
    "\n",
    "            elif w == 'xeos':\n",
    "                parsed_tokens.append(current_line)\n",
    "                break\n",
    "                \n",
    "    # no lines and no tags:\n",
    "    if not lines and not tags:\n",
    "        reached_bol = False\n",
    "        parsed_tokens = []\n",
    "        for w in tokens:\n",
    "            if w == 'xbol-1':\n",
    "                reached_bol = True\n",
    "                continue\n",
    "                \n",
    "            if not reached_bol:\n",
    "                continue\n",
    "                \n",
    "            if 'xbol' in w:\n",
    "                continue\n",
    "                \n",
    "            elif w not in ['xbol', 'xeol', 'xeos']:\n",
    "                parsed_tokens.append(w)\n",
    "                \n",
    "            elif w == 'xeos':\n",
    "                break\n",
    "                \n",
    "    # no lines and tags:\n",
    "    if not lines and tags:\n",
    "        reached_bol = False\n",
    "        parsed_tokens = []\n",
    "        for w in tokens:\n",
    "            if w == 'xbol-1':\n",
    "                reached_bol = True\n",
    "                parsed_tokens.append('xbol')\n",
    "                continue\n",
    "                \n",
    "            if not reached_bol:\n",
    "                continue\n",
    "                \n",
    "            if 'xbol' in w:\n",
    "                parsed_tokens.append('xbol')\n",
    "                continue\n",
    "                \n",
    "            elif w == 'xeos':\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                parsed_tokens.append(w)\n",
    "                \n",
    "    # lines and tags (not necessary):\n",
    "    if lines and tags:\n",
    "        raise Exception(f'Combination of lines=True and tags=True is not implemented')\n",
    "        \n",
    "    return parsed_tokens\n",
    "\n",
    "def calculate_rhyme_density(tokens, rhymeType='perfect', rhymeLocation='all'):\n",
    "    \"\"\"\n",
    "    Computes rhyme density for a list of tokens\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    rhymeType : str\n",
    "        - 'perfect' is a perfect rhyme\n",
    "        - 'stressed' is a rhyming in the vowel sound + stress only\n",
    "        - 'allVowels' is a rhyming at all vowel syllables\n",
    "        \n",
    "    rhymeLocation : str\n",
    "        choose to look at 'all' text or 'end' (last word in each line)\n",
    "    \"\"\"\n",
    "    \n",
    "    assert rhymeType in ['perfect', 'stressed', 'allVowels'], \"Unexpected value for rhymeType\"\n",
    "    assert rhymeLocation in ['all', 'end'], \"Unexpected value for rhymeLocation\"\n",
    "    \n",
    "    rhymePart_cnt = Counter()\n",
    "    rhyme_cnt = 0\n",
    "    distinct_rhyme_cnt = 0\n",
    "    \n",
    "    if rhymeLocation == 'all':\n",
    "        tokens = parse_tokens(tokens, lines=False, tags=False, contraction=True)\n",
    "        \n",
    "    elif rhymeLocation == 'end':\n",
    "        tokens = [line[-1] for line in parse_tokens(tokens, lines=True, tags=False, contraction=True)\\\n",
    "                  if line]\n",
    "        \n",
    "    # only retrieve first pronunciation from `phones_for_words`\n",
    "    # we can enhance here by doing permutations of pronunciations\n",
    "    pros = [pronouncing.phones_for_word(token)[0] for token in tokens\\\n",
    "            if pronouncing.phones_for_word(token)]\n",
    "    for pro in pros:\n",
    "        if rhymeType == 'perfect':\n",
    "            rhymePart_cnt[pronouncing.rhyming_part(pro)] += 1\n",
    "        elif rhymeType == 'stressed':\n",
    "            # look at only stressed syllables\n",
    "            # slightly modified logic from JP implementation\n",
    "            rhyming_parts = pronouncing.rhyming_part(pro).split()\n",
    "            if rhyming_parts:\n",
    "                rhyming_parts = [part for part in rhyming_parts if part[-1] in ['1', '2']]\n",
    "            if rhyming_parts:\n",
    "                rhyming_parts = rhyming_parts[0]\n",
    "            else:\n",
    "                continue\n",
    "            rhymePart_cnt[rhyming_parts] += 1\n",
    "        elif rhymeType == 'allVowels':\n",
    "            # look at all vowel parts - new method\n",
    "            rhyming_parts = pronouncing.rhyming_part(pro).split()\n",
    "            rhyming_parts = [part for part in rhyming_parts if part[-1].isdigit()]\n",
    "            for rhyme in rhyming_parts:\n",
    "                rhymePart_cnt[rhyme] += 1\n",
    "\n",
    "    for v in rhymePart_cnt.values():\n",
    "        rhyme_cnt += v-1\n",
    "            \n",
    "    # denominator - word for 'perfect'; vowel syllables for 'vowel'\n",
    "    # denominator = sum(rhymePart_cnt.values())-1\n",
    "    denominator = len(tokens)-1\n",
    "    \n",
    "    if denominator > 0:\n",
    "        rhymeDensity = rhyme_cnt / denominator\n",
    "    else:\n",
    "        rhymeDensity = None\n",
    "            \n",
    "        \n",
    "#     return tokens, pros, rhymePart_cnt, rhyming_parts, rhyme_cnt, rhymeDensity\n",
    "    return rhymeDensity\n",
    "    \n",
    "def bleu(tokens, ref_list, nGram=4, nGramType='cumulative', shouldSmooth=True):\n",
    "    '''\n",
    "    calculates BLEU score\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens: list\n",
    "        the new token list that represents the string that needs to be scored\n",
    "    ref_list: list\n",
    "        expects a list of reference texts to compare (as strings)\n",
    "    nGram: int\n",
    "        choose between 1-4.  Determines which ngram(s) to use in the scoring\n",
    "    nGramType: string\n",
    "        'cumulative' uses a simple average of all ngrams from 1 to nGram. 'exclusive' is the chosen nGram only.\n",
    "    shouldSmooth: boolean\n",
    "        if False, calculates the BLEU score without smoothing. Recommended to use smoothing (set to True)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score: BLEU score using nGram settings input, smoothed by default (can be turned off)\n",
    "    '''\n",
    "\n",
    "    # basic checks\n",
    "    if nGram not in [1,2,3,4]:\n",
    "        raise ValueError('nGram must be between 1 and 4')\n",
    "\n",
    "    if nGramType not in ['cumulative','exclusive']:\n",
    "        raise ValueError('nGramType must either be cumulative (average of nGrams less than n) or exclusive (1=unigram, etc.)')\n",
    "\n",
    "    # pre-score\n",
    "    weight_dict = {('cumulative',1):(1,0,0,0)\n",
    "                  ,('cumulative',2):(.5,.5,0,0)\n",
    "                  ,('cumulative',3):(.3333,.3333,.3333,0)\n",
    "                  ,('cumulative',4):(.25,.25,.25,.25)\n",
    "                  ,('exclusive',1):(1,0,0,0)\n",
    "                  ,('exclusive',2):(0,1,0,0)\n",
    "                  ,('exclusive',3):(0,0,1,0)\n",
    "                  ,('exclusive',4):(0,0,0,1)}\n",
    "\n",
    "    candidate = parse_tokens(tokens, lines=False, tags=False, contraction=True)\n",
    "    references = [parse_tokens(r, lines=False, tags=False, contraction=True) for r in ref_list]\n",
    "\n",
    "    weights = weight_dict[(nGramType,nGram)]\n",
    "\n",
    "    # scoring\n",
    "    if shouldSmooth:\n",
    "        smoother = bleu_score.SmoothingFunction().method5\n",
    "    else:\n",
    "        smoother = bleu_score.SmoothingFunction().method1\n",
    "    score = bleu_score.sentence_bleu(references, candidate, weights, smoothing_function=smoother)\n",
    "    return score\n",
    "\n",
    "def findLineStress(tokenized_line):\n",
    "    '''\n",
    "    find accentual stress of a given tokenized line, based on CMU dict.\n",
    "    Uses relative stress per word, so somewhat limited.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenized_line : list\n",
    "        list of tokens from line, usually preprocessed to remove non-words\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    parselist: list of potential stresses after parsing.\n",
    "        0 is unstressed, 1 is primary stress, 2 is secondary stress (middle)\n",
    "    '''\n",
    "    \n",
    "    parses = ['']\n",
    "    for word in tokenized_line:\n",
    "        pros = pronouncing.phones_for_word(word)\n",
    "        if pros:\n",
    "            for phonelist in [pronouncing.phones_for_word(word)]:\n",
    "                stressOptions = deepcopy(parses)\n",
    "                currLen = len(parses)\n",
    "                newparse = []\n",
    "                # I don't really need to loop through pronunciations\n",
    "                # just distinct stress patterns, so a little inefficient here\n",
    "                for pronunciation in phonelist:\n",
    "                    wordStress = pronouncing.stresses(pronunciation)\n",
    "                    for option in range(currLen):\n",
    "                        newparse.append(''+str(stressOptions[option]) + str(wordStress))\n",
    "            parses = newparse\n",
    "\n",
    "    return list(set(parses))\n",
    "\n",
    "def levenshtein(s1, s2):\n",
    "    '''calculate levenshtein distance for two input strings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s1: string\n",
    "        first string for comparison\n",
    "    s2: string\n",
    "        second string for comparison\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    distance: levenshtein distance between two strings...that is,\n",
    "    the lowest number of modifications to turn s1 into s2\n",
    "    '''\n",
    "    s1 = str(s1)\n",
    "    s2 = str(s2)\n",
    "\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein(s2, s1)\n",
    "\n",
    "    # otherwise len(s1) >= len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer\n",
    "            deletions = current_row[j] + 1       # than s2\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]\n",
    "    \n",
    "def findMeter(tokens):\n",
    "    '''finds meter with smallest edit distance\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    token_list : list\n",
    "        list of tokens making up a song\n",
    "\n",
    "    Result\n",
    "    ------\n",
    "    Updates attributes:\n",
    "        edits_per_line: average lowest edit distance per line for any standard accentual-syllabic verse\n",
    "        options: list of potential meters for the lowest edit distance\n",
    "    '''\n",
    "\n",
    "    # define\n",
    "    meter_dict = {'0101':'Iambic dimeter'\n",
    "                  ,'010101':'Iambic trimeter'\n",
    "                  ,'01010101':'Iambic tetrameter'\n",
    "                  ,'0101010101':'Iambic pentameter'\n",
    "                  ,'010101010101':'Iambic hexameter'\n",
    "                  ,'01010101010101':'Iambic heptameter'\n",
    "                  ,'0101010101010101':'Iambic octameter'\n",
    "                  ,'1010':'Trochaic dimeter'\n",
    "                  ,'101010':'Trochaic trimeter'\n",
    "                  ,'10101010':'Trochaic tetrameter'\n",
    "                  ,'1010101010':'Trochaic pentameter'\n",
    "                  ,'101010101010':'Trochaic hexameter'\n",
    "                  ,'10101010101010':'Trochaic heptameter'\n",
    "                  ,'1010101010101010':'Trochaic octameter'\n",
    "                  ,'001001':'Anapestic dimeter'\n",
    "                  ,'001001001':'Anapestic trimeter'\n",
    "                  ,'001001001001':'Anapestic tetrameter'\n",
    "                  ,'001001001001001':'Anapestic pentameter'\n",
    "                  ,'001001001001001001':'Anapestic hexameter'\n",
    "                  ,'001001001001001001001':'Anapestic heptameter'\n",
    "                  ,'100100':'Dactyllic dimeter'\n",
    "                  ,'100100100':'Dactyllic trimeter'\n",
    "                  ,'100100100100':'Dactyllic tetrameter'\n",
    "                  ,'100100100100100':'Dactyllic pentameter'\n",
    "                  ,'100100100100100100':'Dactyllic hexameter'\n",
    "                  ,'100100100100100100100':'Dactyllic heptameter'}\n",
    "\n",
    "    # initialize\n",
    "    vote_cnt = Counter()\n",
    "    try:\n",
    "        lines = parse_tokens(tokens, lines=True, tags=False, contraction=True)\n",
    "        line_cnt = len(lines)\n",
    "        minDist = 999\n",
    "\n",
    "        # update distances\n",
    "        for line in lines:\n",
    "            for k,v in meter_dict.items():\n",
    "                minDist = 999\n",
    "                for reading in findLineStress(line):\n",
    "                    dist = levenshtein(k,reading)\n",
    "                    if dist < minDist:\n",
    "                        minDist = dist\n",
    "                vote_cnt[v] += minDist\n",
    "\n",
    "        lowest = min(vote_cnt.values())\n",
    "        options = [k for k,v in vote_cnt.items() if v==lowest]\n",
    "        editsPerLine = lowest/float(line_cnt)\n",
    "    \n",
    "    except:\n",
    "        options = None\n",
    "        editsPerLine = None\n",
    "    \n",
    "    finally:\n",
    "        # use set_metric\n",
    "        return options, editsPerLine\n",
    "\n",
    "def get_POS_conformity(tokens):\n",
    "    \"\"\"\n",
    "    Calculates absolute difference from published proportions of POS, between 0 and 1.\n",
    "\n",
    "    \"\"\"\n",
    "    # define lookups\n",
    "    mapping = {'CC':'CC','DT':'DT','PDT':'DT','WDT':'DT','IN':'IN','JJ':'JJ','JJR':'JJ','JJS':'JJ'\n",
    "               ,'NN':'NN','NNS':'NN','NNP':'NN','NNPS':'NN','LS':'OT','CD':'OT','EX':'OT','FW':'OT'\n",
    "               ,'POS':'OT','UH':'OT','RB':'RB','RBR':'RB','RBS':'RB','WRB':'RB','TO':'TO','MD':'VB'\n",
    "               ,'RP':'VB','VB':'VB','VBD':'VB','VBG':'VB','VBN':'VB','VBP':'VB','VBZ':'VB','PRP':'WP'\n",
    "               ,'PRP$':'WP','WP':'WP','WP$':'WP'}\n",
    "    comp_dict = {'CC':0.0212,'DT':0.0982,'IN':0.0998,'JJ':0.0613,'NN':0.3051,'RB':0.0766,'TO':0.0351\n",
    "                 ,'VB':0.285,'WP':0.0058,'OT':0.012}\n",
    "\n",
    "    # initialize\n",
    "    pos_cnt = Counter()\n",
    "    total_word_cnt = 0\n",
    "    pos_dict = defaultdict(float)\n",
    "    pos_dict['adjustment'] = 0\n",
    "    absdiff = 0\n",
    "\n",
    "    # prepare data\n",
    "    tokenized_text = parse_tokens(tokens, lines=False, tags=False, contraction=False)\n",
    "    tag_list = nltk.pos_tag(tokenized_text)\n",
    "\n",
    "    # initial proportions\n",
    "    for t in tag_list:\n",
    "        pos_cnt[t[1]] +=1\n",
    "        total_word_cnt +=1\n",
    "    pos_raw_dict = {k: v/float(total_word_cnt) for k,v in dict(pos_cnt).items()}\n",
    "\n",
    "    # adjust for items missing in mapping (mostly punctuation)\n",
    "    for k,v in pos_raw_dict.items():\n",
    "        if k in mapping:\n",
    "            pos_dict[mapping[k]] += v\n",
    "        else:\n",
    "            pos_dict['adjustment'] += v\n",
    "    for k,v in pos_dict.items():\n",
    "        pos_dict[k] = pos_dict[k]/(1-pos_dict['adjustment'])\n",
    "    del pos_dict['adjustment']\n",
    "\n",
    "    # compare to observed ratios, calculate absolute difference\n",
    "    for k in comp_dict.keys():\n",
    "        absdiff += abs(comp_dict[k] - pos_dict.get(k,0))\n",
    "\n",
    "    # use set_metric\n",
    "    return absdiff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "DIR = '../data/json/batch-01'\n",
    "DIR2 = '../data/json/batch-02'\n",
    "\n",
    "def open_json(file):\n",
    "    if not file.split('/')[-1].startswith('.'):\n",
    "        with open(file) as f:\n",
    "            xx = json.load(f)\n",
    "        return xx\n",
    "    \n",
    "def flatten(d, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, collections.MutableMapping):\n",
    "            items.extend(flatten(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "def read_scores_into_list(directory):\n",
    "    batch_list = [open_json(os.path.join(directory, file)) for file in os.listdir(directory)]\n",
    "    flattened = [flatten(b) for b in batch_list if b]\n",
    "    return flattened\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_scores(flat_dict,bleu_ref=get_bleu_reference()):\n",
    "    tokens = flat_dict['lyric']\n",
    "\n",
    "    #findMeter\n",
    "    closestMeters, editsPerLine = findMeter(tokens)\n",
    "    flat_dict['metrics_closestMeters'] = closestMeters\n",
    "    flat_dict['metrics_editsPerLine'] = editsPerLine\n",
    "\n",
    "    #POS\n",
    "    flat_dict['metrics_POS_conformity'] = get_POS_conformity(tokens)\n",
    "\n",
    "    #rhyme\n",
    "    flat_dict['metrics_rhymeDensityAP'] = calculate_rhyme_density(tokens, rhymeType='perfect',   rhymeLocation='all')\n",
    "    flat_dict['metrics_rhymeDensityAV'] = calculate_rhyme_density(tokens, rhymeType='allVowels', rhymeLocation='all')\n",
    "    flat_dict['metrics_rhymeDensityAS'] = calculate_rhyme_density(tokens, rhymeType='stressed',  rhymeLocation='all')\n",
    "    flat_dict['metrics_rhymeDensityEP'] = calculate_rhyme_density(tokens, rhymeType='perfect',   rhymeLocation='end')\n",
    "    flat_dict['metrics_rhymeDensityEV'] = calculate_rhyme_density(tokens, rhymeType='allVowels', rhymeLocation='end')\n",
    "    flat_dict['metrics_rhymeDensityES'] = calculate_rhyme_density(tokens, rhymeType='stressed',  rhymeLocation='end')\n",
    "\n",
    "    #BLEU\n",
    "    flat_dict['metrics_BLEU_1_excl_Unsmoothed'] = bleu(tokens, bleu_ref, nGram=1, nGramType='exclusive', shouldSmooth=False)\n",
    "    flat_dict['metrics_BLEU_2_excl_Unsmoothed'] = bleu(tokens, bleu_ref, nGram=2, nGramType='exclusive', shouldSmooth=False)\n",
    "    flat_dict['metrics_BLEU_3_excl_Unsmoothed'] = bleu(tokens, bleu_ref, nGram=3, nGramType='exclusive', shouldSmooth=False)\n",
    "    flat_dict['metrics_BLEU_4_excl_Unsmoothed'] = bleu(tokens, bleu_ref, nGram=4, nGramType='exclusive', shouldSmooth=False)\n",
    "    flat_dict['metrics_BLEU_3_cumul_Smoothed']  = bleu(tokens, bleu_ref, nGram=3, nGramType='cumulative', shouldSmooth=True)\n",
    "    flat_dict['metrics_BLEU_4_cumul_Smoothed']  = bleu(tokens, bleu_ref, nGram=4, nGramType='cumulative', shouldSmooth=True)\n",
    "    \n",
    "    return flat_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read batch-1\n",
    "x = read_scores_into_list(DIR)\n",
    "df = pd.DataFrame(update_scores(x))\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/scores/batch1.csv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read batch-2\n",
    "x = read_scores_into_list(DIR2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [update_scores(a) for a in x[0:1000] if a]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = [update_scores(a) for a in x[1000:2000] if a]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x3 = [update_scores(a) for a in x[2000:3000] if a]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x4 = [update_scores(a) for a in x[3000:4000] if a]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x5 = [update_scores(a) for a in x[4000:5000] if a]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x6 = [update_scores(a) for a in x[5000:6000] if a]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x7 = [update_scores(a) for a in x[6000:] if a]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = update_scores(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame()\n",
    "df2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6426\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lyric': ['xbos',\n",
       "  'xgenre',\n",
       "  'pop',\n",
       "  'xtitle',\n",
       "  'wasted',\n",
       "  'time',\n",
       "  'xbol-1',\n",
       "  'every',\n",
       "  'time',\n",
       "  'i',\n",
       "  'see',\n",
       "  'you',\n",
       "  'xeol',\n",
       "  'xbol-2',\n",
       "  'every',\n",
       "  'time',\n",
       "  'you',\n",
       "  'go',\n",
       "  'xeol',\n",
       "  'xbol-3',\n",
       "  'i',\n",
       "  'don',\n",
       "  \"'t\",\n",
       "  'know',\n",
       "  'why',\n",
       "  ',',\n",
       "  'i',\n",
       "  'don',\n",
       "  \"'t\",\n",
       "  'know',\n",
       "  'why',\n",
       "  'xeol',\n",
       "  'xbol-4',\n",
       "  'every',\n",
       "  'time',\n",
       "  'you',\n",
       "  'leave',\n",
       "  'xeol',\n",
       "  'xbol-5',\n",
       "  'xeol',\n",
       "  'xbol-6',\n",
       "  'every',\n",
       "  'time',\n",
       "  'i',\n",
       "  'see',\n",
       "  'your',\n",
       "  'face',\n",
       "  'xeol',\n",
       "  'xbol-7',\n",
       "  'every',\n",
       "  'time',\n",
       "  'i',\n",
       "  'see',\n",
       "  'your',\n",
       "  'face',\n",
       "  'xeol',\n",
       "  'xbol-8',\n",
       "  'xeol',\n",
       "  'xbol-9',\n",
       "  'every',\n",
       "  'time',\n",
       "  'i',\n",
       "  'see',\n",
       "  'your',\n",
       "  'face',\n",
       "  'xeol',\n",
       "  'xbol-10',\n",
       "  'every',\n",
       "  'time',\n",
       "  'i',\n",
       "  'see',\n",
       "  'your',\n",
       "  'face',\n",
       "  'xeol',\n",
       "  'xbol-11',\n",
       "  'every',\n",
       "  'time',\n",
       "  'i',\n",
       "  'see',\n",
       "  'your',\n",
       "  'face',\n",
       "  'xeol',\n",
       "  'xbol-12',\n",
       "  'every'],\n",
       " 'meta_GPU': True,\n",
       " 'meta_audio': None,\n",
       " 'meta_beam_width': 3,\n",
       " 'meta_context_length': 40,\n",
       " 'meta_genre': 'pop',\n",
       " 'meta_max_len': 80,\n",
       " 'meta_model_name': '4.2-LM-108k-lines-genre-song_title',\n",
       " 'meta_model_type': 'language',\n",
       " 'meta_multinomial': True,\n",
       " 'meta_seed_text': 'xbos',\n",
       " 'meta_temperature': 1.2,\n",
       " 'meta_title': None,\n",
       " 'meta_top_k': 3,\n",
       " 'meta_verbose': 0,\n",
       " 'metrics_BLEU_1_excl_Unsmoothed': 0.8909090909090909,\n",
       " 'metrics_BLEU_2_excl_Unsmoothed': 0.48148148148148145,\n",
       " 'metrics_BLEU_3_cumul_Smoothed': 0.5218244960655852,\n",
       " 'metrics_BLEU_3_excl_Unsmoothed': 0.09433962264150943,\n",
       " 'metrics_BLEU_4_cumul_Smoothed': 0.3432713328118058,\n",
       " 'metrics_BLEU_4_excl_Unsmoothed': 0.0576923076923077,\n",
       " 'metrics_POS_conformity': 0.6084502572898799,\n",
       " 'metrics_closestMeters': ['Iambic dimeter', 'Trochaic dimeter'],\n",
       " 'metrics_editsPerLine': 184.0909090909091,\n",
       " 'metrics_rhymeDensityAP': 0.7777777777777778,\n",
       " 'metrics_rhymeDensityAS': 0.8148148148148148,\n",
       " 'metrics_rhymeDensityAV': 1.1111111111111112,\n",
       " 'metrics_rhymeDensityEP': 0.5,\n",
       " 'metrics_rhymeDensityES': 0.5,\n",
       " 'metrics_rhymeDensityEV': 0.5}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(x))\n",
    "print()\n",
    "x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('../data/scores/batch2.csv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1544303859'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(DIR)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xbos xgenre pop xtitle dear god xbol-1 [verse-1] \n",
      " xbol-2 it 's been a long time \n",
      " xbol-3 since i had to leave you \n",
      " xbol-4 i 've been saying \n",
      " xbol-5 that i 've been doing too much \n",
      " xbol-6 but i can\n"
     ]
    }
   ],
   "source": [
    "#print(' '.join(df['lyric'].head(1)[0]))\n",
    "samp1_tokenized = df['lyric'].head(1)[0]\n",
    "samp1 = ' '.join(df['lyric'].head(1)[0])\n",
    "print(samp1.replace('xeol','\\n'))\n",
    "#samp1_tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xbos xgenre pop xtitle can 't get enough of your love xbol-1 i can 't get enough of your love \n",
      " xbol-2 i can 't get enough of your love \n",
      " xbol-3 can 't get enough of your love \n",
      " xbol-4 i can 't\n"
     ]
    }
   ],
   "source": [
    "samp2_tokenized = df['lyric'].iloc[[1]][1]\n",
    "samp2 = ' '.join(df['lyric'].iloc[[1]][1])\n",
    "print(samp2.replace('xeol','\\n'))\n",
    "#samp2_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4091 0.0952 0.005 0.0053 0.2333 0.1363\n"
     ]
    }
   ],
   "source": [
    "reference_dir = '../data/lyrics/reference/'\n",
    "refs = glob.glob(reference_dir+'*.txt')\n",
    "ref_list = []\n",
    "\n",
    "## add BLEU reference code\n",
    "for ref in refs:\n",
    "    with open(ref) as rf:\n",
    "        ref_raw_text = rf.read()\n",
    "        ref_list.append([token for token in ref_raw_text.split()])\n",
    "\n",
    "text = samp2_tokenized\n",
    "#print(text)\n",
    "#print()\n",
    "#print(ref_list)\n",
    "print(round(bleu(text,ref_list,nGram=1,nGramType='exclusive',shouldSmooth=False),4)\n",
    "                            ,round(bleu(text,ref_list,nGram=2,nGramType='exclusive',shouldSmooth=False),4)\n",
    "                            ,round(bleu(text,ref_list,nGram=3,nGramType='exclusive',shouldSmooth=False),4)\n",
    "                            ,round(bleu(text,ref_list,nGram=4,nGramType='exclusive',shouldSmooth=False),4)\n",
    "                            ,round(bleu(text,ref_list,nGram=3,nGramType='cumulative',shouldSmooth=True),4)\n",
    "                            ,round(bleu(text,ref_list,nGram=4,nGramType='cumulative',shouldSmooth=True),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
