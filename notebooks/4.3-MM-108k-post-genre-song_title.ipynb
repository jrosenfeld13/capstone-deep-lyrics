{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.tokenize\n",
    "import itertools\n",
    "import datetime\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "\n",
    "from copy import copy, deepcopy\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_collection.multimodal_data import AudioDataset, MultimodalDataLoader, map_weights\n",
    "from src.nlp.neural_model import MultiLinearDecoder, MultiModalPostRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definitions\n",
    "Define this notebooks \"initializer model\" (pretrained parameters and itos mapping) and the output model as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '4.3-MM-108k-post-genre-song_title'\n",
    "MODEL_PATH = Path(f'../data/models/{model_name}')\n",
    "MODEL_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "init_model_name = '4.2-LM-108k-lines-genre-song_title'\n",
    "INIT_MODEL_PATH = Path(f'../data/models/{init_model_name}')\n",
    "INIT_MODEL_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and featurization\n",
    "To create the model's tokens with the correct train-test split, run the code below. Only needed once on the notebook's first ever run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FIRST_RUN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_lyrics_with_tags(lyrics, line_num=True, genre=False, song_title=False):\n",
    "    tk = nltk.tokenize.LineTokenizer(blanklines='keep')\n",
    "    tokd = tk.tokenize(lyrics)\n",
    "    \n",
    "    re_tk = nltk.tokenize.RegexpTokenizer(r'\\[[^\\]]+\\]|\\w+|[\\d\\.,]+|\\S+',\n",
    "                                          discard_empty=False)\n",
    "    re_tokd = re_tk.tokenize_sents(tokd)\n",
    "\n",
    "    if genre:\n",
    "        [s.insert(0, f'xGENRE') for s in re_tokd] # insert start token for each line\n",
    "\n",
    "    elif song_title:\n",
    "        [s.insert(0, f'xTITLE') for s in re_tokd] # insert start token for each line\n",
    "\n",
    "    else:\n",
    "        if line_num:\n",
    "            [s.insert(0, f'xBOL {line_num+1}') for line_num, s in enumerate(re_tokd)] # insert start token for each line\n",
    "        else:\n",
    "            [s.insert(0, f'xBOL') for s in re_tokd] # insert start token for each line\n",
    "\n",
    "        [s.append('xEOL') for s in re_tokd] # append end token for each line\n",
    "\n",
    "    flat = list(itertools.chain(*re_tokd))\n",
    "    # lower case and de-space\n",
    "    flat = [w.lower().replace(' ', '-') for w in flat]\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tokens(model_path):\n",
    "    '''\n",
    "    500k link: https://storage.googleapis.com/capstone-deep-lyrics/lyrics-500k.csv\n",
    "    108k link: https://storage.googleapis.com/w210-capstone/data/lyrics-valid.csv\n",
    "    '''\n",
    "    model_path = Path(MODEL_PATH)\n",
    "    model_path.mkdir(exist_ok=True)\n",
    "\n",
    "    small_corpus_url = 'https://storage.googleapis.com/w210-capstone/data/lyrics-valid.csv'\n",
    "    audio_url = 'https://storage.googleapis.com/w210-capstone/data/msd-aggregate.csv'\n",
    "    tag_url = 'https://storage.googleapis.com/w210-capstone/data/lyrics_tags.csv'\n",
    "\n",
    "    # load scraped data\n",
    "    df = pd.read_csv(small_corpus_url,\n",
    "                     header=None, escapechar='\\\\',\n",
    "                     names=['msd_id', 'lyrics'])\n",
    "\n",
    "    # only keep lyrics with length < 5000\n",
    "    df = df[df.lyrics.str.len() < 5000]\n",
    "\n",
    "    # bring in audio features\n",
    "    df_audio = pd.read_csv(audio_url)\n",
    "    df_audio.rename(columns={'track_id': 'msd_id'}, inplace=True)\n",
    "\n",
    "    # bring in tags\n",
    "    df_tags = pd.read_csv(tag_url, index_col=0)\n",
    "    df_tags.drop(columns=['lyrics'], inplace=True)\n",
    "\n",
    "    # match audio to keys from lyrics\n",
    "    df_audio = pd.merge(df, df_audio, how='inner', on='msd_id')\n",
    "\n",
    "    # mat\n",
    "    df_all = pd.merge(df_audio, df_tags, how='inner', on='msd_id')\n",
    "    df = df_all[['tag1', 'tag2', 'title', 'lyrics']]\n",
    "\n",
    "    print('Tokenizing...')\n",
    "    lyrics = df.lyrics.apply(tokenize_lyrics_with_tags, line_num=True)\n",
    "    genre = df.tag1.astype(str).apply(tokenize_lyrics_with_tags, line_num=False, genre=True)\n",
    "    song_title = df.title.apply(tokenize_lyrics_with_tags, line_num=False, song_title=True)\n",
    "\n",
    "    full_song = (['xbos'] + genre + song_title + lyrics + ['xeos'])\n",
    "\n",
    "    # split train/test\n",
    "    df_train, df_test = train_test_split(full_song, test_size=0.2, random_state=2018)\n",
    "\n",
    "    # tokens\n",
    "    print('Saving...')\n",
    "    tokens = np.array(df_train)\n",
    "    np.save(model_path/'train_tok.npy', tokens)\n",
    "\n",
    "    tokens = np.array(df_test)\n",
    "    np.save(model_path/'valid_tok.npy', tokens)\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(model_path):\n",
    "    '''\n",
    "    500k link: https://storage.googleapis.com/capstone-deep-lyrics/lyrics-500k.csv\n",
    "    108k link: https://storage.googleapis.com/w210-capstone/data/lyrics-valid.csv\n",
    "    '''\n",
    "    model_path = Path(model_path)\n",
    "    model_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    small_corpus_url = 'https://storage.googleapis.com/w210-capstone/data/lyrics-valid.csv'\n",
    "    audio_url = 'https://storage.googleapis.com/w210-capstone/data/msd-aggregate.csv'\n",
    "    tag_url = 'https://storage.googleapis.com/w210-capstone/data/lyrics_tags.csv'\n",
    "    \n",
    "    df = pd.read_csv(small_corpus_url,\n",
    "                 header=None, escapechar='\\\\',\n",
    "                 names=['msd_id', 'lyrics'])\n",
    "    \n",
    "    # only keep lyrics with length < 5000\n",
    "    df = df[df.lyrics.str.len() < 5000]\n",
    "    \n",
    "    # bring in audio features\n",
    "    df_audio = pd.read_csv(audio_url)\n",
    "    df_audio.rename(columns={'track_id': 'msd_id'}, inplace=True)\n",
    "    \n",
    "    # bring in tags\n",
    "    df_tags = pd.read_csv(tag_url, index_col=0)\n",
    "    df_tags.drop(columns=['lyrics'], inplace=True)\n",
    "    \n",
    "    # match audio to keys from lyrics\n",
    "    df_audio = pd.merge(df, df_audio, how='inner', on='msd_id')\n",
    "    \n",
    "    # mat\n",
    "    df_all = pd.merge(df_audio, df_tags, how='inner', on='msd_id')\n",
    "    \n",
    "    \n",
    "    # split data using same seed as lm_data_lyrics.py\n",
    "    df_train, df_valid = train_test_split(df_all,\n",
    "                                         test_size=0.2,\n",
    "                                         random_state=2018)\n",
    "    \n",
    "    df_train.to_csv(model_path/'df_train.csv')\n",
    "    df_valid.to_csv(model_path/'df_valid.csv')\n",
    "    \n",
    "    return df_train, df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "Saving...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "if MODEL_FIRST_RUN:\n",
    "    process_tokens(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_FIRST_RUN:\n",
    "    df_train, df_valid = process_audio(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_features(X):\n",
    "    return np.log(X)\n",
    "\n",
    "def bin_tempo(X):\n",
    "    '''\n",
    "    ref: https://en.wikipedia.org/wiki/Tempo#Italian_tempo_markings\n",
    "    These are rough loosely based on tempo markings above\n",
    "    Have considered both classical forms of music and popular\n",
    "    '''\n",
    "    assert X.shape[1] == 1, \"Only 1 column can be binned\"\n",
    "    bins = [0, 60, 76, 108, 120, 156, 176, 200, 500]\n",
    "    return pd.DataFrame(pd.cut(X.iloc[:,0], bins=bins))\n",
    "tempo_feat = FunctionTransformer(bin_tempo, validate=False)\n",
    "\n",
    "def bin_time_signature(X):\n",
    "    assert X.shape[1] == 1, \"Only 1 column can be binned\"\n",
    "    X['time_signature_bin'] = \"Other Signature\"\n",
    "    X.loc[X['time_signature'] == 4, 'time_signature_bin'] = '4/4 Signature'\n",
    "    X.loc[X['time_signature'] == 3, 'time_signature_bin'] = '3/4 Signature'\n",
    "    return X[['time_signature_bin']]\n",
    "\n",
    "def to_string(X):\n",
    "    return X.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_feat = FunctionTransformer(log_features, validate=False)\n",
    "time_feat = FunctionTransformer(bin_time_signature, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocessor(df_train=None):\n",
    "        \n",
    "    # continous features\n",
    "    numeric_features = ['loudness']\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # log features\n",
    "    log_features = ['duration']\n",
    "    log_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('log_feat', log_feat),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # categorical features\n",
    "    categorical_features = ['key', 'mode']\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('stringify', FunctionTransformer(to_string, validate=False)),\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(sparse=False))\n",
    "    ])\n",
    "\n",
    "    # time signature feature\n",
    "    time_feature = ['time_signature']\n",
    "    time_transformer = Pipeline(steps=[\n",
    "        ('binner', time_feat),\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('stringify', FunctionTransformer(to_string, validate=False)),\n",
    "        ('onehot', OneHotEncoder(sparse=False))\n",
    "    ])\n",
    "\n",
    "    # tempo feature\n",
    "    tempo_feature = ['tempo']\n",
    "    tempo_transformer = Pipeline(steps=[\n",
    "        ('binner', tempo_feat),\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('stringify', FunctionTransformer(to_string, validate=False)),\n",
    "        ('onehot', OneHotEncoder(sparse=False))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('log', log_transformer, log_features),\n",
    "            ('cat', categorical_transformer, categorical_features),\n",
    "            ('time', time_transformer, time_feature),\n",
    "            ('tempo', tempo_transformer, tempo_feature)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    \n",
    "    preprocessor.fit(df_train)\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created the tokens, let's load them into a `DataBunch` to train our LM further or generate text with a pre-trained LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_FIRST_RUN:\n",
    "    data_lm = TextLMDataBunch.from_tokens(MODEL_PATH,\n",
    "                                          bs=128,\n",
    "                                          max_vocab=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODEL_FIRST_RUN:\n",
    "    data_lm = TextLMDataBunch.from_id_files(MODEL_PATH/'tmp')\n",
    "    data_lm.path = MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20002"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lm.train_ds.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = copy(data_lm.train_ds)\n",
    "valid_text = copy(data_lm.valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODEL_FIRST_RUN:\n",
    "    df_train = pd.read_csv(MODEL_PATH/'df_train.csv', index_col=0)\n",
    "    df_valid = pd.read_csv(MODEL_PATH/'df_valid.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j_rosen_1392/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "preprocessor = create_preprocessor(df_train)\n",
    "\n",
    "df_train_tfm = preprocessor.transform(df_train)\n",
    "df_valid_tfm = preprocessor.transform(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSOR_PATH = MODEL_PATH/f'{model_name}_preprocessor.pkl'\n",
    "PREPROCESSOR_STORAGE = f'gs://w210-capstone/models/{model_name}_preprocessor.pkl'\n",
    "\n",
    "with open(PREPROCESSOR_PATH, 'wb') as f:\n",
    "    pickle.dump(preprocessor, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PREPROCESSOR_PATH, 'rb') as f:\n",
    "    test_pre = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://../data/models/4.3-MM-108k-post-genre-song_title/4.3-MM-108k-post-genre-song_title_preprocessor.pkl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  5.6 KiB/  5.6 KiB]                                                \n",
      "Operation completed over 1 objects/5.6 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp $PREPROCESSOR_PATH $PREPROCESSOR_STORAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio = AudioDataset(df_train_tfm, train_text)\n",
    "valid_audio = AudioDataset(df_valid_tfm, valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_data = MultimodalDataLoader(audio_dataset=train_audio,\n",
    "                                  dataset=train_text)\n",
    "\n",
    "multi_data_valid = MultimodalDataLoader(audio_dataset=valid_audio,\n",
    "                                  dataset=valid_text)\n",
    "\n",
    "multi_db = DataBunch(multi_data, multi_data_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = True\n",
    "DOWNLOAD_INIT_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODEL PARAMS\n",
    "audio_sz = train_audio.feature_size\n",
    "vocab_sz = 20002\n",
    "emb_sz = 400\n",
    "n_hid = 1150\n",
    "n_layers = 3\n",
    "pad_token = 1\n",
    "qrnn = False\n",
    "bidir = False\n",
    "drop_mult = 0.5\n",
    "dps = np.array([0.25, 0.1, 0.2, 0.02, 0.15]) * drop_mult\n",
    "hidden_p = dps[4]\n",
    "input_p = dps[0]\n",
    "embed_p = dps[3]\n",
    "weight_p = dps[2]\n",
    "tie_weights = False\n",
    "output_p = dps[1]\n",
    "bias = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "multimodal_rnn = MultiModalPostRNN(audio_sz=audio_sz,\n",
    "                              vocab_sz=vocab_sz,\n",
    "                              emb_sz=emb_sz,\n",
    "                              n_hid=n_hid,\n",
    "                              n_layers=n_layers,\n",
    "                              pad_token=pad_token,\n",
    "                              qrnn=qrnn,\n",
    "                              bidir=bidir,\n",
    "                              hidden_p=hidden_p,\n",
    "                              input_p=input_p,\n",
    "                              embed_p=embed_p,\n",
    "                              weight_p=weight_p,\n",
    "                              output_p=output_p,\n",
    "                              bias=bias,\n",
    "                              tie_encoder=tie_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = RNNLearner(multi_db, multimodal_rnn)\n",
    "learn.path=MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_FIRST_RUN:\n",
    "    PRETRAINED_TO_MULTI = {\n",
    "        '0.encoder.weight': 'encoder.weight',\n",
    "        '0.encoder_dp.emb.weight': 'encoder_dp.emb.weight',\n",
    "        '0.rnns.0.weight_hh_l0_raw': 'multimode.0.weight_hh_l0_raw',\n",
    "        '0.rnns.0.module.weight_ih_l0': 'multimode.0.module.weight_ih_l0',\n",
    "        '0.rnns.0.module.weight_hh_l0': 'multimode.0.module.weight_hh_l0',\n",
    "        '0.rnns.0.module.bias_ih_l0': 'multimode.0.module.bias_ih_l0',\n",
    "        '0.rnns.0.module.bias_hh_l0': 'multimode.0.module.bias_hh_l0',\n",
    "        '0.rnns.1.weight_hh_l0_raw': 'multimode.1.weight_hh_l0_raw',\n",
    "        '0.rnns.1.module.weight_ih_l0': 'multimode.1.module.weight_ih_l0',\n",
    "        '0.rnns.1.module.weight_hh_l0': 'multimode.1.module.weight_hh_l0',\n",
    "        '0.rnns.1.module.bias_ih_l0': 'multimode.1.module.bias_ih_l0',\n",
    "        '0.rnns.1.module.bias_hh_l0': 'multimode.1.module.bias_hh_l0', \n",
    "        '0.rnns.2.weight_hh_l0_raw': 'multimode.2.weight_hh_l0_raw', \n",
    "        '0.rnns.2.module.weight_ih_l0': 'multimode.2.module.weight_ih_l0',\n",
    "        '0.rnns.2.module.weight_hh_l0': 'multimode.2.module.weight_hh_l0',\n",
    "        '0.rnns.2.module.bias_ih_l0': 'multimode.2.module.bias_ih_l0',\n",
    "        '0.rnns.2.module.bias_hh_l0': 'multimode.2.module.bias_hh_l0',\n",
    "        '1.decoder.weight': 'multidecoder.decoder.weight',\n",
    "        '1.decoder.bias': 'multidecoder.decoder.bias'\n",
    "    }\n",
    "\n",
    "    map_weights(learn,\n",
    "                INIT_MODEL_PATH/f'models/{init_model_name}_best.pth',\n",
    "                INIT_MODEL_PATH/f'tmp/itos.pkl',\n",
    "                PRETRAINED_TO_MULTI,\n",
    "                pre_rnn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_url = 'https://storage.googleapis.com/capstone-deep-lyrics/3.2-ULMFiT-108k_best.pth'\n",
    "# itos_url = 'https://storage.googleapis.com/capstone-deep-lyrics/3.2-ULMFiT-108k_best.pth'\n",
    "\n",
    "# if DOWNLOAD_INIT_MODEL:\n",
    "#     Path(MODEL_PATH/'models').mkdir(exist_ok=True)\n",
    "#     download_url(weights_url, MODEL_PATH/f'models/{model_name}_best.pth', overwrite=False)\n",
    "#     download_url(weights_url, MODEL_PATH/f'models/{model_name}_best.pth', overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_load(self, name:PathOrStr):\n",
    "    \"\"\"Load model onto CPU that was trained on a GPU `name` from `self.model_dir`.\n",
    "       We need these because the fastai load function doesn't allow for a remapping of the storage location.\"\"\"\n",
    "    self.model.load_state_dict(torch.load(self.path/self.model_dir/f'{name}.pth', map_location=lambda storage, loc: storage))\n",
    "\n",
    "setattr(RNNLearner, 'cpu_load', cpu_load) #monkey patch onto our RNNLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODEL_FIRST_RUN:\n",
    "    if not GPU:\n",
    "        learn.cpu_load(f'{model_name}_best')\n",
    "    else:\n",
    "        learn.load(f'{model_name}_best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SaveModel(LearnerCallback):\n",
    "    \"\"\"Save Latest Model\"\"\"\n",
    "    def __init__(self, learn:Learner, model_name='saved_model'):\n",
    "        super().__init__(learn)\n",
    "        self.model_name = model_name\n",
    "        self.model_date = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "        self.best_loss = None\n",
    "        self.perplexity = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch:int, metrics, last_metrics, **kwargs):\n",
    "        loss, *_ = last_metrics\n",
    "        perp = np.exp(loss)\n",
    "        self.perplexity.append(perp)\n",
    "        if self.best_loss == None or loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "            self.learn.save(f'{self.model_name}_best')\n",
    "        return False\n",
    "    \n",
    "    def on_train_end(self, epoch:int, **kwargs):\n",
    "        self.learn.save(f'{self.model_name}_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_callback = SaveModel(learn, model_name=f'{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 17:51\n",
      "epoch  train_loss  valid_loss  accuracy\n",
      "1      2.955110    2.967624    0.456658  (17:51)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if TRAIN:\n",
    "#     learn.fit_one_cycle(10, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 2:56:54\n",
      "epoch  train_loss  valid_loss  accuracy\n",
      "1      3.497352    3.927819    0.399917  (17:45)\n",
      "2      3.310748    3.660103    0.415094  (17:37)\n",
      "3      3.029750    3.407333    0.431795  (17:40)\n",
      "4      2.877578    3.222411    0.446730  (17:37)\n",
      "5      2.762602    3.119586    0.455332  (17:46)\n",
      "6      2.765076    3.067299    0.459284  (17:41)\n",
      "7      2.694141    3.039227    0.461307  (17:39)\n",
      "8      2.674448    3.024786    0.462355  (17:49)\n",
      "9      2.700220    3.020312    0.462684  (17:37)\n",
      "10     2.683760    3.018750    0.462768  (17:38)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    learn.unfreeze()\n",
    "    learn.fit_one_cycle(10, 1e-4, callbacks=[save_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation loss:  3.0187497\n"
     ]
    }
   ],
   "source": [
    "print(\"best validation loss: \", learn.save_model.best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.save_encoder(f'{model_name}_encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN: pass\n",
    "MODEL_OUTPUT_NAME = MODEL_PATH/'models'/f'{model_name}_best.pth'\n",
    "CLOUD_STORAGE = f'gs://w210-capstone/models/{model_name}_best.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://../data/models/4.3-MM-108k-post-genre-song_title/models/4.3-MM-108k-post-genre-song_title_best.pth [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run\n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "\\\n",
      "Operation completed over 1 objects/183.2 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp $MODEL_OUTPUT_NAME $CLOUD_STORAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHITECTURE_PATH = MODEL_PATH/'models'/f'{model_name}_architecture.pkl'\n",
    "ARCHITECTURE_STORAGE = f'gs://w210-capstone/models/{model_name}_architecture.pkl'\n",
    "ITOS_PATH = MODEL_PATH/'tmp'/'itos.pkl'\n",
    "ITOS_STORAGE = f'gs://w210-capstone/models/{model_name}_itos.pkl'\n",
    "\n",
    "model_dump = learn.model\n",
    "with open(ARCHITECTURE_PATH, 'wb') as f:\n",
    "    pickle.dump(model_dump, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://../data/models/4.3-MM-108k-post-genre-song_title/models/4.3-MM-108k-post-genre-song_title_architecture.pkl [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run\n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "-\n",
      "Operation completed over 1 objects/369.2 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp $ARCHITECTURE_PATH $ARCHITECTURE_STORAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://../data/models/4.3-MM-108k-post-genre-song_title/tmp/itos.pkl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][310.2 KiB/310.2 KiB]                                                \n",
      "Operation completed over 1 objects/310.2 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp $ITOS_PATH $ITOS_STORAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step(learner, context, audio, context_length, temp=1):\n",
    "    \n",
    "    # FIX THIS\n",
    "    audio_size = train_audio.feature_size\n",
    "\n",
    "    model = learner.model\n",
    "    \n",
    "    if GPU:\n",
    "        context = LongTensor(context[-context_length:]).view(-1,1).cuda()\n",
    "    else:\n",
    "        context = LongTensor(context[-context_length:]).view(-1,1).cpu()\n",
    "    \n",
    "    context = torch.autograd.Variable(context)\n",
    "    \n",
    "    model.reset()\n",
    "    model.eval()\n",
    "\n",
    "    if audio is None:\n",
    "        audio_features = Tensor([0]*audio_size*len(context))\\\n",
    "        .view(-1, 1, audio_size).cuda()\n",
    "    else:\n",
    "        audio_features = np.tile(audio, len(context))\n",
    "        audio_features = Tensor(audio_features).view(-1, 1, len(audio)).cuda()\n",
    "        \n",
    "    # forward pass the \"context\" into the model\n",
    "    result, *_ = model(context, audio_features)\n",
    "    result = result[-1]\n",
    "\n",
    "    # set unk and pad to 0 prob\n",
    "    # i.e. never pick unknown or pad\n",
    "    result[0] = -np.inf\n",
    "    result[1] = -np.inf\n",
    "\n",
    "    # softmax and normalize\n",
    "    probabilities = F.softmax(result/temp, dim=0)\n",
    "    probabilities = np.asarray(probabilities.detach().cpu(), dtype=np.float)\n",
    "    probabilities /= np.sum(probabilities) \n",
    "    return probabilities\n",
    "\n",
    "def get_word_from_index(idx):\n",
    "\n",
    "    return data_lm.valid_ds.vocab.textify([idx])\n",
    "\n",
    "\n",
    "def print_words(context):\n",
    "    for i in range(len(context)):\n",
    "        \n",
    "        step = context[i]\n",
    "\n",
    "        word = data_lm.valid_ds.vocab.textify([step])\n",
    "\n",
    "        if word == 'xeol':\n",
    "            word = 'xeol \\n'\n",
    "        elif 'xbol' in word:\n",
    "            word = 'xbol'\n",
    "        elif word == 'xeos': \n",
    "            print(word)\n",
    "            break\n",
    "            \n",
    "        print(word, end=' ')   \n",
    "\n",
    "def generate_text(learner, seed_text=['xbos'], audio=None, max_len=500, GPU=False, context_length=20, beam_width=5, temp=1, verbose=True, graph=False):\n",
    "    \"\"\"Generates text with a given learner and returns best options.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    learner : RNNLearner Language Model (RNNLearner.language_model())\n",
    "        Fastai RNNLearner with tokenized language model data already loaded \n",
    "        \n",
    "    seed_text : list or str\n",
    "        List of strings where each item is a token. (e.g. ['the', 'cat']) or string that is split on white space\n",
    "\n",
    "    max_len : int\n",
    "        Number of words in generated sequence\n",
    "        \n",
    "    gpu : bool\n",
    "        If you're using a GPU or not...\n",
    "    \n",
    "    context_length : int\n",
    "        Amount of words that get input as \"context\" into the model. Set to 0 for no limit   \n",
    "        \n",
    "    beam_width : int\n",
    "        How many new word indices to try out...computationally expensive\n",
    "    \n",
    "    verbose : bool\n",
    "        If True, prints every possible context for a given word cycle\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    context_and_scores : list of lists\n",
    "        Returns a sorted list of the entire tree search of contexts and their respective scores in the form:\n",
    "        [[context, score], [context, score], ..., [context, score]]\n",
    "    \"\"\"\n",
    "        \n",
    "    if isinstance(seed_text, str):\n",
    "        seed_text = data_lm.train_ds.vocab.numericalize(seed_text.split(' '))\n",
    "    \n",
    "    \n",
    "    # Width for the beam search, to be externalized along with general decoding\n",
    "    beam_width = beam_width\n",
    "    \n",
    "    if graph:\n",
    "        optimization_graph = Digraph()\n",
    "\n",
    "    # List of candidate word sequence. We'll maintain #beam_width top sequences here.\n",
    "    # The context is a list of words, the scores are the sum of the log probabilities of each word\n",
    "    context_and_scores = [[seed_text, 0.0]]\n",
    "    \n",
    "    # Loop over max number of words\n",
    "    for word_number in range(max_len):\n",
    "        print(f'Generating word: {word_number+1} / {max_len}')\n",
    "\n",
    "        candidates = []\n",
    "        \n",
    "        # For each possible context that we've generated so far, generate new probabilities, \n",
    "        # and pick an additional #beam_width next candidates\n",
    "        for i in range(len(context_and_scores)):\n",
    "            # Get a new sequence of word indices and log-probability\n",
    "            # Example: [[2, 138, 661], 23.181717]\n",
    "            context, score = context_and_scores[i]\n",
    "            \n",
    "            # Obtain probabilities for next word given the context \n",
    "            probabilities = generate_step(learner, context, audio, context_length, temp)\n",
    "\n",
    "            # Multinomial draw from the probabilities\n",
    "            multinom_draw = np.random.multinomial(beam_width, probabilities)\n",
    "            top_probabilities = np.argwhere(multinom_draw != 0).flatten()\n",
    "                        \n",
    "            #For each possible new candidate, update the context and scores\n",
    "            for j in range(len(top_probabilities)):\n",
    "                next_word_idx = top_probabilities[j]\n",
    "                new_context = context + [next_word_idx]\n",
    "                candidate = [new_context, (score - np.log(probabilities[next_word_idx]))]\n",
    "                candidates.append(candidate)\n",
    "                \n",
    "                if graph:\n",
    "                    optimization_graph.node(\"%d_%d\" % (word_number, next_word_idx), \"%s (%.2f)\" % (get_word_from_index(next_word_idx), candidate[1]))\n",
    "                    optimization_graph.edge(\"%d_%d\" % (word_number - 1, context[len(context) -1]), \"%d_%d\" % (word_number, next_word_idx))\n",
    "                \n",
    "        #update the running tally of context and scores and sort by probability of each entry\n",
    "        context_and_scores = candidates\n",
    "        context_and_scores = sorted(context_and_scores, key = lambda x: x[1]) #sort by top entries\n",
    "\n",
    "        context_and_scores = context_and_scores[:15] #for now, only keep the top 15 to speed things up but we can/should change this to beam_width or something else\n",
    "        \n",
    "        if verbose:\n",
    "            for context, score in context_and_scores:\n",
    "                print_words(context)\n",
    "                print('\\n')\n",
    "\n",
    "    if graph:\n",
    "        now = str(datetime.datetime.now())\n",
    "        optimization_graph.render(directory='graph_viz/', filename=now, cleanup=True)\n",
    "    return context_and_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "\n",
    "data_model =   {\n",
    "      \"danceability\": 0.6027456183070403,\n",
    "      \"duration_ms\": 0.5962133916683182,\n",
    "      \"energy\": 0.14658129805029452,\n",
    "      \"genres\": [\n",
    "        \"folk\",\n",
    "        \"gipsy\"\n",
    "      ],\n",
    "      \"mode\": 0,\n",
    "      \"tags\": [\n",
    "        \"tags\",\n",
    "        \"tags\"\n",
    "      ],\n",
    "      \"tempo\": 0.5637376656633328,\n",
    "      \"title\": \"title\",\n",
    "      \"year\": 1465\n",
    "    }\n",
    "\n",
    "df = json_normalize(data_model)\n",
    "\n",
    "df.rename(columns={'duration_ms':'duration'}, inplace=True)\n",
    "df['loudness'] = -3\n",
    "df['time_signature'] = 4\n",
    "df['key'] = 1\n",
    "\n",
    "# ['tempo', 'mode', 'duration', 'key', 'loudness', 'time_signature']\n",
    "\n",
    "audio_features = preprocessor.transform(df).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating word: 1 / 60\n",
      "Generating word: 2 / 60\n",
      "Generating word: 3 / 60\n",
      "Generating word: 4 / 60\n",
      "Generating word: 5 / 60\n",
      "Generating word: 6 / 60\n",
      "Generating word: 7 / 60\n",
      "Generating word: 8 / 60\n",
      "Generating word: 9 / 60\n",
      "Generating word: 10 / 60\n",
      "Generating word: 11 / 60\n",
      "Generating word: 12 / 60\n",
      "Generating word: 13 / 60\n",
      "Generating word: 14 / 60\n",
      "Generating word: 15 / 60\n",
      "Generating word: 16 / 60\n",
      "Generating word: 17 / 60\n",
      "Generating word: 18 / 60\n",
      "Generating word: 19 / 60\n",
      "Generating word: 20 / 60\n",
      "Generating word: 21 / 60\n",
      "Generating word: 22 / 60\n",
      "Generating word: 23 / 60\n",
      "Generating word: 24 / 60\n",
      "Generating word: 25 / 60\n",
      "Generating word: 26 / 60\n",
      "Generating word: 27 / 60\n",
      "Generating word: 28 / 60\n",
      "Generating word: 29 / 60\n",
      "Generating word: 30 / 60\n",
      "Generating word: 31 / 60\n",
      "Generating word: 32 / 60\n",
      "Generating word: 33 / 60\n",
      "Generating word: 34 / 60\n",
      "Generating word: 35 / 60\n",
      "Generating word: 36 / 60\n",
      "Generating word: 37 / 60\n",
      "Generating word: 38 / 60\n",
      "Generating word: 39 / 60\n",
      "Generating word: 40 / 60\n",
      "Generating word: 41 / 60\n",
      "Generating word: 42 / 60\n",
      "Generating word: 43 / 60\n",
      "Generating word: 44 / 60\n",
      "Generating word: 45 / 60\n",
      "Generating word: 46 / 60\n",
      "Generating word: 47 / 60\n",
      "Generating word: 48 / 60\n",
      "Generating word: 49 / 60\n",
      "Generating word: 50 / 60\n",
      "Generating word: 51 / 60\n",
      "Generating word: 52 / 60\n",
      "Generating word: 53 / 60\n",
      "Generating word: 54 / 60\n",
      "Generating word: 55 / 60\n",
      "Generating word: 56 / 60\n",
      "Generating word: 57 / 60\n",
      "Generating word: 58 / 60\n",
      "Generating word: 59 / 60\n",
      "Generating word: 60 / 60\n"
     ]
    }
   ],
   "source": [
    "final_scores = generate_text(learn, GPU=GPU,\n",
    "                             seed_text='xbos xgenre',\n",
    "                             audio=audio_features,\n",
    "                             max_len=60, context_length=40,\n",
    "                             beam_width=3, verbose=False,\n",
    "                             temp=1.5, graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xbos xgenre downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs 1.431734304170443\n",
      "\n",
      "\n",
      "xbos xgenre color' downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs 4.521138338397349\n",
      "\n",
      "\n",
      "xbos xgenre downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs color' downstairs downstairs downstairs downstairs 5.777580079877366\n",
      "\n",
      "\n",
      "xbos xgenre downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs verboten downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs 8.164733573190956\n",
      "\n",
      "\n",
      "xbos xgenre downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs dormida downstairs downstairs downstairs 8.47489822563233\n",
      "\n",
      "\n",
      "xbos xgenre color' downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs color' downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs 8.679863408918369\n",
      "\n",
      "\n",
      "xbos xgenre color' downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs color' downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs 8.69526759050904\n",
      "\n",
      "\n",
      "xbos xgenre color' downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs color' downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs 8.820701310013561\n",
      "\n",
      "\n",
      "xbos xgenre color' downstairs downstairs downstairs downstairs downstairs berlin downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs 10.046418938284459\n",
      "\n",
      "\n",
      "xbos xgenre downstairs downstairs downstairs downstairs downstairs omkring downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs 10.305144173941164\n",
      "\n",
      "\n",
      "xbos xgenre color' downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs verboten downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs 10.682225868811395\n",
      "\n",
      "\n",
      "xbos xgenre color' downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs pendre downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs 10.771690188374663\n",
      "\n",
      "\n",
      "xbos xgenre color' downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs squeezing downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs 11.070989909413644\n",
      "\n",
      "\n",
      "xbos xgenre color' downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs color' color' downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs 12.038689373902677\n",
      "\n",
      "\n",
      "xbos xgenre color' downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs color' color' downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs downstairs 12.464624124613843\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print all of the final options of songs\n",
    "for song, score in final_scores:\n",
    "    print_words(song)\n",
    "    print(score)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nlp.generate_lyrics import DeepLyric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_lyric = DeepLyric(learn.model, weights=None, itos=data_lm.train_ds.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_lyric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating word: 1 / 50\n",
      "Generating word: 2 / 50\n",
      "Generating word: 3 / 50\n",
      "Generating word: 4 / 50\n",
      "Generating word: 5 / 50\n",
      "Generating word: 6 / 50\n",
      "Generating word: 7 / 50\n",
      "Generating word: 8 / 50\n",
      "Generating word: 9 / 50\n",
      "Generating word: 10 / 50\n",
      "Generating word: 11 / 50\n",
      "Generating word: 12 / 50\n",
      "Generating word: 13 / 50\n",
      "Generating word: 14 / 50\n",
      "Generating word: 15 / 50\n",
      "Generating word: 16 / 50\n",
      "Generating word: 17 / 50\n",
      "Generating word: 18 / 50\n",
      "Generating word: 19 / 50\n",
      "Generating word: 20 / 50\n",
      "Generating word: 21 / 50\n",
      "Generating word: 22 / 50\n",
      "Generating word: 23 / 50\n",
      "Generating word: 24 / 50\n",
      "Generating word: 25 / 50\n",
      "Generating word: 26 / 50\n",
      "Generating word: 27 / 50\n",
      "Generating word: 28 / 50\n",
      "Generating word: 29 / 50\n",
      "Generating word: 30 / 50\n",
      "Generating word: 31 / 50\n",
      "Generating word: 32 / 50\n",
      "Generating word: 33 / 50\n",
      "Generating word: 34 / 50\n",
      "Generating word: 35 / 50\n",
      "Generating word: 36 / 50\n",
      "Generating word: 37 / 50\n",
      "Generating word: 38 / 50\n",
      "Generating word: 39 / 50\n",
      "Generating word: 40 / 50\n",
      "Generating word: 41 / 50\n",
      "Generating word: 42 / 50\n",
      "Generating word: 43 / 50\n",
      "Generating word: 44 / 50\n",
      "Generating word: 45 / 50\n",
      "Generating word: 46 / 50\n",
      "Generating word: 47 / 50\n",
      "Generating word: 48 / 50\n",
      "Generating word: 49 / 50\n",
      "Generating word: 50 / 50\n"
     ]
    }
   ],
   "source": [
    "deep_lyric.generate_text(seed_text='xbos xgenre country', \n",
    "                         verbose=1, context_length=100, beam_width=3,\n",
    "                         max_len=50, top_k=5, temperature=1.45,\n",
    "                         GPU=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SONG START\n",
      " genre: country \n",
      " title: the kiss of gold \n",
      "\n",
      "xbol-1 when i 'm with you \n",
      " there 's so much out there for me \n",
      " here and now , my dear \n",
      " there is nothing more left to do \n",
      " \n",
      " when my whole life through \n",
      " i "
     ]
    }
   ],
   "source": [
    "deep_lyric.print_lyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_lyric.save_lyrics_to_file('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating word: 1 / 20\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of backend CUDA but got backend CPU for argument #3 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-aa332739c49e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdeep_lyric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdeep_lyric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_lyrics_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'test{i}.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPU\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/capstone-deep-lyrics/notebooks/src/nlp/generate_lyrics.py\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(self, seed_text, max_len, GPU, context_length, beam_width, verbose, temperature, top_k, audio)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_and_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;31m# Obtain probabilities for next word given the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                 \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0;31m# Multinomial draw from the probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/capstone-deep-lyrics/notebooks/src/nlp/generate_lyrics.py\u001b[0m in \u001b[0;36mget_text_distribution\u001b[0;34m(self, context, context_length, temperature, GPU, audio)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;31m# forward pass the \"context\" into the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'language'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pre-multi'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'post-multi'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/text/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mraw_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_dp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mnew_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mraw_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhid_dp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/fastai/text/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, words, scale)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmasked_embed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n\u001b[0;32m---> 70\u001b[0;31m                            self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m#def _repackage_var(h:Tensors)->Tensors:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of backend CUDA but got backend CPU for argument #3 'index'"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    deep_lyric.generate_text(max_len=20)\n",
    "    deep_lyric.save_lyrics_to_file(f'test{i}.txt', GPU=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
