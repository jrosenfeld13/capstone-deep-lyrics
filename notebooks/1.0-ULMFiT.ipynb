{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.tokenize\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msd_id</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAAAAW128F429D538</td>\n",
       "      <td>\\n\\n[Verse 1]\\nAlright, I might\\nHave had a li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAAABD128F429CF47</td>\n",
       "      <td>\\n\\nDarling, I don't know much\\nBut I know I l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAAADZ128F9348C2E</td>\n",
       "      <td>\\n\\nSiento una pena muy honda dentro del alma\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAAAEF128F4273421</td>\n",
       "      <td>\\n\\nAdam Ant/Marco Pirroni\\nEvery girl is a so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAAAFD128F92F423A</td>\n",
       "      <td>\\n\\nI've just erased it's been a while, I've g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               msd_id                                             lyrics\n",
       "0  TRAAAAW128F429D538  \\n\\n[Verse 1]\\nAlright, I might\\nHave had a li...\n",
       "1  TRAAABD128F429CF47  \\n\\nDarling, I don't know much\\nBut I know I l...\n",
       "2  TRAAADZ128F9348C2E  \\n\\nSiento una pena muy honda dentro del alma\\...\n",
       "3  TRAAAEF128F4273421  \\n\\nAdam Ant/Marco Pirroni\\nEvery girl is a so...\n",
       "4  TRAAAFD128F92F423A  \\n\\nI've just erased it's been a while, I've g..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/interim/subset-10k/genius_lyrics.csv')\n",
    "# drop lyrics >5000\n",
    "df = df[df.lyrics.str.len() < 5000]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "1. First consider each line its own \"sentence\", keeping track of blanklines\n",
    "2. Regexp Tokenizer with the following:  \n",
    " - Bracket enclosed texts (usually song part header)\n",
    " - All words\n",
    " - Any numeric -- keep commas and periods together\n",
    " - All other non-whitespace characters\n",
    "3. Wrap each line with `<s>` and `</s>` tokens\n",
    "4. Wrap each song with `<d>` and `</d>` tokens (documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_lyrics(lyrics):\n",
    "    tk = nltk.tokenize.LineTokenizer(blanklines='keep')\n",
    "    tokd = tk.tokenize(lyrics)\n",
    "    \n",
    "    re_tk = nltk.tokenize.RegexpTokenizer(r'\\[[^\\]]+\\]|\\w+|[\\d\\.,]+|\\S+',\n",
    "                                          discard_empty=False)\n",
    "    re_tokd = re_tk.tokenize_sents(tokd)\n",
    "    \n",
    "    [s.insert(0, '<s>') for s in re_tokd] # insert start token for each line\n",
    "    [s.append('</s>') for s in re_tokd] # append end token for each line\n",
    "    \n",
    "    flat = list(itertools.chain(*re_tokd))\n",
    "    flat.insert(0, '<d>')\n",
    "    flat.append('</d>')\n",
    "    # lower case and de-space\n",
    "    flat = [w.lower().replace(' ', '-') for w in flat]\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msd_id</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>tokd</th>\n",
       "      <th>tokd_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAAAAW128F429D538</td>\n",
       "      <td>\\n\\n[Verse 1]\\nAlright, I might\\nHave had a li...</td>\n",
       "      <td>[&lt;d&gt;, &lt;s&gt;, &lt;/s&gt;, &lt;s&gt;, &lt;/s&gt;, &lt;s&gt;, [verse-1], &lt;/...</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAAABD128F429CF47</td>\n",
       "      <td>\\n\\nDarling, I don't know much\\nBut I know I l...</td>\n",
       "      <td>[&lt;d&gt;, &lt;s&gt;, &lt;/s&gt;, &lt;s&gt;, &lt;/s&gt;, &lt;s&gt;, darling, ,, i...</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAAADZ128F9348C2E</td>\n",
       "      <td>\\n\\nSiento una pena muy honda dentro del alma\\...</td>\n",
       "      <td>[&lt;d&gt;, &lt;s&gt;, &lt;/s&gt;, &lt;s&gt;, &lt;/s&gt;, &lt;s&gt;, siento, una, ...</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAAAEF128F4273421</td>\n",
       "      <td>\\n\\nAdam Ant/Marco Pirroni\\nEvery girl is a so...</td>\n",
       "      <td>[&lt;d&gt;, &lt;s&gt;, &lt;/s&gt;, &lt;s&gt;, &lt;/s&gt;, &lt;s&gt;, adam, ant, /m...</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAAAFD128F92F423A</td>\n",
       "      <td>\\n\\nI've just erased it's been a while, I've g...</td>\n",
       "      <td>[&lt;d&gt;, &lt;s&gt;, &lt;/s&gt;, &lt;s&gt;, &lt;/s&gt;, &lt;s&gt;, i, 've, just,...</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               msd_id                                             lyrics  \\\n",
       "0  TRAAAAW128F429D538  \\n\\n[Verse 1]\\nAlright, I might\\nHave had a li...   \n",
       "1  TRAAABD128F429CF47  \\n\\nDarling, I don't know much\\nBut I know I l...   \n",
       "2  TRAAADZ128F9348C2E  \\n\\nSiento una pena muy honda dentro del alma\\...   \n",
       "3  TRAAAEF128F4273421  \\n\\nAdam Ant/Marco Pirroni\\nEvery girl is a so...   \n",
       "4  TRAAAFD128F92F423A  \\n\\nI've just erased it's been a while, I've g...   \n",
       "\n",
       "                                                tokd  tokd_len  \n",
       "0  [<d>, <s>, </s>, <s>, </s>, <s>, [verse-1], </...       651  \n",
       "1  [<d>, <s>, </s>, <s>, </s>, <s>, darling, ,, i...       361  \n",
       "2  [<d>, <s>, </s>, <s>, </s>, <s>, siento, una, ...       127  \n",
       "3  [<d>, <s>, </s>, <s>, </s>, <s>, adam, ant, /m...       322  \n",
       "4  [<d>, <s>, </s>, <s>, </s>, <s>, i, 've, just,...       181  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokd'] = df.lyrics.apply(tokenize_lyrics)\n",
    "df['tokd_len'] = df.tokd.apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, just save both as both train and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_path = '../data/interim/subset-10k/tok_test'\n",
    "tok_test = np.array(df.tokd)\n",
    "np.save(token_path+'/train_tok.npy', tok_test)\n",
    "np.save(token_path+'/valid_tok.npy', tok_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<d>',\n",
       " '<s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '[verse-1]',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'alright',\n",
       " ',',\n",
       " 'i',\n",
       " 'might',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'have',\n",
       " 'had',\n",
       " 'a',\n",
       " 'little',\n",
       " 'glare',\n",
       " 'when',\n",
       " 'i',\n",
       " 'stared',\n",
       " 'at',\n",
       " 'ya',\n",
       " 'ho',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'but',\n",
       " 'i',\n",
       " 'didn',\n",
       " \"'t\",\n",
       " 'know',\n",
       " 'she',\n",
       " 'was',\n",
       " 'like',\n",
       " 'that',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'she',\n",
       " 'stared',\n",
       " 'right',\n",
       " 'back',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'my',\n",
       " 'niggas',\n",
       " 'warnin',\n",
       " 'me',\n",
       " 'that',\n",
       " 'she',\n",
       " 'was',\n",
       " 'comin',\n",
       " 'on',\n",
       " 'to',\n",
       " 'me',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'i',\n",
       " 'react',\n",
       " 'like',\n",
       " 'a',\n",
       " 'mack',\n",
       " 'do',\n",
       " ',',\n",
       " 'i',\n",
       " 'act',\n",
       " 'cool',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'just',\n",
       " 'to',\n",
       " 'test',\n",
       " 'her',\n",
       " ',',\n",
       " \"'cause\",\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'no',\n",
       " 'jester',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'i',\n",
       " 'suggest',\n",
       " 'her',\n",
       " 'and',\n",
       " 'her',\n",
       " 'friend',\n",
       " 'be',\n",
       " 'outtie',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'because',\n",
       " 'i',\n",
       " 'don',\n",
       " \"'t\",\n",
       " 'want',\n",
       " 'to',\n",
       " 'make',\n",
       " 'my',\n",
       " 'pal',\n",
       " 'get',\n",
       " 'rowdy',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'and',\n",
       " 'doubt',\n",
       " 'me',\n",
       " ',',\n",
       " 'our',\n",
       " 'friendship',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'but',\n",
       " 'when',\n",
       " 'lips',\n",
       " 'touch',\n",
       " ',',\n",
       " 'i',\n",
       " 'go',\n",
       " 'crazy',\n",
       " 'in',\n",
       " 'the',\n",
       " 'clutch',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'sorta',\n",
       " 'like',\n",
       " 'a',\n",
       " 'schitzo',\n",
       " 'i',\n",
       " 'forgets',\n",
       " 'my',\n",
       " 'bros',\n",
       " 'and',\n",
       " 'pals',\n",
       " 'over',\n",
       " 'gals',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'i',\n",
       " 'didn',\n",
       " \"'t\",\n",
       " 'mean',\n",
       " 'to',\n",
       " ',',\n",
       " 'but',\n",
       " 'when',\n",
       " 'you',\n",
       " 'fiend',\n",
       " 'you',\n",
       " 'do',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'strange',\n",
       " 'things',\n",
       " 'for',\n",
       " 'the',\n",
       " 'boot',\n",
       " 'denim',\n",
       " 'no',\n",
       " 'matter',\n",
       " 'who',\n",
       " \"'s\",\n",
       " 'in',\n",
       " 'em',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'grab',\n",
       " 'a',\n",
       " 'flooze',\n",
       " 'then',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'in',\n",
       " 'traffic',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'don',\n",
       " \"'t\",\n",
       " 'laugh',\n",
       " 'it',\n",
       " 'might',\n",
       " 'be',\n",
       " 'your',\n",
       " 'girl',\n",
       " 'that',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'talkin',\n",
       " 'about',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'i',\n",
       " 'didn',\n",
       " \"'t\",\n",
       " 'mean',\n",
       " 'to',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '[chorus-4x]',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'it',\n",
       " 'ain',\n",
       " \"'t\",\n",
       " 'my',\n",
       " 'fault',\n",
       " 'that',\n",
       " 'your',\n",
       " 'girl',\n",
       " 'got',\n",
       " 'caught',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'it',\n",
       " 'ain',\n",
       " \"'t\",\n",
       " 'my',\n",
       " 'fault',\n",
       " 'that',\n",
       " 'your',\n",
       " 'girl',\n",
       " 'got',\n",
       " 'caught',\n",
       " '(i',\n",
       " 'didn',\n",
       " \"'t\",\n",
       " 'mean',\n",
       " 'to',\n",
       " ')',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '[verse-2]',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'another',\n",
       " 'incident',\n",
       " 'when',\n",
       " 'i',\n",
       " 'went',\n",
       " 'way',\n",
       " 'beyond',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'what',\n",
       " 'i',\n",
       " 'shoulda',\n",
       " 'done',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'john',\n",
       " 'shoulda',\n",
       " 'stopped',\n",
       " 'before',\n",
       " 'them',\n",
       " 'drawers',\n",
       " 'dropped',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'but',\n",
       " 'i',\n",
       " 'didn',\n",
       " \"'t,\",\n",
       " 'ain',\n",
       " \"'t\",\n",
       " 'no',\n",
       " 'quittin',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'really',\n",
       " 'didn',\n",
       " \"'t\",\n",
       " 'care',\n",
       " 'whose',\n",
       " 'girl',\n",
       " 'i',\n",
       " 'was',\n",
       " 'hittin',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'i',\n",
       " 'admit',\n",
       " 'skins',\n",
       " 'ain',\n",
       " \"'t\",\n",
       " 'a',\n",
       " 'reason',\n",
       " 'to',\n",
       " 'lose',\n",
       " 'friends',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'but',\n",
       " 'then',\n",
       " 'again',\n",
       " 'i',\n",
       " 'didn',\n",
       " \"'t\",\n",
       " 'know',\n",
       " ',',\n",
       " 'sorry',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'accept',\n",
       " 'apologies',\n",
       " 'and',\n",
       " 'live',\n",
       " 'on',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'are',\n",
       " 'we',\n",
       " 'gonna',\n",
       " 'hold',\n",
       " 'grudges',\n",
       " '?',\n",
       " 'well',\n",
       " ',',\n",
       " 'oh',\n",
       " 'fudge',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'new',\n",
       " 'year',\n",
       " 'and',\n",
       " 'i',\n",
       " 'do',\n",
       " 'fear',\n",
       " 'ya',\n",
       " 'actions',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'ya',\n",
       " 'actin',\n",
       " 'shy',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'i',\n",
       " 'mean',\n",
       " 'ya',\n",
       " 'need',\n",
       " 'to',\n",
       " 'be',\n",
       " 'smackin',\n",
       " 'that',\n",
       " 'ho',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'tryin',\n",
       " 'to',\n",
       " 'front',\n",
       " 'on',\n",
       " 'me',\n",
       " ',',\n",
       " 'ya',\n",
       " 'know',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'that',\n",
       " \"'s\",\n",
       " 'exactly',\n",
       " 'how',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'gonna',\n",
       " 'be',\n",
       " ',',\n",
       " 'ya',\n",
       " 'know',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'i',\n",
       " 'run',\n",
       " 'my',\n",
       " 'game',\n",
       " 'on',\n",
       " 'any',\n",
       " 'g',\n",
       " 'ya',\n",
       " 'know',\n",
       " '</s>',\n",
       " '<s>',\n",
       " \"'cause\",\n",
       " 'we',\n",
       " 'the',\n",
       " 'most',\n",
       " 'entertainin',\n",
       " \"'\",\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'i',\n",
       " 'meant',\n",
       " 'to',\n",
       " 'blame',\n",
       " 'women',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'for',\n",
       " 'makin',\n",
       " 'me',\n",
       " 'do',\n",
       " 'what',\n",
       " 'the',\n",
       " 'obscene',\n",
       " 'do',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'when',\n",
       " 'you',\n",
       " 'call',\n",
       " 'i',\n",
       " 'screen',\n",
       " 'you',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'sorry',\n",
       " ',',\n",
       " 'i',\n",
       " 'didn',\n",
       " \"'t\",\n",
       " 'mean',\n",
       " 'to',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '[chorus]',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '[verse-3]',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'why',\n",
       " 'do',\n",
       " 'hoes',\n",
       " 'be',\n",
       " 'schemin',\n",
       " \"',\",\n",
       " 'dreamin',\n",
       " \"',\",\n",
       " 'actin',\n",
       " \"'\",\n",
       " 'like',\n",
       " 'demons',\n",
       " '?',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'my',\n",
       " 'semen',\n",
       " 'don',\n",
       " \"'t\",\n",
       " 'give',\n",
       " 'a',\n",
       " 'fuck',\n",
       " 'because',\n",
       " 'they',\n",
       " 'beat',\n",
       " 'skins',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'too',\n",
       " 'many',\n",
       " 'girls',\n",
       " 'front',\n",
       " 'cute',\n",
       " 'when',\n",
       " 'they',\n",
       " 'want',\n",
       " 'loot',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'they',\n",
       " 'tell',\n",
       " 'lies',\n",
       " ',',\n",
       " 'the',\n",
       " 'swell',\n",
       " 'guys',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'can',\n",
       " 'peep',\n",
       " 'it',\n",
       " ',',\n",
       " 'and',\n",
       " 'that',\n",
       " \"'s\",\n",
       " 'weak',\n",
       " 'shit',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'that',\n",
       " \"'s\",\n",
       " 'why',\n",
       " 'i',\n",
       " 'run',\n",
       " 'game',\n",
       " 'on',\n",
       " 'every',\n",
       " 'freak',\n",
       " 'i',\n",
       " 'get',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'born',\n",
       " 'in',\n",
       " 'oakland',\n",
       " ',',\n",
       " 'grew',\n",
       " 'up',\n",
       " 'bumpin',\n",
       " \"'\",\n",
       " 'too',\n",
       " '$hort',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'i',\n",
       " 'know',\n",
       " 'the',\n",
       " 'rules',\n",
       " 'on',\n",
       " 'a',\n",
       " 'flooze',\n",
       " ',',\n",
       " 'so',\n",
       " 'who',\n",
       " \"'s\",\n",
       " 'short',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'i',\n",
       " 'once',\n",
       " 'had',\n",
       " 'a',\n",
       " 'dunce',\n",
       " 'ho',\n",
       " ',',\n",
       " 'who',\n",
       " 'smoked',\n",
       " 'blunts',\n",
       " ',',\n",
       " 'yo',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'she',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'get',\n",
       " 'me',\n",
       " 'so',\n",
       " 'i',\n",
       " 'looked',\n",
       " 'at',\n",
       " 'the',\n",
       " 'front',\n",
       " 'door',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'and',\n",
       " 'now',\n",
       " 'she',\n",
       " \"'s\",\n",
       " 'not',\n",
       " 'with',\n",
       " 'me',\n",
       " ',',\n",
       " 'so',\n",
       " 'forget',\n",
       " 'it',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'but',\n",
       " 'every',\n",
       " 'once',\n",
       " 'in',\n",
       " 'a',\n",
       " 'while',\n",
       " 'i',\n",
       " \"'ll\",\n",
       " 'hit',\n",
       " 'it',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'and',\n",
       " 'it',\n",
       " 'happened',\n",
       " 'to',\n",
       " 'be',\n",
       " 'your',\n",
       " 'girl',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'i',\n",
       " 'didn',\n",
       " \"'t\",\n",
       " 'mean',\n",
       " 'to',\n",
       " ',',\n",
       " 'intervene',\n",
       " 'through',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'your',\n",
       " 'relationship',\n",
       " ',',\n",
       " 'and',\n",
       " 'i',\n",
       " 'hate',\n",
       " 'dumb',\n",
       " 'dips',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'but',\n",
       " 'i',\n",
       " 'really',\n",
       " 'can',\n",
       " \"'t\",\n",
       " 'trip',\n",
       " 'when',\n",
       " 'they',\n",
       " 'ride',\n",
       " 'my',\n",
       " 'tip',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'i',\n",
       " 'didn',\n",
       " \"'t\",\n",
       " 'mean',\n",
       " 'to',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '[chorus]',\n",
       " '</s>',\n",
       " '<s>',\n",
       " '</s>',\n",
       " '</d>']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ULMFiT Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10002\n"
     ]
    }
   ],
   "source": [
    "data_lm = TextLMDataBunch.from_tokens(token_path,\n",
    "                                      bs=128,\n",
    "                                      max_vocab=10000)\n",
    "print(data_lm.train_ds.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([95, 128]) torch.Size([12160])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;d&gt;</td>\n",
       "      <td>la</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>ticking</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>us</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>it</td>\n",
       "      <td>bumper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>vie</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>and</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>if</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>shines</td>\n",
       "      <td>inna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>des</td>\n",
       "      <td>i</td>\n",
       "      <td>the</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>my</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>like</td>\n",
       "      <td>di</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>xxunk</td>\n",
       "      <td>know</td>\n",
       "      <td>fuse</td>\n",
       "      <td>talking</td>\n",
       "      <td>on</td>\n",
       "      <td>echoes</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>laser</td>\n",
       "      <td>air</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>,</td>\n",
       "      <td>what</td>\n",
       "      <td>is</td>\n",
       "      <td>off</td>\n",
       "      <td>some</td>\n",
       "      <td>can</td>\n",
       "      <td>jose</td>\n",
       "      <td>light</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>xxunk</td>\n",
       "      <td>we</td>\n",
       "      <td>burning</td>\n",
       "      <td>about</td>\n",
       "      <td>xxunk</td>\n",
       "      <td>reach</td>\n",
       "      <td>:</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[verse-1]</td>\n",
       "      <td>,</td>\n",
       "      <td>have</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>bro</td>\n",
       "      <td>shit</td>\n",
       "      <td>your</td>\n",
       "      <td>oh</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>j</td>\n",
       "      <td>is</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>(we</td>\n",
       "      <td>ears</td>\n",
       "      <td>listen</td>\n",
       "      <td>(on</td>\n",
       "      <td>jook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>’suis</td>\n",
       "      <td>a</td>\n",
       "      <td>i</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>in</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>i</td>\n",
       "      <td>the</td>\n",
       "      <td>gal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alright</td>\n",
       "      <td>dans</td>\n",
       "      <td>lie</td>\n",
       "      <td>need</td>\n",
       "      <td>you</td>\n",
       "      <td>the</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>'m</td>\n",
       "      <td>run</td>\n",
       "      <td>(jook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>,</td>\n",
       "      <td>mon</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>you</td>\n",
       "      <td>fucking</td>\n",
       "      <td>cut</td>\n",
       "      <td>then</td>\n",
       "      <td>here</td>\n",
       "      <td>)</td>\n",
       "      <td>gal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>i</td>\n",
       "      <td>xxunk</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>now</td>\n",
       "      <td>love</td>\n",
       "      <td>)</td>\n",
       "      <td>i</td>\n",
       "      <td>to</td>\n",
       "      <td>it</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>might</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>it</td>\n",
       "      <td>oh</td>\n",
       "      <td>it</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>could</td>\n",
       "      <td>light</td>\n",
       "      <td>'s</td>\n",
       "      <td>jook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>'s</td>\n",
       "      <td>please</td>\n",
       "      <td>,</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>save</td>\n",
       "      <td>your</td>\n",
       "      <td>in</td>\n",
       "      <td>gal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>c</td>\n",
       "      <td>a</td>\n",
       "      <td>don</td>\n",
       "      <td>bitch</td>\n",
       "      <td>‘til</td>\n",
       "      <td>my</td>\n",
       "      <td>fire</td>\n",
       "      <td>my</td>\n",
       "      <td>(jook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>have</td>\n",
       "      <td>’est</td>\n",
       "      <td>lie</td>\n",
       "      <td>'t</td>\n",
       "      <td>i</td>\n",
       "      <td>we</td>\n",
       "      <td>calls</td>\n",
       "      <td>baby</td>\n",
       "      <td>dreams</td>\n",
       "      <td>gal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>had</td>\n",
       "      <td>mon</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>turn</td>\n",
       "      <td>know</td>\n",
       "      <td>peel</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>at</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>a</td>\n",
       "      <td>xxunk</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>me</td>\n",
       "      <td>,</td>\n",
       "      <td>like</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>night</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>little</td>\n",
       "      <td>de</td>\n",
       "      <td>and</td>\n",
       "      <td>away</td>\n",
       "      <td>know</td>\n",
       "      <td>a</td>\n",
       "      <td>what</td>\n",
       "      <td>cause</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>xxunk</td>\n",
       "      <td>xxunk</td>\n",
       "      <td>it</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>,</td>\n",
       "      <td>xxunk</td>\n",
       "      <td>happens</td>\n",
       "      <td>you</td>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>junk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0      1     2        3        4      5        6       7       8  \\\n",
       "0         <d>     la  </s>  ticking      <s>     us      <s>    </s>      it   \n",
       "1         <s>    vie   <s>      and     </s>   </s>       if     <s>  shines   \n",
       "2        </s>    des     i      the      <s>    <s>       my    </s>    like   \n",
       "3         <s>  xxunk  know     fuse  talking     on   echoes     <s>   laser   \n",
       "4        </s>      ,  what       is      off   some      can    jose   light   \n",
       "5         <s>  xxunk    we  burning    about  xxunk    reach       :    </s>   \n",
       "6   [verse-1]      ,  have     </s>      bro   shit     your      oh     <s>   \n",
       "7        </s>      j    is      <s>     </s>    (we     ears  listen     (on   \n",
       "8         <s>  ’suis     a        i      <s>     in     </s>       i     the   \n",
       "9     alright   dans   lie     need      you    the      <s>      'm     run   \n",
       "10          ,    mon  </s>      you  fucking    cut     then    here       )   \n",
       "11          i  xxunk   <s>      now     love      )        i      to      it   \n",
       "12      might   </s>    it       oh       it   </s>    could   light      's   \n",
       "13       </s>    <s>    's   please        ,    <s>     save    your      in   \n",
       "14        <s>      c     a      don    bitch   ‘til       my    fire      my   \n",
       "15       have   ’est   lie       't        i     we    calls    baby  dreams   \n",
       "16        had    mon  </s>     turn     know   peel     </s>    </s>      at   \n",
       "17          a  xxunk   <s>       me        ,   like      <s>     <s>   night   \n",
       "18     little     de   and     away     know      a     what   cause    </s>   \n",
       "19      xxunk  xxunk    it     </s>        ,  xxunk  happens     you     <s>   \n",
       "\n",
       "         9  \n",
       "0   bumper  \n",
       "1     inna  \n",
       "2       di  \n",
       "3      air  \n",
       "4     </s>  \n",
       "5      <s>  \n",
       "6      and  \n",
       "7     jook  \n",
       "8      gal  \n",
       "9    (jook  \n",
       "10     gal  \n",
       "11       )  \n",
       "12    jook  \n",
       "13     gal  \n",
       "14   (jook  \n",
       "15     gal  \n",
       "16       )  \n",
       "17    </s>  \n",
       "18     <s>  \n",
       "19    junk  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = next(iter(data_lm.train_dl))\n",
    "example = x[:20,:10].cpu()\n",
    "texts = pd.DataFrame([data_lm.train_ds.vocab.textify(l).split(' ') for l in example])\n",
    "print(x.shape, y.shape)\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ULMFiT Model architecture and create and embedding matrix that includes the new words. The new words are initialized to the mean value of all prior vocab...\n",
    "\n",
    "TODO: maybe update the initialization points to the mean value of prior vocab that we keep in this model. e.g. average of the words that are in the lyrics corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): RNNCore(\n",
       "    (encoder): Embedding(10002, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(10002, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1150)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1150, 1150)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1150, 400)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=10002, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = RNNLearner.language_model(data_lm,\n",
    "                                  pretrained_fnames=['lstm_wt103', 'itos_wt103'],\n",
    "                                  drop_mult=0.5)\n",
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a glance at the new words that we've added to our vocabularly -- we add quite a bit. This is expected because this is such a specialized corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Vocab:  3200\n",
      "['<s>', '</s>', \"'t\", \"'m\", '<d>', '</d>', \"'ve\", 'gonna', '[chorus]', 'nigga', 'gotta', \"'cause\", '’t', '’s', '[hook]', '[verse-1]', '[verse-2]', '’m', \"'all\", \"',\"]\n"
     ]
    }
   ],
   "source": [
    "with open('../data/imdb_sample/models/itos_wt103.pkl', 'rb') as f:\n",
    "    aa = pickle.load(f)\n",
    "aa = set(aa)\n",
    "bb = data_lm.train_ds.vocab.itos\n",
    "new_words = [w for w in bb if w not in aa]\n",
    "new_words_id = data_lm.train_ds.vocab.numericalize(new_words)\n",
    "print(\"New Vocab: \", len(new_words))\n",
    "print(new_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1810,  0.5646, -0.1060,  ...,  0.4994,  0.0770, -0.2302],\n",
       "        [ 0.1604,  0.4524,  0.0713,  ..., -0.0967, -0.1500, -0.2420],\n",
       "        [ 0.1753,  0.5136,  0.0942,  ..., -0.1172, -0.1426, -0.2452],\n",
       "        ...,\n",
       "        [ 0.1253,  0.3070, -0.0439,  ..., -0.2400, -0.1736, -0.2471],\n",
       "        [ 0.3863,  0.1121, -0.0464,  ...,  0.1902, -0.1707, -0.1216],\n",
       "        [ 0.6130,  0.5969, -0.0734,  ..., -0.0203, -0.0900, -0.2581]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model.state_dict().get('0.encoder.weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit one cycle, but keep all layers frozen except the linear encoder and decoder. Start with a realtively low learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, max=1), HTML(value='0.00% [0/1 00:00<00:00]'))), HTML(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 04:01\n",
      "epoch  train loss  valid loss  accuracy\n",
      "0      4.169597    3.902785    0.330811  (04:01)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2691,  0.6207, -0.3944,  ...,  0.5027,  0.2028, -0.2300],\n",
       "        [ 0.2803,  0.9418, -0.1011,  ..., -0.3540, -0.0756, -0.4386],\n",
       "        [-0.3792,  0.4529,  0.2847,  ..., -0.3854, -0.2198, -0.2500],\n",
       "        ...,\n",
       "        [ 0.2301,  0.5036, -0.1625,  ..., -0.2492, -0.0224, -0.2389],\n",
       "        [ 0.5351,  0.1089, -0.0427,  ...,  0.1091, -0.1642, -0.0938],\n",
       "        [ 0.5106,  0.5838, -0.0701,  ..., -0.0717,  0.0637, -0.2155]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model.state_dict().get('0.encoder.weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, max=5), HTML(value='0.00% [0/5 00:00<00:00]'))), HTML(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 24:05\n",
      "epoch  train loss  valid loss  accuracy\n",
      "0      3.842185    3.609371    0.372844  (04:50)\n",
      "1      3.633488    3.416342    0.394314  (04:48)\n",
      "2      3.493614    3.282526    0.409824  (04:48)\n",
      "3      3.388098    3.205011    0.418733  (04:47)\n",
      "4      3.360687    3.182761    0.421562  (04:51)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(5, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('10k-subset-custom-tok')\n",
    "learn.load('10k-subset-custom-tok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/syang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = learn.model\n",
    "# s = word_tokenize(\"<d><s></s><s></s>\")\n",
    "s = ['<d>', '<s>', '</s>', '<s>', '</s>',\n",
    "     '<s>', '[verse]', '</s>']\n",
    "# s = ['<d>']\n",
    "t = LongTensor(data_lm.train_ds.vocab.numericalize(s)).view(-1,1).cuda()\n",
    "t = torch.autograd.Variable(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<d> <s> \n",
      " <s> \n",
      " <s> [verse] \n",
      " <s> everything looks good like town \n",
      " <s> but maybe every night i 'll be your man \n",
      " <s> \n",
      " <s> [chorus] \n",
      " <s> my little girl is in a love song \n",
      " <s> she 's been at home after midnight night \n",
      " <s> a night long ago , honey \n",
      " <s> \n",
      " <s> [verse-3] \n",
      " <s> well , she was ten feet tall than her \n",
      " <s> child 's dress , but she was was looking back \n",
      " <s> she mean you were a bad man ; she been and then she was dancing \n",
      " <s> had to get her friends somethin ' too hard and she had trouble \n",
      " <s> ain 't no place for me to hold her smiling \n",
      " <s> my temperature was so warm that every day you stopped song \n",
      " <s> so you should put me down and i 'll do something \n",
      " <s> but never be well home \n",
      " <s> \n",
      " <s> [pre-hook] \n",
      " <s> (it 's all so alright \n",
      " <s> this one is a grown man <s> \n",
      " <s> [outro] \n",
      " <s> well i 'm sure \n",
      " <s> i 'll be climbing saturday night \n",
      " <s> \n",
      " <s> shining in high heels \n",
      " <s> shining up in a cradle \n",
      " <s> i 'm smile \n",
      " <s> i know that you 'll find me \n",
      " <s> there 's a key \n",
      " <s> \n",
      " <s> [verse-2] \n",
      " <s> only babe and alone \n",
      " <s> baby , you know , i know i 'm "
     ]
    }
   ],
   "source": [
    "# generation with multinomial\n",
    "# there is some bug with torch library with torch.multinomial\n",
    "# that throws and exception and make you restart the kernel\n",
    "# reverting to numpy\n",
    "m.reset()\n",
    "m.eval()\n",
    "\n",
    "# print starting seed\n",
    "for s in t:\n",
    "    to_print = data_lm.valid_ds.vocab.textify(s)\n",
    "    if to_print == '</s>':\n",
    "        to_print = '\\n'\n",
    "    print(to_print, end=' ')\n",
    "\n",
    "for i in range(250):\n",
    "    # forward pass\n",
    "    res, *_ = m(t)\n",
    "    r = res[-1]\n",
    "    \n",
    "    # set unk and pad to 0 prob\n",
    "    r[0] = -np.inf\n",
    "    r[1] = -np.inf\n",
    "    \n",
    "    # softmax\n",
    "    r2 = F.softmax(r, dim=0)\n",
    "    r2 = np.asarray(r2.detach().cpu(), dtype=np.float)\n",
    "    r2 /= np.sum(r2) # solve rounding issues for multinom function\n",
    "    \n",
    "    # draw multinom\n",
    "    n = np.argmax(np.random.multinomial(1, r2))\n",
    "    \n",
    "    word = data_lm.valid_ds.vocab.textify([n])\n",
    "    n = LongTensor([n]).view(-1, 1).cuda()\n",
    "    t = torch.cat((t, n))    \n",
    "    if word == '</s>':\n",
    "        word = '\\n'\n",
    "    print(word, end=' ')\n",
    "    if word == '</d>': break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.26910752,  0.6206992 , -0.39436367, ...,  0.5027352 ,\n",
       "         0.202811  , -0.2299545 ],\n",
       "       [ 0.28033113,  0.9418297 , -0.10114889, ..., -0.35397708,\n",
       "        -0.07556843, -0.43856415],\n",
       "       [-0.37922502,  0.4529314 ,  0.2846954 , ..., -0.3854074 ,\n",
       "        -0.2197569 , -0.24996242],\n",
       "       ...,\n",
       "       [ 0.23007387,  0.5036217 , -0.1624992 , ..., -0.24915165,\n",
       "        -0.02237413, -0.2388532 ],\n",
       "       [ 0.535128  ,  0.10889628, -0.04272555, ...,  0.10906475,\n",
       "        -0.16416293, -0.09379104],\n",
       "       [ 0.5106017 ,  0.5837977 , -0.07005332, ..., -0.07169558,\n",
       "         0.06370288, -0.21551183]], dtype=float32)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = learn.model.state_dict().get('0.encoder.weight').cpu().numpy()\n",
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embed = pd.DataFrame(data=embed,\n",
    "                        index=data_lm.train_ds.vocab.itos)\n",
    "df_embed.to_csv('../data/models/embeddings.csv',\n",
    "                sep='\\t',\n",
    "                index=False,\n",
    "                header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.DataFrame(data=data_lm.train_ds.vocab.itos,\n",
    "                       columns=['token'])\n",
    "df_meta.to_csv('../data/models/embeddings_meta.csv',\n",
    "               sep='\\t',\n",
    "               header=False,\n",
    "               index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.26910752,  0.6206992 , -0.39436367,  0.5027352 ,  0.202811  ,\n",
       "        -0.2299545 ],\n",
       "       [ 0.28033113,  0.9418297 , -0.10114889, -0.35397708, -0.07556843,\n",
       "        -0.43856415],\n",
       "       [-0.37922502,  0.4529314 ,  0.2846954 , -0.3854074 , -0.2197569 ,\n",
       "        -0.24996242],\n",
       "       [ 0.23007387,  0.5036217 , -0.1624992 , -0.24915165, -0.02237413,\n",
       "        -0.2388532 ],\n",
       "       [ 0.535128  ,  0.10889628, -0.04272555,  0.10906475, -0.16416293,\n",
       "        -0.09379104],\n",
       "       [ 0.5106017 ,  0.5837977 , -0.07005332, -0.07169558,  0.06370288,\n",
       "        -0.21551183]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[ 0.26910752,  0.6206992 , -0.39436367, 0.5027352 ,\n",
    "         0.202811  , -0.2299545 ],\n",
    "       [ 0.28033113,  0.9418297 , -0.10114889, -0.35397708,\n",
    "        -0.07556843, -0.43856415],\n",
    "       [-0.37922502,  0.4529314 ,  0.2846954 , -0.3854074 ,\n",
    "        -0.2197569 , -0.24996242],\n",
    "       [ 0.23007387,  0.5036217 , -0.1624992 , -0.24915165,\n",
    "        -0.02237413, -0.2388532 ],\n",
    "       [ 0.535128  ,  0.10889628, -0.04272555,  0.10906475,\n",
    "        -0.16416293, -0.09379104],\n",
    "       [ 0.5106017 ,  0.5837977 , -0.07005332, -0.07169558,\n",
    "         0.06370288, -0.21551183]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-16ed7b408bda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotly\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import manifold\n",
    "from sklearn.utils import check_random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = manifold.TSNE(n_components=3, init='pca', random_state=0)\n",
    "trans_data = tsne.fit_transform(embed).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = go.Scatter3d(x=trans_data[0], y=trans_data[1], z=trans_data[2],\n",
    "                  mode='markers', \n",
    "                  marker=dict(color=x, \n",
    "                              colorscale=cmap,\n",
    "                              showscale=False,\n",
    "                              line=dict(color='black', width=1)))\n",
    "layout=dict(margin=dict(l=10, r=10,\n",
    "                        t=30, b=10)\n",
    "           )\n",
    "\n",
    "fig = go.Figure(data=[p1], layout=layout)\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.iplot(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
