{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Analysis Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to analyze how multimodal learning has affected our weights. This is different than evaluation on the final generated lyrics. Instead, here we focus of if the features we implemented have \"moved the needle\" in terms of the distribution of the next word.\n",
    "\n",
    "This notebook is meant to be run after the model fitting notebooks. Thus, we will ignore any data collection processes, making the assumption that these have already been executed in the modeling notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.tokenize\n",
    "import itertools\n",
    "import datetime\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "\n",
    "from copy import copy, deepcopy\n",
    "from enum import Enum\n",
    "\n",
    "from graphviz import Digraph\n",
    "\n",
    "import datetime\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from src.data_collection.multimodal_data import *\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Generate Text For Pure LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a `get_probs` option for text generation. For this method, generate a shorter sequence of text with a very wide beam. Then, will perform a bag of words analysis across different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step(learner, context, context_length, temp=1):\n",
    "\n",
    "    model = learner.model\n",
    "    \n",
    "    if GPU:\n",
    "        context = LongTensor(context[-context_length:]).view(-1,1).cuda()\n",
    "    else:\n",
    "        context = LongTensor(context[-context_length:]).view(-1,1).cpu()\n",
    "    \n",
    "    context = torch.autograd.Variable(context)\n",
    "    \n",
    "    model.reset()\n",
    "    model.eval()\n",
    "        \n",
    "    # forward pass the \"context\" into the model\n",
    "    result, *_ = model(context)\n",
    "    result = result[-1]\n",
    "\n",
    "    # set unk and pad to 0 prob\n",
    "    # i.e. never pick unknown or pad\n",
    "    result[0] = -np.inf\n",
    "    result[1] = -np.inf\n",
    "\n",
    "    # softmax and normalize\n",
    "    probabilities = F.softmax(result/temp, dim=0)\n",
    "    probabilities = np.asarray(probabilities.detach().cpu(), dtype=np.float)\n",
    "    probabilities /= np.sum(probabilities) \n",
    "    return probabilities\n",
    "\n",
    "def get_word_from_index(idx):\n",
    "\n",
    "    return data_lm.valid_ds.vocab.textify([idx])\n",
    "\n",
    "\n",
    "def print_words(context):\n",
    "    for i in range(len(context)):\n",
    "        \n",
    "        step = context[i]\n",
    "\n",
    "        word = data_lm.valid_ds.vocab.textify([step])\n",
    "\n",
    "        if word == 'xeol':\n",
    "            word = 'xeol \\n'\n",
    "        elif 'xbol' in word:\n",
    "            word = word\n",
    "        elif word == 'xeos': \n",
    "            print(word)\n",
    "            break\n",
    "            \n",
    "        print(word, end=' ')   \n",
    "\n",
    "def generate_text(learner, seed_text=['xbos'],\n",
    "                  max_len=500, GPU=False, context_length=20,\n",
    "                  beam_width=5, temp=1, multinomial=True,\n",
    "                  verbose=True, graph=False, get_probs=False):\n",
    "    \"\"\"Generates text with a given learner and returns best options.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    learner : RNNLearner Language Model (RNNLearner.language_model())\n",
    "        Fastai RNNLearner with tokenized language model data already loaded \n",
    "        \n",
    "    seed_text : list or str\n",
    "        List of strings where each item is a token. (e.g. ['the', 'cat']) or string that is split on white space\n",
    "\n",
    "    max_len : int\n",
    "        Number of words in generated sequence\n",
    "        \n",
    "    gpu : bool\n",
    "        If you're using a GPU or not...\n",
    "    \n",
    "    context_length : int\n",
    "        Amount of words that get input as \"context\" into the model. Set to 0 for no limit   \n",
    "        \n",
    "    beam_width : int\n",
    "        How many new word indices to try out...computationally expensive\n",
    "    \n",
    "    verbose : bool\n",
    "        If True, prints every possible context for a given word cycle\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    context_and_scores : list of lists\n",
    "        Returns a sorted list of the entire tree search of contexts and their respective scores in the form:\n",
    "        [[context, score], [context, score], ..., [context, score]]\n",
    "    \"\"\"\n",
    "        \n",
    "    if isinstance(seed_text, str):\n",
    "        seed_text = data_lm.train_ds.vocab.numericalize(seed_text.split(' '))\n",
    "    \n",
    "    \n",
    "    # Width for the beam search, to be externalized along with general decoding\n",
    "    beam_width = beam_width\n",
    "    \n",
    "    if graph:\n",
    "        optimization_graph = Digraph()\n",
    "\n",
    "    # List of candidate word sequence. We'll maintain #beam_width top sequences here.\n",
    "    # The context is a list of words, the scores are the sum of the log probabilities of each word\n",
    "    context_and_scores = [[seed_text, 0.0]]\n",
    "    \n",
    "    # Loop over max number of words\n",
    "    for word_number in tqdm(range(max_len)):\n",
    "#         print(f'Generating word: {word_number+1} / {max_len}')\n",
    "\n",
    "        candidates = []\n",
    "        next_word_probs = []\n",
    "        \n",
    "        # For each possible context that we've generated so far, generate new probabilities, \n",
    "        # and pick an additional #beam_width next candidates\n",
    "        for i in range(len(context_and_scores)):\n",
    "            # Get a new sequence of word indices and log-probability\n",
    "            # Example: [[2, 138, 661], 23.181717]\n",
    "            context, score = context_and_scores[i]\n",
    "            \n",
    "            # Obtain probabilities for next word given the context \n",
    "            probabilities = generate_step(learner, context, context_length, temp)\n",
    "\n",
    "            # Multinomial draw from the probabilities\n",
    "            if multinomial:\n",
    "                multinom_draw = np.random.multinomial(beam_width, probabilities)\n",
    "                top_probabilities = np.argwhere(multinom_draw != 0).flatten()                    \n",
    "                \n",
    "            # top-k from probabilities    \n",
    "            else:\n",
    "                top_probabilities = np.argsort(-probabilities)[:beam_width]\n",
    "                        \n",
    "            #For each possible new candidate, update the context and scores\n",
    "            for j in range(len(top_probabilities)):\n",
    "                next_word_idx = top_probabilities[j]\n",
    "                new_context = context + [next_word_idx]\n",
    "                candidate = [new_context, (score - np.log(probabilities[next_word_idx]))]\n",
    "                candidates.append(candidate)\n",
    "                \n",
    "                if get_probs:\n",
    "                    next_word_prob = probabilities[next_word_idx]\n",
    "                    potential_next_word = get_word_from_index(next_word_idx)\n",
    "                    prior_context = [get_word_from_index(w) for w in context]\n",
    "                    next_word_probs.append((prior_context, potential_next_word, next_word_prob))\n",
    "                \n",
    "                if graph:\n",
    "                    optimization_graph.node(\"%d_%d\" % (word_number, next_word_idx), \"%s (%.2f)\" % (get_word_from_index(next_word_idx), candidate[1]))\n",
    "                    optimization_graph.edge(\"%d_%d\" % (word_number - 1, context[len(context) -1]), \"%d_%d\" % (word_number, next_word_idx))\n",
    "                \n",
    "        #update the running tally of context and scores and sort by probability of each entry\n",
    "        context_and_scores = candidates\n",
    "        context_and_scores = sorted(context_and_scores, key = lambda x: x[1]) #sort by top entries\n",
    "\n",
    "        context_and_scores = context_and_scores[:30] #for now, only keep the top 30 to speed things up but we can/should change this to beam_width or something else\n",
    "        \n",
    "        if verbose:\n",
    "            for context, score in context_and_scores:\n",
    "                print_words(context)\n",
    "                print('\\n')\n",
    "\n",
    "    if graph:\n",
    "        now = str(datetime.datetime.now())\n",
    "        optimization_graph.render(directory='graph_viz/', filename=now, cleanup=True)\n",
    "        \n",
    "    if get_probs:\n",
    "        next_word_probs = sorted(next_word_probs, key=lambda x: -x[2])\n",
    "        return next_word_probs\n",
    "        \n",
    "    return context_and_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import stopwords and add my own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/syang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "stop_words.add(\"'t\")\n",
    "stop_words.add(\"'ll\")\n",
    "stop_words.add(\"’t\")\n",
    "stop_words.add(\"'ve\")\n",
    "stop_words.add(\",\")\n",
    "stop_words.add(\"'s\")\n",
    "stop_words.add(\"'re\")\n",
    "stop_words.add(\"'m\")\n",
    "stop_words.add(\"don\")\n",
    "stop_words.add(\"won\")\n",
    "stop_words.add(\"xbol\")\n",
    "stop_words.add(\"xbos\")\n",
    "stop_words.add(\"xeol\")\n",
    "stop_words.add(\"xeos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure Language Model (3.2-ULMFiT-108k)\n",
    "* Transfer learning from wikitext-103 to 108k corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(f'../data/models/3.2-ULMFiT-108k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = TextLMDataBunch.from_tokens(MODEL_PATH,\n",
    "                                      bs=128,\n",
    "                                      max_vocab=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk', 'xxpad', 'xbol', 'xeol', ',', 'i', 'the', 'you', 'to', 'and']"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lm.train_ds.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = RNNLearner.language_model(data_lm,\n",
    "                                  pretrained_fnames=['3.2-ULMFiT-108k_best',\n",
    "                                                     '3.2-ULMFiT-108k_itos'],\n",
    "                                  drop_mult=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.64it/s]\n"
     ]
    }
   ],
   "source": [
    "next_word_probs = generate_text(learn, GPU=GPU,\n",
    "                             seed_text='xbos xbol [verse-1] xeol xbol',\n",
    "                             max_len=2, context_length=200,\n",
    "                             beam_width=1000, verbose=False,\n",
    "                             temp=1, multinomial=False, graph=False, get_probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word_probs = [s for s in next_word_probs if s[1] not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>next_word</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[xbos, xbol, [verse-1], xeol, xbol, one]</td>\n",
       "      <td>day</td>\n",
       "      <td>0.290520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[xbos, xbol, [verse-1], xeol, xbol, no]</td>\n",
       "      <td>one</td>\n",
       "      <td>0.241687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[xbos, xbol, [verse-1], xeol, xbol, they]</td>\n",
       "      <td>say</td>\n",
       "      <td>0.168200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[xbos, xbol, [verse-1], xeol, xbol, here]</td>\n",
       "      <td>comes</td>\n",
       "      <td>0.165289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[xbos, xbol, [verse-1], xeol, xbol, how]</td>\n",
       "      <td>many</td>\n",
       "      <td>0.128627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[xbos, xbol, [verse-1], xeol, xbol, my]</td>\n",
       "      <td>heart</td>\n",
       "      <td>0.094780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[xbos, xbol, [verse-1], xeol, xbol, so]</td>\n",
       "      <td>many</td>\n",
       "      <td>0.085646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[xbos, xbol, [verse-1], xeol, xbol, how]</td>\n",
       "      <td>long</td>\n",
       "      <td>0.074577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[xbos, xbol, [verse-1], xeol, xbol, no]</td>\n",
       "      <td>matter</td>\n",
       "      <td>0.071768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[xbos, xbol, [verse-1], xeol, xbol, one]</td>\n",
       "      <td>night</td>\n",
       "      <td>0.071546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     context next_word  probability\n",
       "0   [xbos, xbol, [verse-1], xeol, xbol, one]       day     0.290520\n",
       "1    [xbos, xbol, [verse-1], xeol, xbol, no]       one     0.241687\n",
       "2  [xbos, xbol, [verse-1], xeol, xbol, they]       say     0.168200\n",
       "3  [xbos, xbol, [verse-1], xeol, xbol, here]     comes     0.165289\n",
       "4   [xbos, xbol, [verse-1], xeol, xbol, how]      many     0.128627\n",
       "5    [xbos, xbol, [verse-1], xeol, xbol, my]     heart     0.094780\n",
       "6    [xbos, xbol, [verse-1], xeol, xbol, so]      many     0.085646\n",
       "7   [xbos, xbol, [verse-1], xeol, xbol, how]      long     0.074577\n",
       "8    [xbos, xbol, [verse-1], xeol, xbol, no]    matter     0.071768\n",
       "9   [xbos, xbol, [verse-1], xeol, xbol, one]     night     0.071546"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(next_word_probs, columns=['context', 'next_word', 'probability'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>next_word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>3.362332e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>2.910156e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>many</th>\n",
       "      <td>2.176971e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>say</th>\n",
       "      <td>2.010951e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>2.009788e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comes</th>\n",
       "      <td>1.932980e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>1.732307e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>1.673621e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart</th>\n",
       "      <td>1.511759e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>got</th>\n",
       "      <td>1.465183e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>could</th>\n",
       "      <td>1.396111e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long</th>\n",
       "      <td>1.272330e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>night</th>\n",
       "      <td>1.248038e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>1.194325e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>1.101350e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>man</th>\n",
       "      <td>1.064807e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eyes</th>\n",
       "      <td>1.041544e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baby</th>\n",
       "      <td>1.036580e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>7.674185e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matter</th>\n",
       "      <td>7.251915e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>came</th>\n",
       "      <td>7.071555e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>girl</th>\n",
       "      <td>6.657870e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>used</th>\n",
       "      <td>6.636602e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>never</th>\n",
       "      <td>6.574491e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>life</th>\n",
       "      <td>6.539243e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>took</th>\n",
       "      <td>6.441442e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>6.430798e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>face</th>\n",
       "      <td>6.378402e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world</th>\n",
       "      <td>5.686991e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>5.429876e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>co</th>\n",
       "      <td>9.627701e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jazz</th>\n",
       "      <td>9.582232e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cream</th>\n",
       "      <td>9.480978e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'en</th>\n",
       "      <td>9.358466e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'l</th>\n",
       "      <td>9.308627e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ka</th>\n",
       "      <td>9.283582e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>+</th>\n",
       "      <td>1.964950e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leading</th>\n",
       "      <td>1.225776e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>following</th>\n",
       "      <td>1.173724e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>9.144422e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starting</th>\n",
       "      <td>8.829972e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.732445e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>becoming</th>\n",
       "      <td>8.534373e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>using</th>\n",
       "      <td>7.497474e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carrying</th>\n",
       "      <td>7.160597e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>design</th>\n",
       "      <td>6.900913e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>towards</th>\n",
       "      <td>6.564782e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fourteen</th>\n",
       "      <td>6.175258e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mostly</th>\n",
       "      <td>6.154774e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[verse]</th>\n",
       "      <td>5.842068e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eleven</th>\n",
       "      <td>5.779879e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>5.758642e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>backwards</th>\n",
       "      <td>5.567018e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refrain</th>\n",
       "      <td>5.471007e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>5.429788e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blowing</th>\n",
       "      <td>5.409609e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>otherwise</th>\n",
       "      <td>5.299578e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>5.256109e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>version</th>\n",
       "      <td>5.213993e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strip</th>\n",
       "      <td>5.193349e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4016 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            probability\n",
       "next_word              \n",
       "day        3.362332e-01\n",
       "one        2.910156e-01\n",
       "many       2.176971e-01\n",
       "say        2.010951e-01\n",
       "said       2.009788e-01\n",
       "comes      1.932980e-01\n",
       "love       1.732307e-01\n",
       "time       1.673621e-01\n",
       "heart      1.511759e-01\n",
       "got        1.465183e-01\n",
       "could      1.396111e-01\n",
       "long       1.272330e-01\n",
       "night      1.248038e-01\n",
       "know       1.194325e-01\n",
       "little     1.101350e-01\n",
       "man        1.064807e-01\n",
       "eyes       1.041544e-01\n",
       "baby       1.036580e-01\n",
       "come       7.674185e-02\n",
       "matter     7.251915e-02\n",
       "came       7.071555e-02\n",
       "girl       6.657870e-02\n",
       "used       6.636602e-02\n",
       "never      6.574491e-02\n",
       "life       6.539243e-02\n",
       "took       6.441442e-02\n",
       "!          6.430798e-02\n",
       "face       6.378402e-02\n",
       "world      5.686991e-02\n",
       "way        5.429876e-02\n",
       "...                 ...\n",
       "co         9.627701e-07\n",
       "jazz       9.582232e-07\n",
       "cream      9.480978e-07\n",
       "'en        9.358466e-07\n",
       "'l         9.308627e-07\n",
       "ka         9.283582e-07\n",
       "+          1.964950e-07\n",
       "leading    1.225776e-07\n",
       "following  1.173724e-07\n",
       "(          9.144422e-08\n",
       "starting   8.829972e-08\n",
       "30         8.732445e-08\n",
       "becoming   8.534373e-08\n",
       "using      7.497474e-08\n",
       "carrying   7.160597e-08\n",
       "design     6.900913e-08\n",
       "towards    6.564782e-08\n",
       "fourteen   6.175258e-08\n",
       "mostly     6.154774e-08\n",
       "[verse]    5.842068e-08\n",
       "eleven     5.779879e-08\n",
       "support    5.758642e-08\n",
       "backwards  5.567018e-08\n",
       "refrain    5.471007e-08\n",
       "100        5.429788e-08\n",
       "blowing    5.409609e-08\n",
       "otherwise  5.299578e-08\n",
       "2000       5.256109e-08\n",
       "version    5.213993e-08\n",
       "strip      5.193349e-08\n",
       "\n",
       "[4016 rows x 1 columns]"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('next_word').sum().sort_values(by='probability', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAE6VJREFUeJzt3X+QXfV93vH3Y1QMrmKEx7HqQWpFxkpag1wPbIA0TbsyCcgmtWgLHTpMLFxaTT3YJRk8sUjGJf5ZOTEh8SR2RxNpCrYnG0KcorHsEFW24knHYBBgxI9gZEzxAoG4UmhUsBuZT/+4ZyeX/e5Ky72rvTfm/ZrR7D3f8z33PGLv8uz5ca9SVUiS1O8Vow4gSRo/loMkqWE5SJIaloMkqWE5SJIaloMkqXHMckiyI8kzSe7vG3tNkt1JHum+ntqNJ8knkhxIcl+Ss/q22dTNfyTJpr7xs5Ps77b5RJIs9l9SkvTSLOTI4b8BG2aNbQH2VNVaYE+3DPBWYG33ZzPwKeiVCXAdcC5wDnDdTKF0czb3bTd7X5KkJXbMcqiqrwAHZw1vBG7sHt8IXNw3flP13A6sSPJ64EJgd1UdrKpDwG5gQ7fu1VX11eq9G++mvueSJI3IsgG3W1lVTwFU1VNJXteNnwZ8u2/edDd2tPHpOcbnlGQzvaMMTj755LNXr149YPzBvfDCC7ziFeN5qcZsgxnnbDDe+cw2mFFl+8Y3vvGdqvrhhcwdtBzmM9f1ghpgfE5VtQ3YBjAxMVF33XXXIBmHsnfvXiYnJ5d8vwthtsGMczYY73xmG8yosiX5XwudO2h1Pd2dEqL7+kw3Pg30/zq/CnjyGOOr5hiXJI3QoOWwE5i542gTcGvf+Du6u5bOA57tTj/dBlyQ5NTuQvQFwG3dur9Kcl53l9I7+p5LkjQixzytlOR3gUngtUmm6d11tBW4OcmVwOPApd30LwBvAw4AzwHvBKiqg0k+BNzZzftgVc1c5H4XvTuiTga+2P2RJI3QMcuhqv7tPKvOn2NuAVfN8zw7gB1zjN8FnHmsHJKkpTOel/IlSSNlOUiSGpaDJKlhOUiSGpaDJKmx2O+QlsbGmi27FjTvmnVHuGKBcxfisa0XLdpzSaPikYMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqTFUOST5hSQPJLk/ye8mOSnJ6UnuSPJIkt9LcmI395Xd8oFu/Zq+57m2G384yYXD/ZUkScMauBySnAb8J2Ciqs4ETgAuAz4G3FBVa4FDwJXdJlcCh6rqDcAN3TySvLHb7gxgA/DJJCcMmkuSNLxhTystA05Osgx4FfAU8Bbglm79jcDF3eON3TLd+vOTpBufqqrvVdW3gAPAOUPmkiQNIVU1+MbJ1cBHgOeBPwauBm7vjg5Ishr4YlWdmeR+YENVTXfrvgmcC/xKt81nuvHt3Ta3zLG/zcBmgJUrV549NTU1cPZBHT58mOXLly/5fhfCbC+2/4lnFzRv5cnw9POLt991p52yeE+G39dBma21fv36fVU1sZC5ywbdSZJT6f3Wfzrwl8DvA2+dY+pM+2SedfONt4NV24BtABMTEzU5OfnSQi+CvXv3Mor9LoTZXuyKLbsWNO+adUe4fv/APwqNxy6fXLTnAr+vgzLbcIY5rfTTwLeq6i+q6q+BzwH/BFjRnWYCWAU82T2eBlYDdOtPAQ72j8+xjSRpBIYph8eB85K8qrt2cD7wIPBl4JJuzibg1u7xzm6Zbv2XqndOaydwWXc30+nAWuBrQ+SSJA1p4GPpqrojyS3A3cAR4B56p3x2AVNJPtyNbe822Q58OskBekcMl3XP80CSm+kVyxHgqqr6/qC5JEnDG+pEa1VdB1w3a/hR5rjbqKq+C1w6z/N8hN6FbUnSGPAd0pKkhuUgSWpYDpKkhuUgSWpYDpKkhuUgSWpYDpKkhuUgSWpYDpKkhuUgSWpYDpKkhuUgSWpYDpKkhuUgSWpYDpKkhuUgSWpYDpKkhuUgSWpYDpKkhuUgSWpYDpKkhuUgSWpYDpKkxrJRB9APtjVbdgFwzbojXNE9ljT+PHKQJDUsB0lSw3KQJDUsB0lSw3KQJDUsB0lSw3KQJDUsB0lSw3KQJDUsB0lSY6hySLIiyS1J/izJQ0l+IslrkuxO8kj39dRubpJ8IsmBJPclOavveTZ18x9JsmnYv5QkaTjDHjn8JvBHVfUPgX8MPARsAfZU1VpgT7cM8FZgbfdnM/ApgCSvAa4DzgXOAa6bKRRJ0mgMXA5JXg38M2A7QFX9v6r6S2AjcGM37Ubg4u7xRuCm6rkdWJHk9cCFwO6qOlhVh4DdwIZBc0mShpeqGmzD5M3ANuBBekcN+4CrgSeqakXfvENVdWqSzwNbq+pPu/E9wPuASeCkqvpwN/5+4Pmq+vgc+9xM76iDlStXnj01NTVQ9mEcPnyY5cuXL/l+F2Ics+1/4lkAVp4MTz8/4jDzWOxs6047ZfGejPH8vs4w22BGlW39+vX7qmpiIXOH+cjuZcBZwHuq6o4kv8nfnEKaS+YYq6OMt4NV2+gVEhMTEzU5OfmSAi+GvXv3Mor9LsQ4Zrui7yO7r98/np8Qv9jZHrt8ctGeC8bz+zrDbIMZ52wzhrnmMA1MV9Ud3fIt9Mri6e50Ed3XZ/rmr+7bfhXw5FHGJUkjMvCvS1X150m+neTHquph4Hx6p5geBDYBW7uvt3ab7ATenWSK3sXnZ6vqqSS3AR/tuwh9AXDtoLmkUVuzyP+o0UL/oaTHtl60qPvVy9uwx9LvAT6b5ETgUeCd9I5Gbk5yJfA4cGk39wvA24ADwHPdXKrqYJIPAXd28z5YVQeHzCVJGsJQ5VBV9wJzXdw4f465BVw1z/PsAHYMk0WStHh8h7QkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqWE5SJIaloMkqTF0OSQ5Ick9ST7fLZ+e5I4kjyT5vSQnduOv7JYPdOvX9D3Htd34w0kuHDaTJGk4i3HkcDXwUN/yx4AbqmotcAi4shu/EjhUVW8AbujmkeSNwGXAGcAG4JNJTliEXJKkAQ1VDklWARcBv9MtB3gLcEs35Ubg4u7xxm6Zbv353fyNwFRVfa+qvgUcAM4ZJpckaTipqsE3Tm4B/gvwQ8B7gSuA27ujA5KsBr5YVWcmuR/YUFXT3bpvAucCv9Jt85lufHu3zS2zdkeSzcBmgJUrV549NTU1cPZBHT58mOXLly/5fhdiHLPtf+JZAFaeDE8/P+Iw8xjnbLDwfOtOO+X4h5llHF9zM8zWWr9+/b6qmljI3GWD7iTJzwLPVNW+JJMzw3NMrWOsO9o2Lx6s2gZsA5iYmKjJycm5ph1Xe/fuZRT7XYhxzHbFll0AXLPuCNfvH/jldlyNczZYeL7HLp88/mFmGcfX3AyzDWeYn4ifBN6e5G3AScCrgd8AViRZVlVHgFXAk938aWA1MJ1kGXAKcLBvfEb/NpKkERj4mkNVXVtVq6pqDb0Lyl+qqsuBLwOXdNM2Abd2j3d2y3Trv1S9c1o7gcu6u5lOB9YCXxs0lyRpeMfjWPp9wFSSDwP3ANu78e3Ap5McoHfEcBlAVT2Q5GbgQeAIcFVVff845JIkLdCilENV7QX2do8fZY67jarqu8Cl82z/EeAji5FFkjQ83yEtSWpYDpKkhuUgSWpYDpKkhuUgSWpYDpKkhuUgSWpYDpKkhuUgSWpYDpKkhuUgSWpYDpKkhuUgSWpYDpKkhuUgSWpYDpKkhuUgSWpYDpKkhuUgSWpYDpKkhuUgSWpYDpKkhuUgSWpYDpKkhuUgSWpYDpKkhuUgSWpYDpKkhuUgSWpYDpKkhuUgSWpYDpKkhuUgSWpYDpKkxsDlkGR1ki8neSjJA0mu7sZfk2R3kke6r6d240nyiSQHktyX5Ky+59rUzX8kyabh/1qSpGEMc+RwBLimqv4RcB5wVZI3AluAPVW1FtjTLQO8FVjb/dkMfAp6ZQJcB5wLnANcN1MokqTRGLgcquqpqrq7e/xXwEPAacBG4MZu2o3Axd3jjcBN1XM7sCLJ64ELgd1VdbCqDgG7gQ2D5pIkDS9VNfyTJGuArwBnAo9X1Yq+dYeq6tQknwe2VtWfduN7gPcBk8BJVfXhbvz9wPNV9fE59rOZ3lEHK1euPHtqamro7C/V4cOHWb58+ZLvdyHGMdv+J54FYOXJ8PTzIw4zj3HOBgvPt+60U45/mFnG8TU3w2yt9evX76uqiYXMXTbszpIsB/4A+Pmq+j9J5p06x1gdZbwdrNoGbAOYmJioycnJl5x3WHv37mUU+12Iccx2xZZdAFyz7gjX7x/65XZcjHM2WHi+xy6fPP5hZhnH19wMsw1nqLuVkvwdesXw2ar6XDf8dHe6iO7rM934NLC6b/NVwJNHGZckjcgwdysF2A48VFW/3rdqJzBzx9Em4Na+8Xd0dy2dBzxbVU8BtwEXJDm1uxB9QTcmSRqRYY6lfxL4OWB/knu7sV8CtgI3J7kSeBy4tFv3BeBtwAHgOeCdAFV1MMmHgDu7eR+sqoND5JJeltZ0p/CW0jXrjnDFll08tvWiJd+3jq+By6G7sDzfBYbz55hfwFXzPNcOYMegWSRJi8t3SEuSGpaDJKlhOUiSGpaDJKlhOUiSGpaDJKlhOUiSGpaDJKlhOUiSGpaDJKlhOUiSGuP7IfZaVKP4UDZJf3t55CBJalgOkqSG5SBJalgOkqSG5SBJalgOkqSG5SBJalgOkqSG5SBJarws3yE9zLuFr1l3hCsG3P6xrRcNvF9JWkoeOUiSGpaDJKlhOUiSGpaDJKlhOUiSGi/Lu5UkLa5R/Xsh3gF4/HjkIElqWA6SpIblIElqWA6SpIYXpJfQ8b5oN8xHe0hSP48cJEkNy0GS1BibckiyIcnDSQ4k2TLqPJL0cjYW1xySnAD8NvAzwDRwZ5KdVfXgaJNJGmdHu453vK/B/aC/AW8sygE4BzhQVY8CJJkCNgKWg6Sx9IP+78KkqpZkR0cNkVwCbKiqf98t/xxwblW9e9a8zcDmbvHHgIeXNGjPa4HvjGC/C2G2wYxzNhjvfGYbzKiy/YOq+uGFTByXI4fMMda0VlVtA7Yd/zjzS3JXVU2MMsN8zDaYcc4G453PbIMZ52wzxuWC9DSwum95FfDkiLJI0sveuJTDncDaJKcnORG4DNg54kyS9LI1FqeVqupIkncDtwEnADuq6oERx5rPSE9rHYPZBjPO2WC885ltMOOcDRiTC9KSpPEyLqeVJEljxHKQJDUshwVKcmmSB5K8kGRi1ro3Jflqt35/kpPGJVu3/u8nOZzkvUuZ62jZkvxMkn3df699Sd4yLtm6ddd2H+XycJILlzrbrCxvTnJ7knuT3JXknFHmmUuS93T/rR5I8qujzjNbkvcmqSSvHXWWGUl+LcmfJbkvyR8mWTHqTP0sh4W7H/hXwFf6B5MsAz4D/MeqOgOYBP56HLL1uQH44tLFeZH5sn0H+BdVtQ7YBHx6qYMx//f0jfTumDsD2AB8svuIl1H5VeADVfVm4D93y2MjyXp6n2jwpu5n4OMjjvQiSVbT+2iex0edZZbdwJlV9SbgG8C1I87zImNxt9LfBlX1EEDSvF/vAuC+qvp6N+9/L3G0o2UjycXAo8D/XeJYwPzZquqevsUHgJOSvLKqvjfqbPT+RzfVZflWkgP0PuLlq0uVbZYCXt09PoXxew/Qu4CtM9+7qnpmxHlmuwH4ReDWUQfpV1V/3Ld4O3DJqLLMxSOH4f0oUEluS3J3kl8cdaAZSf4u8D7gA6POcgz/GrhnKYvhGE4Dvt23PN2NjcrPA7+W5Nv0fisfq98w6f0M/FSSO5L8SZIfH3WgGUneDjwx88vbGPt3jO7ofk4eOfRJ8j+AvzfHql+uqvl+61gG/FPgx4HngD1J9lXVnjHI9gHghqo6PNdRxYizzWx7BvAxekdg45JtQR/nspiOlhM4H/iFqvqDJP8G2A789PHM8xLzLQNOBc6j93Nwc5IfqSW6T/4Y2X6J4/TaWoiFvP6S/DJwBPjsUmY7FsuhT1UN8gM3DfxJVX0HIMkXgLOARS2HAbOdC1zSXSBcAbyQ5LtV9VtjkI0kq4A/BN5RVd9czEwzhvieLunHuRwtZ5KbgKu7xd8Hfud4ZpnLMfK9C/hcVwZfS/ICvQ+W+4tRZkuyDjgd+Hr3y9Eq4O4k51TVn48y24wkm4CfBc5fqjJdKE8rDe824E1JXtVdnP7njMlHjVfVT1XVmqpaA/wG8NHFLoZBdXdm7AKurar/Oeo8s+wELkvyyiSnA2uBr40wz5P0XlcAbwEeGWGWufx3erlI8qPAiYzBp6FW1f6qel3fz8A0cNZSFcOxJNlA77Tv26vquVHnmc1yWKAk/zLJNPATwK4ktwFU1SHg1+l9PtS9wN1Vdfz+hZGXkG0cHCXbu4E3AO/vbtG8N8nrxiFb99EtN9Mr+T8Crqqq7y9ltln+A3B9kq8DH+VvPrZ+XOwAfiTJ/cAUsGncfgseU78F/BCwu3v9/9dRB+rnx2dIkhoeOUiSGpaDJKlhOUiSGpaDJKlhOUiSGpaDJKlhOUiSGv8fLBM1btRLvB0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.log(df['probability']).hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:29<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "final_scores = generate_text(learn, GPU=GPU,\n",
    "                             seed_text='xbos xbol [verse-1] xeol xbol',\n",
    "                             max_len=50, context_length=200,\n",
    "                             beam_width=3, verbose=False,\n",
    "                             temp=1.4, multinomial=True, graph=False, get_probs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xbos xbol [verse-1] xeol \n",
      " xbol inside my head there 's something unsaid xeol \n",
      " xbol i 'm down on my knees to keep someone next to me xeol \n",
      " xbol and i don 't know when or why xeol \n",
      " xbol i don 't care what it 's all about xeol \n",
      " xbol xeol \n",
      " xbol [pre-chorus] xeol \n",
      " xbol i 115.52940888716925\n"
     ]
    }
   ],
   "source": [
    "#print all of the final options of songs\n",
    "song, score = final_scores[0]\n",
    "print_words(song)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure Language Model (3.3-ULMFiT-108k)\n",
    "* Transfer learning from model 3.2 to 500k corpus\n",
    "* Transfer learning from 500k corpus to 108k corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(f'../data/models/3.3-ULMFiT-108k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = TextLMDataBunch.from_tokens(MODEL_PATH,\n",
    "                                      bs=128,\n",
    "                                      max_vocab=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk', 'xxpad', 'xeol', ',', 'i', 'the', 'you', 'to', 'and', 'a']"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lm.train_ds.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = RNNLearner.language_model(data_lm,\n",
    "                                  pretrained_fnames=['3.3-ULMFiT-108k_best',\n",
    "                                                     '3.3-ULMFiT-108k_itos'],\n",
    "                                  drop_mult=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "next_word_probs = generate_text(learn, GPU=GPU,\n",
    "                             seed_text='xbos xbol-1 [verse-1] xeol xbol-2',\n",
    "                             max_len=2, context_length=200,\n",
    "                             beam_width=1000, verbose=False,\n",
    "                             temp=1, multinomial=False, graph=False, get_probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word_probs = [s for s in next_word_probs if s[1] not in stop_words]\n",
    "next_word_probs = [s for s in next_word_probs if 'xbol' not in s[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>next_word</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[xbos, xbol-1, [verse-1], xeol, xbol-2, one]</td>\n",
       "      <td>day</td>\n",
       "      <td>0.251755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[xbos, xbol-1, [verse-1], xeol, xbol-2, no]</td>\n",
       "      <td>one</td>\n",
       "      <td>0.201328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[xbos, xbol-1, [verse-1], xeol, xbol-2, they]</td>\n",
       "      <td>say</td>\n",
       "      <td>0.191211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[xbos, xbol-1, [verse-1], xeol, xbol-2, so]</td>\n",
       "      <td>many</td>\n",
       "      <td>0.125984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[xbos, xbol-1, [verse-1], xeol, xbol-2, how]</td>\n",
       "      <td>many</td>\n",
       "      <td>0.116903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[xbos, xbol-1, [verse-1], xeol, xbol-2, here]</td>\n",
       "      <td>comes</td>\n",
       "      <td>0.109665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[xbos, xbol-1, [verse-1], xeol, xbol-2, how]</td>\n",
       "      <td>long</td>\n",
       "      <td>0.095326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[xbos, xbol-1, [verse-1], xeol, xbol-2, he]</td>\n",
       "      <td>said</td>\n",
       "      <td>0.078437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[xbos, xbol-1, [verse-1], xeol, xbol-2, she]</td>\n",
       "      <td>said</td>\n",
       "      <td>0.075234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[xbos, xbol-1, [verse-1], xeol, xbol-2, my]</td>\n",
       "      <td>heart</td>\n",
       "      <td>0.071960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         context next_word  probability\n",
       "0   [xbos, xbol-1, [verse-1], xeol, xbol-2, one]       day     0.251755\n",
       "1    [xbos, xbol-1, [verse-1], xeol, xbol-2, no]       one     0.201328\n",
       "2  [xbos, xbol-1, [verse-1], xeol, xbol-2, they]       say     0.191211\n",
       "3    [xbos, xbol-1, [verse-1], xeol, xbol-2, so]      many     0.125984\n",
       "4   [xbos, xbol-1, [verse-1], xeol, xbol-2, how]      many     0.116903\n",
       "5  [xbos, xbol-1, [verse-1], xeol, xbol-2, here]     comes     0.109665\n",
       "6   [xbos, xbol-1, [verse-1], xeol, xbol-2, how]      long     0.095326\n",
       "7    [xbos, xbol-1, [verse-1], xeol, xbol-2, he]      said     0.078437\n",
       "8   [xbos, xbol-1, [verse-1], xeol, xbol-2, she]      said     0.075234\n",
       "9    [xbos, xbol-1, [verse-1], xeol, xbol-2, my]     heart     0.071960"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(next_word_probs, columns=['context', 'next_word', 'probability'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>next_word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>2.964834e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>2.697427e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>many</th>\n",
       "      <td>2.458187e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>say</th>\n",
       "      <td>2.332541e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>2.107267e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long</th>\n",
       "      <td>1.447440e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>got</th>\n",
       "      <td>1.430223e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comes</th>\n",
       "      <td>1.269305e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>1.262512e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>1.230618e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>could</th>\n",
       "      <td>1.203694e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baby</th>\n",
       "      <td>1.192115e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>1.142660e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>night</th>\n",
       "      <td>1.135521e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>man</th>\n",
       "      <td>1.044062e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>1.016951e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart</th>\n",
       "      <td>8.617054e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>’s</th>\n",
       "      <td>8.244108e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>7.583455e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>girl</th>\n",
       "      <td>7.080959e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matter</th>\n",
       "      <td>6.708994e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>need</th>\n",
       "      <td>6.647583e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>never</th>\n",
       "      <td>6.340024e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>6.186431e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>look</th>\n",
       "      <td>6.183985e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world</th>\n",
       "      <td>6.076625e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>took</th>\n",
       "      <td>6.055048e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>6.000242e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>5.891895e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>came</th>\n",
       "      <td>5.825580e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deutschland</th>\n",
       "      <td>8.156344e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uma</th>\n",
       "      <td>8.116661e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[post-chorus]</th>\n",
       "      <td>8.101195e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estas</th>\n",
       "      <td>8.077781e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eve</th>\n",
       "      <td>8.005202e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'?</th>\n",
       "      <td>8.002195e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nur</th>\n",
       "      <td>7.998235e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[pre-chorus]</th>\n",
       "      <td>7.940247e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sind</th>\n",
       "      <td>7.934244e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bom</th>\n",
       "      <td>7.918626e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abuse</th>\n",
       "      <td>7.896357e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pai</th>\n",
       "      <td>7.787997e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auch</th>\n",
       "      <td>7.727170e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>immer</th>\n",
       "      <td>7.711474e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pimp</th>\n",
       "      <td>7.709467e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ça</th>\n",
       "      <td>7.706586e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ahí</th>\n",
       "      <td>7.675823e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tão</th>\n",
       "      <td>7.606595e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'!</th>\n",
       "      <td>7.603991e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quoi</th>\n",
       "      <td>7.540845e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>belle</th>\n",
       "      <td>7.537696e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[verse-one]</th>\n",
       "      <td>7.527287e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doch</th>\n",
       "      <td>7.509262e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quanto</th>\n",
       "      <td>7.474481e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rely</th>\n",
       "      <td>7.444283e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nichts</th>\n",
       "      <td>7.417439e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nach</th>\n",
       "      <td>7.381386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alla</th>\n",
       "      <td>7.360010e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eso</th>\n",
       "      <td>7.278101e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nena</th>\n",
       "      <td>7.239101e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4111 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                probability\n",
       "next_word                  \n",
       "day            2.964834e-01\n",
       "one            2.697427e-01\n",
       "many           2.458187e-01\n",
       "say            2.332541e-01\n",
       "said           2.107267e-01\n",
       "long           1.447440e-01\n",
       "got            1.430223e-01\n",
       "comes          1.269305e-01\n",
       "know           1.262512e-01\n",
       "time           1.230618e-01\n",
       "could          1.203694e-01\n",
       "baby           1.192115e-01\n",
       "come           1.142660e-01\n",
       "night          1.135521e-01\n",
       "man            1.044062e-01\n",
       "love           1.016951e-01\n",
       "heart          8.617054e-02\n",
       "’s             8.244108e-02\n",
       "little         7.583455e-02\n",
       "girl           7.080959e-02\n",
       "matter         6.708994e-02\n",
       "need           6.647583e-02\n",
       "never          6.340024e-02\n",
       "way            6.186431e-02\n",
       "look           6.183985e-02\n",
       "world          6.076625e-02\n",
       "took           6.055048e-02\n",
       "last           6.000242e-02\n",
       "!              5.891895e-02\n",
       "came           5.825580e-02\n",
       "...                     ...\n",
       "deutschland    8.156344e-07\n",
       "uma            8.116661e-07\n",
       "[post-chorus]  8.101195e-07\n",
       "estas          8.077781e-07\n",
       "eve            8.005202e-07\n",
       "'?             8.002195e-07\n",
       "nur            7.998235e-07\n",
       "[pre-chorus]   7.940247e-07\n",
       "sind           7.934244e-07\n",
       "bom            7.918626e-07\n",
       "abuse          7.896357e-07\n",
       "pai            7.787997e-07\n",
       "auch           7.727170e-07\n",
       "immer          7.711474e-07\n",
       "pimp           7.709467e-07\n",
       "ça             7.706586e-07\n",
       "ahí            7.675823e-07\n",
       "tão            7.606595e-07\n",
       "'!             7.603991e-07\n",
       "quoi           7.540845e-07\n",
       "belle          7.537696e-07\n",
       "[verse-one]    7.527287e-07\n",
       "doch           7.509262e-07\n",
       "quanto         7.474481e-07\n",
       "rely           7.444283e-07\n",
       "nichts         7.417439e-07\n",
       "nach           7.381386e-07\n",
       "alla           7.360010e-07\n",
       "eso            7.278101e-07\n",
       "nena           7.239101e-07\n",
       "\n",
       "[4111 rows x 1 columns]"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('next_word').sum().sort_values(by='probability', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.log(df['probability']).hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:29<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "final_scores = generate_text(learn, GPU=GPU,\n",
    "                             seed_text='xbos xbol-1 [verse-1] xeol xbol-2',\n",
    "                             max_len=50, context_length=200,\n",
    "                             beam_width=3, verbose=False,\n",
    "                             temp=1.4, multinomial=True, graph=False, get_probs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xbos xbol-1 [verse-1] xeol \n",
      " xbol-2 everything , everything across time xeol \n",
      " xbol-3 everything , everything , everything comes to nothing xeol \n",
      " xbol-4 everything and everything xeol \n",
      " xbol-5 everything is everything xeol \n",
      " xbol-6 xeol \n",
      " xbol-7 [chorus] xeol \n",
      " xbol-8 everything is everything xeol \n",
      " xbol-9 everything is everything xeol \n",
      " xbol-10 everything is everything xeol \n",
      " xbol-11 everything is everything 76.94984640800992\n"
     ]
    }
   ],
   "source": [
    "#print all of the final options of songs\n",
    "song, score = final_scores[0]\n",
    "print_words(song)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal Model (3.4-ULMFiT-MM-108k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where things get a bit trickier...\n",
    "Modify generate function to work with multimodal modeling\n",
    "\n",
    "This is the main analysis. We want to see if changing the audio features changes the predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(f'../data/models/3.4-ULMFiT-MM-108k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = TextLMDataBunch.from_tokens(MODEL_PATH,\n",
    "                                      bs=128,\n",
    "                                      max_vocab=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = copy(data_lm.train_ds)\n",
    "valid_text = copy(data_lm.valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/interim/msd-aggregate/msd-aggregate-train.csv')\n",
    "df_valid = pd.read_csv('../data/interim/msd-aggregate/msd-aggregate-valid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiModalRNN(\n",
       "  (encoder): Embedding(10002, 400, padding_idx=1)\n",
       "  (encoder_dp): EmbeddingDropout(\n",
       "    (emb): Embedding(10002, 400, padding_idx=1)\n",
       "  )\n",
       "  (rnns): None\n",
       "  (input_dp): RNNDropout()\n",
       "  (hidden_dps): ModuleList(\n",
       "    (0): RNNDropout()\n",
       "    (1): RNNDropout()\n",
       "    (2): RNNDropout()\n",
       "  )\n",
       "  (multimode): ModuleList(\n",
       "    (0): WeightDropout(\n",
       "      (module): LSTM(434, 1150)\n",
       "    )\n",
       "    (1): WeightDropout(\n",
       "      (module): LSTM(1150, 1150)\n",
       "    )\n",
       "    (2): WeightDropout(\n",
       "      (module): LSTM(1150, 400)\n",
       "    )\n",
       "  )\n",
       "  (multidecoder): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=10002, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_sz = 34\n",
    "vocab_sz = 10002\n",
    "emb_sz = 400\n",
    "n_hid = 1150\n",
    "n_layers = 3\n",
    "pad_token = 1\n",
    "qrnn = False\n",
    "bidir = False\n",
    "drop_mult = 0.5\n",
    "dps = np.array([0.25, 0.1, 0.2, 0.02, 0.15]) * drop_mult\n",
    "hidden_p = dps[4]\n",
    "input_p = dps[0]\n",
    "embed_p = dps[3]\n",
    "weight_p = dps[2]\n",
    "tie_weights = True\n",
    "output_p = dps[1]\n",
    "bias = True\n",
    "\n",
    "class MultiModalRNN(RNNCore):\n",
    "    def __init__(self, audio_sz, output_p, bias, tie_encoder:bool=True, **kwargs):\n",
    "        super(MultiModalRNN, self).__init__(**kwargs)\n",
    "        self.rnns = None\n",
    "        self.audio_sz = audio_sz\n",
    "        self.multimode = [nn.LSTM(emb_sz + audio_sz if l == 0 else n_hid,\n",
    "                                  (n_hid if l != n_layers - 1 else emb_sz)//self.ndir,\n",
    "                                  1, bidirectional=bidir) for l in range(n_layers)]\n",
    "        self.multimode = [WeightDropout(rnn, weight_p) for rnn in self.multimode]\n",
    "        self.multimode = torch.nn.ModuleList(self.multimode)\n",
    "        \n",
    "        if tie_encoder:\n",
    "            enc = self.encoder\n",
    "        else:\n",
    "            enc = None\n",
    "        \n",
    "        self.multidecoder = LinearDecoder(vocab_sz,\n",
    "                                          emb_sz,\n",
    "                                          output_p,\n",
    "                                          tie_encoder=enc,\n",
    "                                          bias=bias)\n",
    "        \n",
    "    def forward(self, input:LongTensor, input_audio:Tensor)->Tuple[Tensor,Tensor,Tensor]:\n",
    "        sl,bs = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        raw_output = self.input_dp(self.encoder_dp(input))\n",
    "        raw_output = torch.cat([raw_output, input_audio], dim=2)\n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.multimode, self.hidden_dps)):\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output)\n",
    "        self.hidden = to_detach(new_hidden)\n",
    "        \n",
    "        output = self.multidecoder.output_dp(outputs[-1])\n",
    "        decoded = self.multidecoder.decoder(output.view(output.size(0)*output.size(1),\n",
    "                                                        output.size(2)))\n",
    "        \n",
    "        return decoded, raw_outputs, outputs\n",
    "    \n",
    "    def _one_hidden(self, l:int)->Tensor:\n",
    "        \"Return one hidden state.\"\n",
    "        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz)//self.ndir\n",
    "        return self.weights.new(self.ndir, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states.\"\n",
    "        [r.reset() for r in self.multimode if hasattr(r, 'reset')]\n",
    "        self.weights = next(self.parameters()).data\n",
    "        if self.qrnn: self.hidden = [self._one_hidden(l) for l in range(self.n_layers)]\n",
    "        else: self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]\n",
    "    \n",
    "multimodal_rnn = MultiModalRNN(audio_sz=audio_sz,\n",
    "                              vocab_sz=vocab_sz,\n",
    "                              emb_sz=emb_sz,\n",
    "                              n_hid=n_hid,\n",
    "                              n_layers=n_layers,\n",
    "                              pad_token=pad_token,\n",
    "                              qrnn=qrnn,\n",
    "                              bidir=bidir,\n",
    "                              hidden_p=hidden_p,\n",
    "                              input_p=input_p,\n",
    "                              embed_p=embed_p,\n",
    "                              weight_p=weight_p,\n",
    "                              output_p=output_p,\n",
    "                              bias=bias,\n",
    "                              tie_encoder=tie_weights)\n",
    "\n",
    "multimodal_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't actually need any of this data, but the current state isn't modular enough to not include it...\n",
    "\n",
    "One major update we need is to decouple the model with `learner`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "- scikit-learn version 0.20.0 is required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def log_features(X):\n",
    "    return np.log(X)\n",
    "log_feat = FunctionTransformer(log_features, validate=False)\n",
    "\n",
    "def bin_tempo(X):\n",
    "    '''\n",
    "    ref: https://en.wikipedia.org/wiki/Tempo#Italian_tempo_markings\n",
    "    These are rough loosely based on tempo markings above\n",
    "    Have considered both classical forms of music and popular\n",
    "    '''\n",
    "    assert X.shape[1] == 1, \"Only 1 column can be binned\"\n",
    "    bins = [0, 60, 76, 108, 120, 156, 176, 200, 500]\n",
    "    return pd.DataFrame(pd.cut(X.iloc[:,0], bins=bins))\n",
    "tempo_feat = FunctionTransformer(bin_tempo, validate=False)\n",
    "\n",
    "def bin_time_signature(X):\n",
    "    assert X.shape[1] == 1, \"Only 1 column can be binned\"\n",
    "    X['time_signature_bin'] = \"Other Signature\"\n",
    "    X.loc[X['time_signature'] == 4, 'time_signature_bin'] = '4/4 Signature'\n",
    "    X.loc[X['time_signature'] == 3, 'time_signature_bin'] = '3/4 Signature'\n",
    "    return X[['time_signature_bin']]\n",
    "time_feat = FunctionTransformer(bin_time_signature, validate=False)\n",
    "\n",
    "def to_string(X):\n",
    "    return X.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continous features\n",
    "numeric_features = ['artist_familiarity',\n",
    "                    'artist_hotttnesss',\n",
    "                    'loudness',\n",
    "                    'song_hotttnesss']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "# log features\n",
    "log_features = ['duration']\n",
    "log_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('log_feat', log_feat),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "# categorical features\n",
    "categorical_features = ['key', 'mode']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('stringify', FunctionTransformer(to_string, validate=False)),\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n",
    "\n",
    "# confidence features\n",
    "confidence_features = ['key_confidence',\n",
    "                       'mode_confidence',\n",
    "                       'time_signature_confidence'\n",
    "                      ]\n",
    "confidence_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=0))\n",
    "])\n",
    "\n",
    "# time signature feature\n",
    "time_feature = ['time_signature']\n",
    "time_transformer = Pipeline(steps=[\n",
    "    ('binner', time_feat),\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('stringify', FunctionTransformer(to_string, validate=False)),\n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n",
    "\n",
    "# tempo feature\n",
    "tempo_feature = ['tempo']\n",
    "tempo_transformer = Pipeline(steps=[\n",
    "    ('binner', tempo_feat),\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('stringify', FunctionTransformer(to_string, validate=False)),\n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('log', log_transformer, log_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('con', confidence_transformer, confidence_features),\n",
    "        ('time', time_transformer, time_feature),\n",
    "        ('tempo', tempo_transformer, tempo_feature)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syang/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('num', Pipeline(memory=None,\n",
       "     steps=[('imputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "       strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1)))]), ['artist_familiarity', 'artist_hotttnesss', 'loudness', 'song_hotttnes...=<class 'numpy.float64'>, handle_unknown='error',\n",
       "       n_values=None, sparse=True))]), ['tempo'])])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_tfm = preprocessor.transform(df_train)\n",
    "df_valid_tfm = preprocessor.transform(df_valid)\n",
    "\n",
    "train_audio = AudioDataset(df_train_tfm, train_text)\n",
    "valid_audio = AudioDataset(df_valid_tfm, valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_data = MultimodalDataLoader(audio_dataset=train_audio,\n",
    "                                  dataset=train_text)\n",
    "multi_data_valid = MultimodalDataLoader(audio_dataset=valid_audio,\n",
    "                                  dataset=valid_text)\n",
    "multi_db = DataBunch(multi_data, multi_data_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = RNNLearner(multi_db, multimodal_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('3.4-ULMFiT-MM-108k_best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Generate Text For Pure LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step(learner, context, audio, context_length, temp=1):\n",
    "\n",
    "    # FIX THIS\n",
    "    audio_size = train_audio.feature_size\n",
    "\n",
    "    model = learner.model\n",
    "    \n",
    "    if GPU:\n",
    "        context = LongTensor(context[-context_length:]).view(-1,1).cuda()\n",
    "    else:\n",
    "        context = LongTensor(context[-context_length:]).view(-1,1).cpu()\n",
    "    \n",
    "    context = torch.autograd.Variable(context)\n",
    "    \n",
    "    model.reset()\n",
    "    model.eval()\n",
    "    \n",
    "    if audio is None:\n",
    "        audio_features = Tensor([0]*audio_size*len(context))\\\n",
    "        .view(-1, 1, audio_size).cuda()\n",
    "    else:\n",
    "        audio_features = np.tile(audio, len(context))\n",
    "        audio_features = Tensor(audio_features).view(-1, 1, len(audio)).cuda()\n",
    "        \n",
    "    # forward pass the \"context\" into the model\n",
    "    result, *_ = model(context, audio_features)\n",
    "    result = result[-1]\n",
    "\n",
    "    # set unk and pad to 0 prob\n",
    "    # i.e. never pick unknown or pad\n",
    "    result[0] = -np.inf\n",
    "    result[1] = -np.inf\n",
    "\n",
    "    # softmax and normalize\n",
    "    probabilities = F.softmax(result/temp, dim=0)\n",
    "    probabilities = np.asarray(probabilities.detach().cpu(), dtype=np.float)\n",
    "    probabilities /= np.sum(probabilities) \n",
    "    return probabilities\n",
    "\n",
    "def get_word_from_index(idx):\n",
    "\n",
    "    return data_lm.valid_ds.vocab.textify([idx])\n",
    "\n",
    "\n",
    "def print_words(context):\n",
    "    for i in range(len(context)):\n",
    "        \n",
    "        step = context[i]\n",
    "\n",
    "        word = data_lm.valid_ds.vocab.textify([step])\n",
    "\n",
    "        if word == 'xeol':\n",
    "            word = 'xeol \\n'\n",
    "        elif 'xbol' in word:\n",
    "            word = word\n",
    "        elif word == 'xeos': \n",
    "            print(word)\n",
    "            break\n",
    "            \n",
    "        print(word, end=' ')   \n",
    "\n",
    "def generate_text(learner, seed_text=['xbos'], audio=None,\n",
    "                  max_len=500, GPU=False, context_length=20,\n",
    "                  beam_width=5, temp=1, multinomial=True,\n",
    "                  verbose=True, graph=False, get_probs=False):\n",
    "    \"\"\"Generates text with a given learner and returns best options.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    learner : RNNLearner Language Model (RNNLearner.language_model())\n",
    "        Fastai RNNLearner with tokenized language model data already loaded \n",
    "        \n",
    "    seed_text : list or str\n",
    "        List of strings where each item is a token. (e.g. ['the', 'cat']) or string that is split on white space\n",
    "\n",
    "    max_len : int\n",
    "        Number of words in generated sequence\n",
    "        \n",
    "    gpu : bool\n",
    "        If you're using a GPU or not...\n",
    "    \n",
    "    context_length : int\n",
    "        Amount of words that get input as \"context\" into the model. Set to 0 for no limit   \n",
    "        \n",
    "    beam_width : int\n",
    "        How many new word indices to try out...computationally expensive\n",
    "    \n",
    "    verbose : bool\n",
    "        If True, prints every possible context for a given word cycle\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    context_and_scores : list of lists\n",
    "        Returns a sorted list of the entire tree search of contexts and their respective scores in the form:\n",
    "        [[context, score], [context, score], ..., [context, score]]\n",
    "    \"\"\"\n",
    "        \n",
    "    if isinstance(seed_text, str):\n",
    "        seed_text = data_lm.train_ds.vocab.numericalize(seed_text.split(' '))\n",
    "    \n",
    "    \n",
    "    # Width for the beam search, to be externalized along with general decoding\n",
    "    beam_width = beam_width\n",
    "    \n",
    "    if graph:\n",
    "        optimization_graph = Digraph()\n",
    "\n",
    "    # List of candidate word sequence. We'll maintain #beam_width top sequences here.\n",
    "    # The context is a list of words, the scores are the sum of the log probabilities of each word\n",
    "    context_and_scores = [[seed_text, 0.0]]\n",
    "    \n",
    "    # Loop over max number of words\n",
    "    for word_number in tqdm(range(max_len)):\n",
    "#         print(f'Generating word: {word_number+1} / {max_len}')\n",
    "\n",
    "        candidates = []\n",
    "        next_word_probs = []\n",
    "        \n",
    "        # For each possible context that we've generated so far, generate new probabilities, \n",
    "        # and pick an additional #beam_width next candidates\n",
    "        for i in range(len(context_and_scores)):\n",
    "            # Get a new sequence of word indices and log-probability\n",
    "            # Example: [[2, 138, 661], 23.181717]\n",
    "            context, score = context_and_scores[i]\n",
    "            \n",
    "            # Obtain probabilities for next word given the context \n",
    "            probabilities = generate_step(learner, context, audio, context_length, temp)\n",
    "\n",
    "            # Multinomial draw from the probabilities\n",
    "            if multinomial:\n",
    "                multinom_draw = np.random.multinomial(beam_width, probabilities)\n",
    "                top_probabilities = np.argwhere(multinom_draw != 0).flatten()                    \n",
    "                \n",
    "            # top-k from probabilities    \n",
    "            else:\n",
    "                top_probabilities = np.argsort(-probabilities)[:beam_width]\n",
    "                        \n",
    "            #For each possible new candidate, update the context and scores\n",
    "            for j in range(len(top_probabilities)):\n",
    "                next_word_idx = top_probabilities[j]\n",
    "                new_context = context + [next_word_idx]\n",
    "                candidate = [new_context, (score - np.log(probabilities[next_word_idx]))]\n",
    "                candidates.append(candidate)\n",
    "                \n",
    "                if get_probs:\n",
    "                    next_word_prob = probabilities[next_word_idx]\n",
    "                    potential_next_word = get_word_from_index(next_word_idx)\n",
    "                    prior_context = [get_word_from_index(w) for w in context]\n",
    "                    next_word_probs.append((prior_context, potential_next_word, next_word_prob))\n",
    "                \n",
    "                if graph:\n",
    "                    optimization_graph.node(\"%d_%d\" % (word_number, next_word_idx), \"%s (%.2f)\" % (get_word_from_index(next_word_idx), candidate[1]))\n",
    "                    optimization_graph.edge(\"%d_%d\" % (word_number - 1, context[len(context) -1]), \"%d_%d\" % (word_number, next_word_idx))\n",
    "                \n",
    "        #update the running tally of context and scores and sort by probability of each entry\n",
    "        context_and_scores = candidates\n",
    "        context_and_scores = sorted(context_and_scores, key = lambda x: x[1]) #sort by top entries\n",
    "\n",
    "        context_and_scores = context_and_scores[:30] #for now, only keep the top 30 to speed things up but we can/should change this to beam_width or something else\n",
    "        \n",
    "        if verbose:\n",
    "            for context, score in context_and_scores:\n",
    "                print_words(context)\n",
    "                print('\\n')\n",
    "\n",
    "    if graph:\n",
    "        now = str(datetime.datetime.now())\n",
    "        optimization_graph.render(directory='graph_viz/', filename=now, cleanup=True)\n",
    "        \n",
    "    if get_probs:\n",
    "        next_word_probs = sorted(next_word_probs, key=lambda x: -x[2])\n",
    "        return next_word_probs\n",
    "        \n",
    "    return context_and_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features from validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode == 1, loudness == -2.3,\n",
    "# artist_hot == 0.57, artist_familiarity = 0.77, \n",
    "# song_hot == n/a\n",
    "xx = valid_audio[9126]\n",
    "\n",
    "# mode == 0, loudness == -15.5,\n",
    "# artist_hot == 0.36, artist_familiarity = 0.51, \n",
    "# song_hot == 0\n",
    "yy = valid_audio[7650]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Artificial\" Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create artificial audio features\n",
    "# 0.9 for all continuous features\n",
    "# key = 0 (of 12)\n",
    "# mode = 1 (of 2)\n",
    "# time = 1 (of 3)\n",
    "# tempo = 7 (of 9)\n",
    "zz1 = np.array([0.9, 0.9, 0.9, 0.9, 0.9, \n",
    "                0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "                0., 1.,\n",
    "                0.9, 0.9, 0.9,\n",
    "                0., 1., 0.,\n",
    "                0., 0., 1., 0., 0., 0., 0., 0., 0.\n",
    "               ])\n",
    "# 0.2 for all continuous features\n",
    "# key = 0 (of 12)\n",
    "# mode = 0 (of 2)\n",
    "# time = 1 (of 3)\n",
    "# tempo = 2 (of 9)\n",
    "zz2 = np.array([0.2, 0.2, 0.2, 0.2, 0.2, \n",
    "                1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "                0., 1.,\n",
    "                0.9, 0.9, 0.9,\n",
    "                0., 1., 0.,\n",
    "                0., 0., 1., 0., 0., 0., 0., 0., 0.\n",
    "               ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature set #1\n",
    "These are features that appear to be more \"Pop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:12<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "next_word_probs = generate_text(learn, GPU=GPU,\n",
    "                             seed_text='xbos xbol-1 [verse-1] xeol xbol-2',\n",
    "                             audio=zz1,\n",
    "                             max_len=10, context_length=200,\n",
    "                             beam_width=1000, verbose=False,\n",
    "                             temp=1, multinomial=False, graph=False, get_probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word_probs = [s for s in next_word_probs if s[1] not in stop_words]\n",
    "next_word_probs = [s for s in next_word_probs if 'xbol' not in s[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(next_word_probs, columns=['context', 'next_word', 'probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>next_word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ass</th>\n",
       "      <td>3.430205e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stress</th>\n",
       "      <td>1.525948e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knockin</th>\n",
       "      <td>1.344053e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shone</th>\n",
       "      <td>1.995385e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ending</th>\n",
       "      <td>8.662105e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clothes</th>\n",
       "      <td>1.212948e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clean</th>\n",
       "      <td>1.505122e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hip</th>\n",
       "      <td>8.649441e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>others</th>\n",
       "      <td>2.680671e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>que</th>\n",
       "      <td>1.062997e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            probability\n",
       "next_word              \n",
       "ass        3.430205e-05\n",
       "stress     1.525948e-06\n",
       "knockin    1.344053e-09\n",
       "shone      1.995385e-05\n",
       "ending     8.662105e-05\n",
       "clothes    1.212948e-06\n",
       "clean      1.505122e-04\n",
       "hip        8.649441e-05\n",
       "others     2.680671e-04\n",
       "que        1.062997e-04"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz1_words = df.groupby('next_word').sum().sort_values(by='probability', ascending=False)\n",
    "zz1_words.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.log(df['probability']).hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_scores = generate_text(learn, GPU=GPU,\n",
    "#                              seed_text='xbos xbol-1 [verse-1] xeol xbol-2',\n",
    "#                              audio=zz1,\n",
    "#                              max_len=50, context_length=200,\n",
    "#                              beam_width=3, verbose=False,\n",
    "#                              temp=1.4, multinomial=True, graph=False, get_probs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #print all of the final options of songs\n",
    "# song, score = final_scores[0]\n",
    "# print_words(song)\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Features set #2\n",
    "These are features that are less \"Pop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:12<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "next_word_probs = generate_text(learn, GPU=GPU,\n",
    "                             seed_text='xbos xbol-1 [verse-1] xeol xbol-2',\n",
    "                             audio=zz2,\n",
    "                             max_len=10, context_length=200,\n",
    "                             beam_width=1000, verbose=False,\n",
    "                             temp=1, multinomial=False, graph=False, get_probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word_probs = [s for s in next_word_probs if s[1] not in stop_words]\n",
    "next_word_probs = [s for s in next_word_probs if 'xbol' not in s[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(next_word_probs, columns=['context', 'next_word', 'probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>next_word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>east</th>\n",
       "      <td>0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beneath</th>\n",
       "      <td>0.000462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bush</th>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dreamed</th>\n",
       "      <td>0.001942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>longest</th>\n",
       "      <td>0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(never</th>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wastin</th>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dancin</th>\n",
       "      <td>0.000505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wondered</th>\n",
       "      <td>0.000933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>path</th>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           probability\n",
       "next_word             \n",
       "east          0.000095\n",
       "beneath       0.000462\n",
       "bush          0.000003\n",
       "dreamed       0.001942\n",
       "longest       0.000028\n",
       "(never        0.000016\n",
       "wastin        0.000002\n",
       "dancin        0.000505\n",
       "wondered      0.000933\n",
       "path          0.000005"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz2_words = df.groupby('next_word').sum().sort_values(by='probability', ascending=False)\n",
    "zz2_words.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability_x</th>\n",
       "      <th>probability_y</th>\n",
       "      <th>diff</th>\n",
       "      <th>diff_abs</th>\n",
       "      <th>rel_diff_abs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>next_word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.611565e-05</td>\n",
       "      <td>8.816805e-09</td>\n",
       "      <td>4.610683e-05</td>\n",
       "      <td>4.610683e-05</td>\n",
       "      <td>0.999809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pa</th>\n",
       "      <td>2.502828e-05</td>\n",
       "      <td>1.028903e-08</td>\n",
       "      <td>2.501799e-05</td>\n",
       "      <td>2.501799e-05</td>\n",
       "      <td>0.999589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neva</th>\n",
       "      <td>2.635164e-06</td>\n",
       "      <td>1.201998e-09</td>\n",
       "      <td>2.633962e-06</td>\n",
       "      <td>2.633962e-06</td>\n",
       "      <td>0.999544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fever</th>\n",
       "      <td>4.107700e-06</td>\n",
       "      <td>1.402422e-08</td>\n",
       "      <td>4.093676e-06</td>\n",
       "      <td>4.093676e-06</td>\n",
       "      <td>0.996586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hum</th>\n",
       "      <td>2.778501e-06</td>\n",
       "      <td>1.405765e-07</td>\n",
       "      <td>2.637924e-06</td>\n",
       "      <td>2.637924e-06</td>\n",
       "      <td>0.949406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>possibly</th>\n",
       "      <td>2.350971e-06</td>\n",
       "      <td>1.436968e-07</td>\n",
       "      <td>2.207274e-06</td>\n",
       "      <td>2.207274e-06</td>\n",
       "      <td>0.938878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seeking</th>\n",
       "      <td>2.685019e-05</td>\n",
       "      <td>1.907542e-06</td>\n",
       "      <td>2.494264e-05</td>\n",
       "      <td>2.494264e-05</td>\n",
       "      <td>0.928956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trippin</th>\n",
       "      <td>1.186395e-05</td>\n",
       "      <td>8.437511e-07</td>\n",
       "      <td>1.102020e-05</td>\n",
       "      <td>1.102020e-05</td>\n",
       "      <td>0.928881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relate</th>\n",
       "      <td>2.965873e-06</td>\n",
       "      <td>2.295046e-07</td>\n",
       "      <td>2.736369e-06</td>\n",
       "      <td>2.736369e-06</td>\n",
       "      <td>0.922618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reasons</th>\n",
       "      <td>2.711279e-05</td>\n",
       "      <td>2.245955e-06</td>\n",
       "      <td>2.486683e-05</td>\n",
       "      <td>2.486683e-05</td>\n",
       "      <td>0.917163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forgiveness</th>\n",
       "      <td>1.885891e-05</td>\n",
       "      <td>1.582744e-06</td>\n",
       "      <td>1.727616e-05</td>\n",
       "      <td>1.727616e-05</td>\n",
       "      <td>0.916074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anticipating</th>\n",
       "      <td>3.829087e-04</td>\n",
       "      <td>3.504881e-05</td>\n",
       "      <td>3.478599e-04</td>\n",
       "      <td>3.478599e-04</td>\n",
       "      <td>0.908467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strung</th>\n",
       "      <td>4.119819e-05</td>\n",
       "      <td>4.097254e-06</td>\n",
       "      <td>3.710094e-05</td>\n",
       "      <td>3.710094e-05</td>\n",
       "      <td>0.900548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(i've</th>\n",
       "      <td>2.422496e-05</td>\n",
       "      <td>2.413419e-06</td>\n",
       "      <td>2.181154e-05</td>\n",
       "      <td>2.181154e-05</td>\n",
       "      <td>0.900375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peep</th>\n",
       "      <td>4.539750e-06</td>\n",
       "      <td>4.648433e-07</td>\n",
       "      <td>4.074907e-06</td>\n",
       "      <td>4.074907e-06</td>\n",
       "      <td>0.897606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answers</th>\n",
       "      <td>3.370017e-05</td>\n",
       "      <td>3.987906e-06</td>\n",
       "      <td>2.971227e-05</td>\n",
       "      <td>2.971227e-05</td>\n",
       "      <td>0.881665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ali</th>\n",
       "      <td>1.109680e-08</td>\n",
       "      <td>1.334068e-09</td>\n",
       "      <td>9.762731e-09</td>\n",
       "      <td>9.762731e-09</td>\n",
       "      <td>0.879779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>create</th>\n",
       "      <td>6.844378e-06</td>\n",
       "      <td>8.500150e-07</td>\n",
       "      <td>5.994362e-06</td>\n",
       "      <td>5.994362e-06</td>\n",
       "      <td>0.875808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>como</th>\n",
       "      <td>2.979988e-05</td>\n",
       "      <td>4.008114e-06</td>\n",
       "      <td>2.579177e-05</td>\n",
       "      <td>2.579177e-05</td>\n",
       "      <td>0.865499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freak</th>\n",
       "      <td>5.967382e-06</td>\n",
       "      <td>9.200105e-07</td>\n",
       "      <td>5.047371e-06</td>\n",
       "      <td>5.047371e-06</td>\n",
       "      <td>0.845827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>behave</th>\n",
       "      <td>4.562812e-06</td>\n",
       "      <td>8.616735e-07</td>\n",
       "      <td>3.701138e-06</td>\n",
       "      <td>3.701138e-06</td>\n",
       "      <td>0.811153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wall</th>\n",
       "      <td>1.803916e-05</td>\n",
       "      <td>3.656072e-06</td>\n",
       "      <td>1.438309e-05</td>\n",
       "      <td>1.438309e-05</td>\n",
       "      <td>0.797326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>searchin</th>\n",
       "      <td>7.153509e-05</td>\n",
       "      <td>1.463618e-05</td>\n",
       "      <td>5.689891e-05</td>\n",
       "      <td>5.689891e-05</td>\n",
       "      <td>0.795399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(when</th>\n",
       "      <td>1.638936e-05</td>\n",
       "      <td>3.455718e-06</td>\n",
       "      <td>1.293365e-05</td>\n",
       "      <td>1.293365e-05</td>\n",
       "      <td>0.789149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>silently</th>\n",
       "      <td>2.166360e-05</td>\n",
       "      <td>5.317962e-06</td>\n",
       "      <td>1.634564e-05</td>\n",
       "      <td>1.634564e-05</td>\n",
       "      <td>0.754521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patiently</th>\n",
       "      <td>5.239886e-04</td>\n",
       "      <td>1.288566e-04</td>\n",
       "      <td>3.951320e-04</td>\n",
       "      <td>3.951320e-04</td>\n",
       "      <td>0.754085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comprehend</th>\n",
       "      <td>3.080777e-06</td>\n",
       "      <td>7.668518e-07</td>\n",
       "      <td>2.313925e-06</td>\n",
       "      <td>2.313925e-06</td>\n",
       "      <td>0.751085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rely</th>\n",
       "      <td>4.200143e-06</td>\n",
       "      <td>1.065728e-06</td>\n",
       "      <td>3.134414e-06</td>\n",
       "      <td>3.134414e-06</td>\n",
       "      <td>0.746264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waiting</th>\n",
       "      <td>5.118296e-02</td>\n",
       "      <td>1.318666e-02</td>\n",
       "      <td>3.799630e-02</td>\n",
       "      <td>3.799630e-02</td>\n",
       "      <td>0.742362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screw</th>\n",
       "      <td>3.260562e-06</td>\n",
       "      <td>8.426116e-07</td>\n",
       "      <td>2.417951e-06</td>\n",
       "      <td>2.417951e-06</td>\n",
       "      <td>0.741575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>everytime</th>\n",
       "      <td>1.272276e-03</td>\n",
       "      <td>1.268058e-03</td>\n",
       "      <td>4.218360e-06</td>\n",
       "      <td>4.218360e-06</td>\n",
       "      <td>0.003316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop</th>\n",
       "      <td>1.943256e-03</td>\n",
       "      <td>1.937489e-03</td>\n",
       "      <td>5.767729e-06</td>\n",
       "      <td>5.767729e-06</td>\n",
       "      <td>0.002968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uptown</th>\n",
       "      <td>1.678567e-05</td>\n",
       "      <td>1.673645e-05</td>\n",
       "      <td>4.921597e-08</td>\n",
       "      <td>4.921597e-08</td>\n",
       "      <td>0.002932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>counted</th>\n",
       "      <td>1.062028e-04</td>\n",
       "      <td>1.059154e-04</td>\n",
       "      <td>2.874737e-07</td>\n",
       "      <td>2.874737e-07</td>\n",
       "      <td>0.002707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sign</th>\n",
       "      <td>3.503664e-05</td>\n",
       "      <td>3.494623e-05</td>\n",
       "      <td>9.041414e-08</td>\n",
       "      <td>9.041414e-08</td>\n",
       "      <td>0.002581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upside</th>\n",
       "      <td>8.006445e-05</td>\n",
       "      <td>7.985848e-05</td>\n",
       "      <td>2.059780e-07</td>\n",
       "      <td>2.059780e-07</td>\n",
       "      <td>0.002573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>makes</th>\n",
       "      <td>4.027526e-03</td>\n",
       "      <td>4.017479e-03</td>\n",
       "      <td>1.004663e-05</td>\n",
       "      <td>1.004663e-05</td>\n",
       "      <td>0.002494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>steals</th>\n",
       "      <td>5.656996e-06</td>\n",
       "      <td>5.643048e-06</td>\n",
       "      <td>1.394757e-08</td>\n",
       "      <td>1.394757e-08</td>\n",
       "      <td>0.002466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>storm</th>\n",
       "      <td>1.121475e-05</td>\n",
       "      <td>1.118919e-05</td>\n",
       "      <td>2.555502e-08</td>\n",
       "      <td>2.555502e-08</td>\n",
       "      <td>0.002279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meanwhile</th>\n",
       "      <td>3.135649e-05</td>\n",
       "      <td>3.129586e-05</td>\n",
       "      <td>6.063209e-08</td>\n",
       "      <td>6.063209e-08</td>\n",
       "      <td>0.001934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>passion</th>\n",
       "      <td>8.293122e-05</td>\n",
       "      <td>8.277763e-05</td>\n",
       "      <td>1.535850e-07</td>\n",
       "      <td>1.535850e-07</td>\n",
       "      <td>0.001852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mirror</th>\n",
       "      <td>3.816924e-05</td>\n",
       "      <td>3.809947e-05</td>\n",
       "      <td>6.976381e-08</td>\n",
       "      <td>6.976381e-08</td>\n",
       "      <td>0.001828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bold</th>\n",
       "      <td>7.897722e-06</td>\n",
       "      <td>7.884300e-06</td>\n",
       "      <td>1.342150e-08</td>\n",
       "      <td>1.342150e-08</td>\n",
       "      <td>0.001699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actions</th>\n",
       "      <td>1.118861e-05</td>\n",
       "      <td>1.116994e-05</td>\n",
       "      <td>1.866567e-08</td>\n",
       "      <td>1.866567e-08</td>\n",
       "      <td>0.001668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>achieve</th>\n",
       "      <td>1.589452e-07</td>\n",
       "      <td>1.586949e-07</td>\n",
       "      <td>2.503324e-10</td>\n",
       "      <td>2.503324e-10</td>\n",
       "      <td>0.001575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hungry</th>\n",
       "      <td>2.758939e-04</td>\n",
       "      <td>2.754758e-04</td>\n",
       "      <td>4.180934e-07</td>\n",
       "      <td>4.180934e-07</td>\n",
       "      <td>0.001515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer</th>\n",
       "      <td>6.963384e-05</td>\n",
       "      <td>6.952928e-05</td>\n",
       "      <td>1.045638e-07</td>\n",
       "      <td>1.045638e-07</td>\n",
       "      <td>0.001502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parked</th>\n",
       "      <td>2.088414e-05</td>\n",
       "      <td>2.085359e-05</td>\n",
       "      <td>3.055358e-08</td>\n",
       "      <td>3.055358e-08</td>\n",
       "      <td>0.001463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eighteen</th>\n",
       "      <td>3.509437e-05</td>\n",
       "      <td>3.504429e-05</td>\n",
       "      <td>5.007838e-08</td>\n",
       "      <td>5.007838e-08</td>\n",
       "      <td>0.001427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dealing</th>\n",
       "      <td>1.254450e-05</td>\n",
       "      <td>1.252817e-05</td>\n",
       "      <td>1.633493e-08</td>\n",
       "      <td>1.633493e-08</td>\n",
       "      <td>0.001302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rang</th>\n",
       "      <td>3.940190e-05</td>\n",
       "      <td>3.935059e-05</td>\n",
       "      <td>5.130743e-08</td>\n",
       "      <td>5.130743e-08</td>\n",
       "      <td>0.001302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sunk</th>\n",
       "      <td>7.515278e-06</td>\n",
       "      <td>7.506157e-06</td>\n",
       "      <td>9.121329e-09</td>\n",
       "      <td>9.121329e-09</td>\n",
       "      <td>0.001214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>early</th>\n",
       "      <td>6.048888e-04</td>\n",
       "      <td>6.041761e-04</td>\n",
       "      <td>7.127269e-07</td>\n",
       "      <td>7.127269e-07</td>\n",
       "      <td>0.001178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blame</th>\n",
       "      <td>6.257160e-04</td>\n",
       "      <td>6.252133e-04</td>\n",
       "      <td>5.026695e-07</td>\n",
       "      <td>5.026695e-07</td>\n",
       "      <td>0.000803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alone</th>\n",
       "      <td>1.503016e-02</td>\n",
       "      <td>1.502050e-02</td>\n",
       "      <td>9.655561e-06</td>\n",
       "      <td>9.655561e-06</td>\n",
       "      <td>0.000642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reach</th>\n",
       "      <td>5.073383e-04</td>\n",
       "      <td>5.071024e-04</td>\n",
       "      <td>2.358796e-07</td>\n",
       "      <td>2.358796e-07</td>\n",
       "      <td>0.000465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paper</th>\n",
       "      <td>2.571598e-05</td>\n",
       "      <td>2.570507e-05</td>\n",
       "      <td>1.090178e-08</td>\n",
       "      <td>1.090178e-08</td>\n",
       "      <td>0.000424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[verse-3]</th>\n",
       "      <td>6.475325e-06</td>\n",
       "      <td>6.473213e-06</td>\n",
       "      <td>2.112182e-09</td>\n",
       "      <td>2.112182e-09</td>\n",
       "      <td>0.000326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nothing</th>\n",
       "      <td>2.117050e-02</td>\n",
       "      <td>2.116364e-02</td>\n",
       "      <td>6.858779e-06</td>\n",
       "      <td>6.858779e-06</td>\n",
       "      <td>0.000324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created</th>\n",
       "      <td>8.409472e-05</td>\n",
       "      <td>8.407198e-05</td>\n",
       "      <td>2.273592e-08</td>\n",
       "      <td>2.273592e-08</td>\n",
       "      <td>0.000270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1188 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              probability_x  probability_y          diff      diff_abs  \\\n",
       "next_word                                                                \n",
       "24             4.611565e-05   8.816805e-09  4.610683e-05  4.610683e-05   \n",
       "pa             2.502828e-05   1.028903e-08  2.501799e-05  2.501799e-05   \n",
       "neva           2.635164e-06   1.201998e-09  2.633962e-06  2.633962e-06   \n",
       "fever          4.107700e-06   1.402422e-08  4.093676e-06  4.093676e-06   \n",
       "hum            2.778501e-06   1.405765e-07  2.637924e-06  2.637924e-06   \n",
       "possibly       2.350971e-06   1.436968e-07  2.207274e-06  2.207274e-06   \n",
       "seeking        2.685019e-05   1.907542e-06  2.494264e-05  2.494264e-05   \n",
       "trippin        1.186395e-05   8.437511e-07  1.102020e-05  1.102020e-05   \n",
       "relate         2.965873e-06   2.295046e-07  2.736369e-06  2.736369e-06   \n",
       "reasons        2.711279e-05   2.245955e-06  2.486683e-05  2.486683e-05   \n",
       "forgiveness    1.885891e-05   1.582744e-06  1.727616e-05  1.727616e-05   \n",
       "anticipating   3.829087e-04   3.504881e-05  3.478599e-04  3.478599e-04   \n",
       "strung         4.119819e-05   4.097254e-06  3.710094e-05  3.710094e-05   \n",
       "(i've          2.422496e-05   2.413419e-06  2.181154e-05  2.181154e-05   \n",
       "peep           4.539750e-06   4.648433e-07  4.074907e-06  4.074907e-06   \n",
       "answers        3.370017e-05   3.987906e-06  2.971227e-05  2.971227e-05   \n",
       "ali            1.109680e-08   1.334068e-09  9.762731e-09  9.762731e-09   \n",
       "create         6.844378e-06   8.500150e-07  5.994362e-06  5.994362e-06   \n",
       "como           2.979988e-05   4.008114e-06  2.579177e-05  2.579177e-05   \n",
       "freak          5.967382e-06   9.200105e-07  5.047371e-06  5.047371e-06   \n",
       "behave         4.562812e-06   8.616735e-07  3.701138e-06  3.701138e-06   \n",
       "wall           1.803916e-05   3.656072e-06  1.438309e-05  1.438309e-05   \n",
       "searchin       7.153509e-05   1.463618e-05  5.689891e-05  5.689891e-05   \n",
       "(when          1.638936e-05   3.455718e-06  1.293365e-05  1.293365e-05   \n",
       "silently       2.166360e-05   5.317962e-06  1.634564e-05  1.634564e-05   \n",
       "patiently      5.239886e-04   1.288566e-04  3.951320e-04  3.951320e-04   \n",
       "comprehend     3.080777e-06   7.668518e-07  2.313925e-06  2.313925e-06   \n",
       "rely           4.200143e-06   1.065728e-06  3.134414e-06  3.134414e-06   \n",
       "waiting        5.118296e-02   1.318666e-02  3.799630e-02  3.799630e-02   \n",
       "screw          3.260562e-06   8.426116e-07  2.417951e-06  2.417951e-06   \n",
       "...                     ...            ...           ...           ...   \n",
       "everytime      1.272276e-03   1.268058e-03  4.218360e-06  4.218360e-06   \n",
       "stop           1.943256e-03   1.937489e-03  5.767729e-06  5.767729e-06   \n",
       "uptown         1.678567e-05   1.673645e-05  4.921597e-08  4.921597e-08   \n",
       "counted        1.062028e-04   1.059154e-04  2.874737e-07  2.874737e-07   \n",
       "sign           3.503664e-05   3.494623e-05  9.041414e-08  9.041414e-08   \n",
       "upside         8.006445e-05   7.985848e-05  2.059780e-07  2.059780e-07   \n",
       "makes          4.027526e-03   4.017479e-03  1.004663e-05  1.004663e-05   \n",
       "steals         5.656996e-06   5.643048e-06  1.394757e-08  1.394757e-08   \n",
       "storm          1.121475e-05   1.118919e-05  2.555502e-08  2.555502e-08   \n",
       "meanwhile      3.135649e-05   3.129586e-05  6.063209e-08  6.063209e-08   \n",
       "passion        8.293122e-05   8.277763e-05  1.535850e-07  1.535850e-07   \n",
       "mirror         3.816924e-05   3.809947e-05  6.976381e-08  6.976381e-08   \n",
       "bold           7.897722e-06   7.884300e-06  1.342150e-08  1.342150e-08   \n",
       "actions        1.118861e-05   1.116994e-05  1.866567e-08  1.866567e-08   \n",
       "achieve        1.589452e-07   1.586949e-07  2.503324e-10  2.503324e-10   \n",
       "hungry         2.758939e-04   2.754758e-04  4.180934e-07  4.180934e-07   \n",
       "offer          6.963384e-05   6.952928e-05  1.045638e-07  1.045638e-07   \n",
       "parked         2.088414e-05   2.085359e-05  3.055358e-08  3.055358e-08   \n",
       "eighteen       3.509437e-05   3.504429e-05  5.007838e-08  5.007838e-08   \n",
       "dealing        1.254450e-05   1.252817e-05  1.633493e-08  1.633493e-08   \n",
       "rang           3.940190e-05   3.935059e-05  5.130743e-08  5.130743e-08   \n",
       "sunk           7.515278e-06   7.506157e-06  9.121329e-09  9.121329e-09   \n",
       "early          6.048888e-04   6.041761e-04  7.127269e-07  7.127269e-07   \n",
       "blame          6.257160e-04   6.252133e-04  5.026695e-07  5.026695e-07   \n",
       "alone          1.503016e-02   1.502050e-02  9.655561e-06  9.655561e-06   \n",
       "reach          5.073383e-04   5.071024e-04  2.358796e-07  2.358796e-07   \n",
       "paper          2.571598e-05   2.570507e-05  1.090178e-08  1.090178e-08   \n",
       "[verse-3]      6.475325e-06   6.473213e-06  2.112182e-09  2.112182e-09   \n",
       "nothing        2.117050e-02   2.116364e-02  6.858779e-06  6.858779e-06   \n",
       "created        8.409472e-05   8.407198e-05  2.273592e-08  2.273592e-08   \n",
       "\n",
       "              rel_diff_abs  \n",
       "next_word                   \n",
       "24                0.999809  \n",
       "pa                0.999589  \n",
       "neva              0.999544  \n",
       "fever             0.996586  \n",
       "hum               0.949406  \n",
       "possibly          0.938878  \n",
       "seeking           0.928956  \n",
       "trippin           0.928881  \n",
       "relate            0.922618  \n",
       "reasons           0.917163  \n",
       "forgiveness       0.916074  \n",
       "anticipating      0.908467  \n",
       "strung            0.900548  \n",
       "(i've             0.900375  \n",
       "peep              0.897606  \n",
       "answers           0.881665  \n",
       "ali               0.879779  \n",
       "create            0.875808  \n",
       "como              0.865499  \n",
       "freak             0.845827  \n",
       "behave            0.811153  \n",
       "wall              0.797326  \n",
       "searchin          0.795399  \n",
       "(when             0.789149  \n",
       "silently          0.754521  \n",
       "patiently         0.754085  \n",
       "comprehend        0.751085  \n",
       "rely              0.746264  \n",
       "waiting           0.742362  \n",
       "screw             0.741575  \n",
       "...                    ...  \n",
       "everytime         0.003316  \n",
       "stop              0.002968  \n",
       "uptown            0.002932  \n",
       "counted           0.002707  \n",
       "sign              0.002581  \n",
       "upside            0.002573  \n",
       "makes             0.002494  \n",
       "steals            0.002466  \n",
       "storm             0.002279  \n",
       "meanwhile         0.001934  \n",
       "passion           0.001852  \n",
       "mirror            0.001828  \n",
       "bold              0.001699  \n",
       "actions           0.001668  \n",
       "achieve           0.001575  \n",
       "hungry            0.001515  \n",
       "offer             0.001502  \n",
       "parked            0.001463  \n",
       "eighteen          0.001427  \n",
       "dealing           0.001302  \n",
       "rang              0.001302  \n",
       "sunk              0.001214  \n",
       "early             0.001178  \n",
       "blame             0.000803  \n",
       "alone             0.000642  \n",
       "reach             0.000465  \n",
       "paper             0.000424  \n",
       "[verse-3]         0.000326  \n",
       "nothing           0.000324  \n",
       "created           0.000270  \n",
       "\n",
       "[1188 rows x 5 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined = pd.merge(zz1_words, zz2_words, how='outer', left_index=True, right_index=True)\n",
    "# df_combined['rel_ratio_abs'] = (df_combined['probability_x'] / df_combined['probability_y'])\n",
    "df_combined['diff'] = df_combined['probability_x'] - df_combined['probability_y']\n",
    "df_combined['diff_abs'] = np.abs(df_combined['diff'])\n",
    "df_combined['rel_diff_abs'] = df_combined['diff_abs'] / df_combined['probability_x']\n",
    "df_combined.sort_values(by='rel_diff_abs', ascending=False)\\\n",
    "    .loc[df_combined['diff'] > 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.log(df['probability']).hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:29<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "final_scores = generate_text(learn, GPU=GPU,\n",
    "                             seed_text='xbos xbol-1 [verse-1] xeol xbol-2',\n",
    "                             audio=zz2,\n",
    "                             max_len=50, context_length=200,\n",
    "                             beam_width=3, verbose=False,\n",
    "                             temp=1.4, multinomial=True, graph=False, get_probs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xbos xbol-1 [verse-1] xeol \n",
      " xbol-2 here she comes now xeol \n",
      " xbol-3 here she comes now xeol \n",
      " xbol-4 she cries no more xeol \n",
      " xbol-5 here she comes now xeol \n",
      " xbol-6 here she comes xeol \n",
      " xbol-7 xeol \n",
      " xbol-8 [verse-2] xeol \n",
      " xbol-9 here she comes , here she comes xeol \n",
      " xbol-10 here she comes , here she comes 57.72286504894128\n"
     ]
    }
   ],
   "source": [
    "#print all of the final options of songs\n",
    "song, score = final_scores[0]\n",
    "print_words(song)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
