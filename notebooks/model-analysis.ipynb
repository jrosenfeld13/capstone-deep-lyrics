{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Analysis Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to analyze how multimodal learning has affected our weights. This is different than evaluation on the final generated lyrics. Instead, here we focus of if the features we implemented have \"moved the needle\" in terms of the distribution of the next word.\n",
    "\n",
    "This notebook is meant to be run after the model fitting notebooks. Thus, we will ignore any data collection processes, making the assumption that these have already been executed in the modeling notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.tokenize\n",
    "import itertools\n",
    "import datetime\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "\n",
    "from copy import copy, deepcopy\n",
    "from enum import Enum\n",
    "\n",
    "from graphviz import Digraph\n",
    "\n",
    "import datetime\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from src.data_collection.multimodal_data import *\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Generate Text For Pure LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a `get_probs` option for text generation. For this method, generate a shorter sequence of text with a very wide beam. Then, will perform a bag of words analysis across different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step(learner, context, context_length, temp=1):\n",
    "\n",
    "    model = learner.model\n",
    "    \n",
    "    if GPU:\n",
    "        context = LongTensor(context[-context_length:]).view(-1,1).cuda()\n",
    "    else:\n",
    "        context = LongTensor(context[-context_length:]).view(-1,1).cpu()\n",
    "    \n",
    "    context = torch.autograd.Variable(context)\n",
    "    \n",
    "    model.reset()\n",
    "    model.eval()\n",
    "        \n",
    "    # forward pass the \"context\" into the model\n",
    "    result, *_ = model(context)\n",
    "    result = result[-1]\n",
    "\n",
    "    # set unk and pad to 0 prob\n",
    "    # i.e. never pick unknown or pad\n",
    "    result[0] = -np.inf\n",
    "    result[1] = -np.inf\n",
    "\n",
    "    # softmax and normalize\n",
    "    probabilities = F.softmax(result/temp, dim=0)\n",
    "    probabilities = np.asarray(probabilities.detach().cpu(), dtype=np.float)\n",
    "    probabilities /= np.sum(probabilities) \n",
    "    return probabilities\n",
    "\n",
    "def get_word_from_index(idx):\n",
    "\n",
    "    return data_lm.valid_ds.vocab.textify([idx])\n",
    "\n",
    "\n",
    "def print_words(context):\n",
    "    for i in range(len(context)):\n",
    "        \n",
    "        step = context[i]\n",
    "\n",
    "        word = data_lm.valid_ds.vocab.textify([step])\n",
    "\n",
    "        if word == 'xeol':\n",
    "            word = 'xeol \\n'\n",
    "        elif 'xbol' in word:\n",
    "            word = word\n",
    "        elif word == 'xeos': \n",
    "            print(word)\n",
    "            break\n",
    "            \n",
    "        print(word, end=' ')   \n",
    "\n",
    "def generate_text(learner, seed_text=['xbos'],\n",
    "                  max_len=500, GPU=False, context_length=20,\n",
    "                  beam_width=5, temp=1, multinomial=True,\n",
    "                  verbose=True, graph=False, get_probs=False):\n",
    "    \"\"\"Generates text with a given learner and returns best options.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    learner : RNNLearner Language Model (RNNLearner.language_model())\n",
    "        Fastai RNNLearner with tokenized language model data already loaded \n",
    "        \n",
    "    seed_text : list or str\n",
    "        List of strings where each item is a token. (e.g. ['the', 'cat']) or string that is split on white space\n",
    "\n",
    "    max_len : int\n",
    "        Number of words in generated sequence\n",
    "        \n",
    "    gpu : bool\n",
    "        If you're using a GPU or not...\n",
    "    \n",
    "    context_length : int\n",
    "        Amount of words that get input as \"context\" into the model. Set to 0 for no limit   \n",
    "        \n",
    "    beam_width : int\n",
    "        How many new word indices to try out...computationally expensive\n",
    "    \n",
    "    verbose : bool\n",
    "        If True, prints every possible context for a given word cycle\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    context_and_scores : list of lists\n",
    "        Returns a sorted list of the entire tree search of contexts and their respective scores in the form:\n",
    "        [[context, score], [context, score], ..., [context, score]]\n",
    "    \"\"\"\n",
    "        \n",
    "    if isinstance(seed_text, str):\n",
    "        seed_text = data_lm.train_ds.vocab.numericalize(seed_text.split(' '))\n",
    "    \n",
    "    \n",
    "    # Width for the beam search, to be externalized along with general decoding\n",
    "    beam_width = beam_width\n",
    "    \n",
    "    if graph:\n",
    "        optimization_graph = Digraph()\n",
    "\n",
    "    # List of candidate word sequence. We'll maintain #beam_width top sequences here.\n",
    "    # The context is a list of words, the scores are the sum of the log probabilities of each word\n",
    "    context_and_scores = [[seed_text, 0.0]]\n",
    "    \n",
    "    # Loop over max number of words\n",
    "    for word_number in tqdm(range(max_len)):\n",
    "#         print(f'Generating word: {word_number+1} / {max_len}')\n",
    "\n",
    "        candidates = []\n",
    "        next_word_probs = []\n",
    "        \n",
    "        # For each possible context that we've generated so far, generate new probabilities, \n",
    "        # and pick an additional #beam_width next candidates\n",
    "        for i in range(len(context_and_scores)):\n",
    "            # Get a new sequence of word indices and log-probability\n",
    "            # Example: [[2, 138, 661], 23.181717]\n",
    "            context, score = context_and_scores[i]\n",
    "            \n",
    "            # Obtain probabilities for next word given the context \n",
    "            probabilities = generate_step(learner, context, context_length, temp)\n",
    "\n",
    "            # Multinomial draw from the probabilities\n",
    "            if multinomial:\n",
    "                multinom_draw = np.random.multinomial(beam_width, probabilities)\n",
    "                top_probabilities = np.argwhere(multinom_draw != 0).flatten()                    \n",
    "                \n",
    "            # top-k from probabilities    \n",
    "            else:\n",
    "                top_probabilities = np.argsort(-probabilities)[:beam_width]\n",
    "                        \n",
    "            #For each possible new candidate, update the context and scores\n",
    "            for j in range(len(top_probabilities)):\n",
    "                next_word_idx = top_probabilities[j]\n",
    "                new_context = context + [next_word_idx]\n",
    "                candidate = [new_context, (score - np.log(probabilities[next_word_idx]))]\n",
    "                candidates.append(candidate)\n",
    "                \n",
    "                if get_probs:\n",
    "                    next_word_prob = probabilities[next_word_idx]\n",
    "                    potential_next_word = get_word_from_index(next_word_idx)\n",
    "                    prior_context = [get_word_from_index(w) for w in context]\n",
    "                    next_word_probs.append((prior_context, potential_next_word, next_word_prob))\n",
    "                \n",
    "                if graph:\n",
    "                    optimization_graph.node(\"%d_%d\" % (word_number, next_word_idx), \"%s (%.2f)\" % (get_word_from_index(next_word_idx), candidate[1]))\n",
    "                    optimization_graph.edge(\"%d_%d\" % (word_number - 1, context[len(context) -1]), \"%d_%d\" % (word_number, next_word_idx))\n",
    "                \n",
    "        #update the running tally of context and scores and sort by probability of each entry\n",
    "        context_and_scores = candidates\n",
    "        context_and_scores = sorted(context_and_scores, key = lambda x: x[1]) #sort by top entries\n",
    "\n",
    "        context_and_scores = context_and_scores[:30] #for now, only keep the top 30 to speed things up but we can/should change this to beam_width or something else\n",
    "        \n",
    "        if verbose:\n",
    "            for context, score in context_and_scores:\n",
    "                print_words(context)\n",
    "                print('\\n')\n",
    "\n",
    "    if graph:\n",
    "        now = str(datetime.datetime.now())\n",
    "        optimization_graph.render(directory='graph_viz/', filename=now, cleanup=True)\n",
    "        \n",
    "    if get_probs:\n",
    "        next_word_probs = sorted(next_word_probs, key=lambda x: -x[2])\n",
    "        return next_word_probs\n",
    "        \n",
    "    return context_and_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import stopwords and add my own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/syang/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "stop_words.add(\"'t\")\n",
    "stop_words.add(\"'ll\")\n",
    "stop_words.add(\"’t\")\n",
    "stop_words.add(\"'ve\")\n",
    "stop_words.add(\",\")\n",
    "stop_words.add(\"'s\")\n",
    "stop_words.add(\"'re\")\n",
    "stop_words.add(\"'m\")\n",
    "stop_words.add(\"don\")\n",
    "stop_words.add(\"won\")\n",
    "stop_words.add(\"xbol\")\n",
    "stop_words.add(\"xbos\")\n",
    "stop_words.add(\"xeol\")\n",
    "stop_words.add(\"xeos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure Language Model (3.2-ULMFiT-108k)\n",
    "* Transfer learning from wikitext-103 to 108k corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(f'../data/models/3.2-ULMFiT-108k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = TextLMDataBunch.from_tokens(MODEL_PATH,\n",
    "                                      bs=128,\n",
    "                                      max_vocab=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk', 'xxpad', 'xbol', 'xeol', ',', 'i', 'the', 'you', 'to', 'and']"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lm.train_ds.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = RNNLearner.language_model(data_lm,\n",
    "                                  pretrained_fnames=['3.2-ULMFiT-108k_best',\n",
    "                                                     '3.2-ULMFiT-108k_itos'],\n",
    "                                  drop_mult=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.64it/s]\n"
     ]
    }
   ],
   "source": [
    "next_word_probs = generate_text(learn, GPU=GPU,\n",
    "                             seed_text='xbos xbol [verse-1] xeol xbol',\n",
    "                             max_len=2, context_length=200,\n",
    "                             beam_width=1000, verbose=False,\n",
    "                             temp=1, multinomial=False, graph=False, get_probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word_probs = [s for s in next_word_probs if s[1] not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>next_word</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[xbos, xbol, [verse-1], xeol, xbol, one]</td>\n",
       "      <td>day</td>\n",
       "      <td>0.290520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[xbos, xbol, [verse-1], xeol, xbol, no]</td>\n",
       "      <td>one</td>\n",
       "      <td>0.241687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[xbos, xbol, [verse-1], xeol, xbol, they]</td>\n",
       "      <td>say</td>\n",
       "      <td>0.168200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[xbos, xbol, [verse-1], xeol, xbol, here]</td>\n",
       "      <td>comes</td>\n",
       "      <td>0.165289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[xbos, xbol, [verse-1], xeol, xbol, how]</td>\n",
       "      <td>many</td>\n",
       "      <td>0.128627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[xbos, xbol, [verse-1], xeol, xbol, my]</td>\n",
       "      <td>heart</td>\n",
       "      <td>0.094780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[xbos, xbol, [verse-1], xeol, xbol, so]</td>\n",
       "      <td>many</td>\n",
       "      <td>0.085646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[xbos, xbol, [verse-1], xeol, xbol, how]</td>\n",
       "      <td>long</td>\n",
       "      <td>0.074577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[xbos, xbol, [verse-1], xeol, xbol, no]</td>\n",
       "      <td>matter</td>\n",
       "      <td>0.071768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[xbos, xbol, [verse-1], xeol, xbol, one]</td>\n",
       "      <td>night</td>\n",
       "      <td>0.071546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     context next_word  probability\n",
       "0   [xbos, xbol, [verse-1], xeol, xbol, one]       day     0.290520\n",
       "1    [xbos, xbol, [verse-1], xeol, xbol, no]       one     0.241687\n",
       "2  [xbos, xbol, [verse-1], xeol, xbol, they]       say     0.168200\n",
       "3  [xbos, xbol, [verse-1], xeol, xbol, here]     comes     0.165289\n",
       "4   [xbos, xbol, [verse-1], xeol, xbol, how]      many     0.128627\n",
       "5    [xbos, xbol, [verse-1], xeol, xbol, my]     heart     0.094780\n",
       "6    [xbos, xbol, [verse-1], xeol, xbol, so]      many     0.085646\n",
       "7   [xbos, xbol, [verse-1], xeol, xbol, how]      long     0.074577\n",
       "8    [xbos, xbol, [verse-1], xeol, xbol, no]    matter     0.071768\n",
       "9   [xbos, xbol, [verse-1], xeol, xbol, one]     night     0.071546"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(next_word_probs, columns=['context', 'next_word', 'probability'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>next_word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>3.362332e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>2.910156e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>many</th>\n",
       "      <td>2.176971e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>say</th>\n",
       "      <td>2.010951e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>2.009788e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comes</th>\n",
       "      <td>1.932980e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>1.732307e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>1.673621e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart</th>\n",
       "      <td>1.511759e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>got</th>\n",
       "      <td>1.465183e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>could</th>\n",
       "      <td>1.396111e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long</th>\n",
       "      <td>1.272330e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>night</th>\n",
       "      <td>1.248038e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>1.194325e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>1.101350e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>man</th>\n",
       "      <td>1.064807e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eyes</th>\n",
       "      <td>1.041544e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baby</th>\n",
       "      <td>1.036580e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>7.674185e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matter</th>\n",
       "      <td>7.251915e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>came</th>\n",
       "      <td>7.071555e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>girl</th>\n",
       "      <td>6.657870e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>used</th>\n",
       "      <td>6.636602e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>never</th>\n",
       "      <td>6.574491e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>life</th>\n",
       "      <td>6.539243e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>took</th>\n",
       "      <td>6.441442e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>6.430798e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>face</th>\n",
       "      <td>6.378402e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world</th>\n",
       "      <td>5.686991e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>5.429876e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>co</th>\n",
       "      <td>9.627701e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jazz</th>\n",
       "      <td>9.582232e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cream</th>\n",
       "      <td>9.480978e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'en</th>\n",
       "      <td>9.358466e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'l</th>\n",
       "      <td>9.308627e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ka</th>\n",
       "      <td>9.283582e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>+</th>\n",
       "      <td>1.964950e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leading</th>\n",
       "      <td>1.225776e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>following</th>\n",
       "      <td>1.173724e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>9.144422e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starting</th>\n",
       "      <td>8.829972e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.732445e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>becoming</th>\n",
       "      <td>8.534373e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>using</th>\n",
       "      <td>7.497474e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carrying</th>\n",
       "      <td>7.160597e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>design</th>\n",
       "      <td>6.900913e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>towards</th>\n",
       "      <td>6.564782e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fourteen</th>\n",
       "      <td>6.175258e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mostly</th>\n",
       "      <td>6.154774e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[verse]</th>\n",
       "      <td>5.842068e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eleven</th>\n",
       "      <td>5.779879e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>5.758642e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>backwards</th>\n",
       "      <td>5.567018e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refrain</th>\n",
       "      <td>5.471007e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>5.429788e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blowing</th>\n",
       "      <td>5.409609e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>otherwise</th>\n",
       "      <td>5.299578e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>5.256109e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>version</th>\n",
       "      <td>5.213993e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strip</th>\n",
       "      <td>5.193349e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4016 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            probability\n",
       "next_word              \n",
       "day        3.362332e-01\n",
       "one        2.910156e-01\n",
       "many       2.176971e-01\n",
       "say        2.010951e-01\n",
       "said       2.009788e-01\n",
       "comes      1.932980e-01\n",
       "love       1.732307e-01\n",
       "time       1.673621e-01\n",
       "heart      1.511759e-01\n",
       "got        1.465183e-01\n",
       "could      1.396111e-01\n",
       "long       1.272330e-01\n",
       "night      1.248038e-01\n",
       "know       1.194325e-01\n",
       "little     1.101350e-01\n",
       "man        1.064807e-01\n",
       "eyes       1.041544e-01\n",
       "baby       1.036580e-01\n",
       "come       7.674185e-02\n",
       "matter     7.251915e-02\n",
       "came       7.071555e-02\n",
       "girl       6.657870e-02\n",
       "used       6.636602e-02\n",
       "never      6.574491e-02\n",
       "life       6.539243e-02\n",
       "took       6.441442e-02\n",
       "!          6.430798e-02\n",
       "face       6.378402e-02\n",
       "world      5.686991e-02\n",
       "way        5.429876e-02\n",
       "...                 ...\n",
       "co         9.627701e-07\n",
       "jazz       9.582232e-07\n",
       "cream      9.480978e-07\n",
       "'en        9.358466e-07\n",
       "'l         9.308627e-07\n",
       "ka         9.283582e-07\n",
       "+          1.964950e-07\n",
       "leading    1.225776e-07\n",
       "following  1.173724e-07\n",
       "(          9.144422e-08\n",
       "starting   8.829972e-08\n",
       "30         8.732445e-08\n",
       "becoming   8.534373e-08\n",
       "using      7.497474e-08\n",
       "carrying   7.160597e-08\n",
       "design     6.900913e-08\n",
       "towards    6.564782e-08\n",
       "fourteen   6.175258e-08\n",
       "mostly     6.154774e-08\n",
       "[verse]    5.842068e-08\n",
       "eleven     5.779879e-08\n",
       "support    5.758642e-08\n",
       "backwards  5.567018e-08\n",
       "refrain    5.471007e-08\n",
       "100        5.429788e-08\n",
       "blowing    5.409609e-08\n",
       "otherwise  5.299578e-08\n",
       "2000       5.256109e-08\n",
       "version    5.213993e-08\n",
       "strip      5.193349e-08\n",
       "\n",
       "[4016 rows x 1 columns]"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('next_word').sum().sort_values(by='probability', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.log(df['probability']).hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:29<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "final_scores = generate_text(learn, GPU=GPU,\n",
    "                             seed_text='xbos xbol [verse-1] xeol xbol',\n",
    "                             max_len=50, context_length=200,\n",
    "                             beam_width=3, verbose=False,\n",
    "                             temp=1.4, multinomial=True, graph=False, get_probs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xbos xbol [verse-1] xeol \n",
      " xbol inside my head there 's something unsaid xeol \n",
      " xbol i 'm down on my knees to keep someone next to me xeol \n",
      " xbol and i don 't know when or why xeol \n",
      " xbol i don 't care what it 's all about xeol \n",
      " xbol xeol \n",
      " xbol [pre-chorus] xeol \n",
      " xbol i 115.52940888716925\n"
     ]
    }
   ],
   "source": [
    "#print all of the final options of songs\n",
    "song, score = final_scores[0]\n",
    "print_words(song)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure Language Model (3.3-ULMFiT-108k)\n",
    "* Transfer learning from model 3.2 to 500k corpus\n",
    "* Transfer learning from 500k corpus to 108k corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(f'../data/models/3.3-ULMFiT-108k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = TextLMDataBunch.from_tokens(MODEL_PATH,\n",
    "                                      bs=128,\n",
    "                                      max_vocab=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk', 'xxpad', 'xeol', ',', 'i', 'the', 'you', 'to', 'and', 'a']"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lm.train_ds.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = RNNLearner.language_model(data_lm,\n",
    "                                  pretrained_fnames=['3.3-ULMFiT-108k_best',\n",
    "                                                     '3.3-ULMFiT-108k_itos'],\n",
    "                                  drop_mult=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "next_word_probs = generate_text(learn, GPU=GPU,\n",
    "                             seed_text='xbos xbol-1 [verse-1] xeol xbol-2',\n",
    "                             max_len=2, context_length=200,\n",
    "                             beam_width=1000, verbose=False,\n",
    "                             temp=1, multinomial=False, graph=False, get_probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word_probs = [s for s in next_word_probs if s[1] not in stop_words]\n",
    "next_word_probs = [s for s in next_word_probs if 'xbol' not in s[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>next_word</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[xbos, xbol-1, [verse-1], xeol, xbol-2, one]</td>\n",
       "      <td>day</td>\n",
       "      <td>0.251755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[xbos, xbol-1, [verse-1], xeol, xbol-2, no]</td>\n",
       "      <td>one</td>\n",
       "      <td>0.201328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[xbos, xbol-1, [verse-1], xeol, xbol-2, they]</td>\n",
       "      <td>say</td>\n",
       "      <td>0.191211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[xbos, xbol-1, [verse-1], xeol, xbol-2, so]</td>\n",
       "      <td>many</td>\n",
       "      <td>0.125984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[xbos, xbol-1, [verse-1], xeol, xbol-2, how]</td>\n",
       "      <td>many</td>\n",
       "      <td>0.116903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[xbos, xbol-1, [verse-1], xeol, xbol-2, here]</td>\n",
       "      <td>comes</td>\n",
       "      <td>0.109665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[xbos, xbol-1, [verse-1], xeol, xbol-2, how]</td>\n",
       "      <td>long</td>\n",
       "      <td>0.095326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[xbos, xbol-1, [verse-1], xeol, xbol-2, he]</td>\n",
       "      <td>said</td>\n",
       "      <td>0.078437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[xbos, xbol-1, [verse-1], xeol, xbol-2, she]</td>\n",
       "      <td>said</td>\n",
       "      <td>0.075234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[xbos, xbol-1, [verse-1], xeol, xbol-2, my]</td>\n",
       "      <td>heart</td>\n",
       "      <td>0.071960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         context next_word  probability\n",
       "0   [xbos, xbol-1, [verse-1], xeol, xbol-2, one]       day     0.251755\n",
       "1    [xbos, xbol-1, [verse-1], xeol, xbol-2, no]       one     0.201328\n",
       "2  [xbos, xbol-1, [verse-1], xeol, xbol-2, they]       say     0.191211\n",
       "3    [xbos, xbol-1, [verse-1], xeol, xbol-2, so]      many     0.125984\n",
       "4   [xbos, xbol-1, [verse-1], xeol, xbol-2, how]      many     0.116903\n",
       "5  [xbos, xbol-1, [verse-1], xeol, xbol-2, here]     comes     0.109665\n",
       "6   [xbos, xbol-1, [verse-1], xeol, xbol-2, how]      long     0.095326\n",
       "7    [xbos, xbol-1, [verse-1], xeol, xbol-2, he]      said     0.078437\n",
       "8   [xbos, xbol-1, [verse-1], xeol, xbol-2, she]      said     0.075234\n",
       "9    [xbos, xbol-1, [verse-1], xeol, xbol-2, my]     heart     0.071960"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(next_word_probs, columns=['context', 'next_word', 'probability'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>next_word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>2.964834e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>2.697427e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>many</th>\n",
       "      <td>2.458187e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>say</th>\n",
       "      <td>2.332541e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>2.107267e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long</th>\n",
       "      <td>1.447440e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>got</th>\n",
       "      <td>1.430223e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comes</th>\n",
       "      <td>1.269305e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>1.262512e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>1.230618e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>could</th>\n",
       "      <td>1.203694e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baby</th>\n",
       "      <td>1.192115e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>1.142660e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>night</th>\n",
       "      <td>1.135521e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>man</th>\n",
       "      <td>1.044062e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>1.016951e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart</th>\n",
       "      <td>8.617054e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>’s</th>\n",
       "      <td>8.244108e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>7.583455e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>girl</th>\n",
       "      <td>7.080959e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matter</th>\n",
       "      <td>6.708994e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>need</th>\n",
       "      <td>6.647583e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>never</th>\n",
       "      <td>6.340024e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>6.186431e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>look</th>\n",
       "      <td>6.183985e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world</th>\n",
       "      <td>6.076625e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>took</th>\n",
       "      <td>6.055048e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>6.000242e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>5.891895e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>came</th>\n",
       "      <td>5.825580e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deutschland</th>\n",
       "      <td>8.156344e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uma</th>\n",
       "      <td>8.116661e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[post-chorus]</th>\n",
       "      <td>8.101195e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estas</th>\n",
       "      <td>8.077781e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eve</th>\n",
       "      <td>8.005202e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'?</th>\n",
       "      <td>8.002195e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nur</th>\n",
       "      <td>7.998235e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[pre-chorus]</th>\n",
       "      <td>7.940247e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sind</th>\n",
       "      <td>7.934244e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bom</th>\n",
       "      <td>7.918626e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abuse</th>\n",
       "      <td>7.896357e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pai</th>\n",
       "      <td>7.787997e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auch</th>\n",
       "      <td>7.727170e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>immer</th>\n",
       "      <td>7.711474e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pimp</th>\n",
       "      <td>7.709467e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ça</th>\n",
       "      <td>7.706586e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ahí</th>\n",
       "      <td>7.675823e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tão</th>\n",
       "      <td>7.606595e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'!</th>\n",
       "      <td>7.603991e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quoi</th>\n",
       "      <td>7.540845e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>belle</th>\n",
       "      <td>7.537696e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[verse-one]</th>\n",
       "      <td>7.527287e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doch</th>\n",
       "      <td>7.509262e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quanto</th>\n",
       "      <td>7.474481e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rely</th>\n",
       "      <td>7.444283e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nichts</th>\n",
       "      <td>7.417439e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nach</th>\n",
       "      <td>7.381386e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alla</th>\n",
       "      <td>7.360010e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eso</th>\n",
       "      <td>7.278101e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nena</th>\n",
       "      <td>7.239101e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4111 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                probability\n",
       "next_word                  \n",
       "day            2.964834e-01\n",
       "one            2.697427e-01\n",
       "many           2.458187e-01\n",
       "say            2.332541e-01\n",
       "said           2.107267e-01\n",
       "long           1.447440e-01\n",
       "got            1.430223e-01\n",
       "comes          1.269305e-01\n",
       "know           1.262512e-01\n",
       "time           1.230618e-01\n",
       "could          1.203694e-01\n",
       "baby           1.192115e-01\n",
       "come           1.142660e-01\n",
       "night          1.135521e-01\n",
       "man            1.044062e-01\n",
       "love           1.016951e-01\n",
       "heart          8.617054e-02\n",
       "’s             8.244108e-02\n",
       "little         7.583455e-02\n",
       "girl           7.080959e-02\n",
       "matter         6.708994e-02\n",
       "need           6.647583e-02\n",
       "never          6.340024e-02\n",
       "way            6.186431e-02\n",
       "look           6.183985e-02\n",
       "world          6.076625e-02\n",
       "took           6.055048e-02\n",
       "last           6.000242e-02\n",
       "!              5.891895e-02\n",
       "came           5.825580e-02\n",
       "...                     ...\n",
       "deutschland    8.156344e-07\n",
       "uma            8.116661e-07\n",
       "[post-chorus]  8.101195e-07\n",
       "estas          8.077781e-07\n",
       "eve            8.005202e-07\n",
       "'?             8.002195e-07\n",
       "nur            7.998235e-07\n",
       "[pre-chorus]   7.940247e-07\n",
       "sind           7.934244e-07\n",
       "bom            7.918626e-07\n",
       "abuse          7.896357e-07\n",
       "pai            7.787997e-07\n",
       "auch           7.727170e-07\n",
       "immer          7.711474e-07\n",
       "pimp           7.709467e-07\n",
       "ça             7.706586e-07\n",
       "ahí            7.675823e-07\n",
       "tão            7.606595e-07\n",
       "'!             7.603991e-07\n",
       "quoi           7.540845e-07\n",
       "belle          7.537696e-07\n",
       "[verse-one]    7.527287e-07\n",
       "doch           7.509262e-07\n",
       "quanto         7.474481e-07\n",
       "rely           7.444283e-07\n",
       "nichts         7.417439e-07\n",
       "nach           7.381386e-07\n",
       "alla           7.360010e-07\n",
       "eso            7.278101e-07\n",
       "nena           7.239101e-07\n",
       "\n",
       "[4111 rows x 1 columns]"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('next_word').sum().sort_values(by='probability', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.log(df['probability']).hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:29<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "final_scores = generate_text(learn, GPU=GPU,\n",
    "                             seed_text='xbos xbol-1 [verse-1] xeol xbol-2',\n",
    "                             max_len=50, context_length=200,\n",
    "                             beam_width=3, verbose=False,\n",
    "                             temp=1.4, multinomial=True, graph=False, get_probs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xbos xbol-1 [verse-1] xeol \n",
      " xbol-2 everything , everything across time xeol \n",
      " xbol-3 everything , everything , everything comes to nothing xeol \n",
      " xbol-4 everything and everything xeol \n",
      " xbol-5 everything is everything xeol \n",
      " xbol-6 xeol \n",
      " xbol-7 [chorus] xeol \n",
      " xbol-8 everything is everything xeol \n",
      " xbol-9 everything is everything xeol \n",
      " xbol-10 everything is everything xeol \n",
      " xbol-11 everything is everything 76.94984640800992\n"
     ]
    }
   ],
   "source": [
    "#print all of the final options of songs\n",
    "song, score = final_scores[0]\n",
    "print_words(song)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal Model (3.4-ULMFiT-MM-108k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where things get a bit trickier...\n",
    "Modify generate function to work with multimodal modeling\n",
    "\n",
    "This is the main analysis. We want to see if changing the audio features changes the predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(f'../data/models/3.4-ULMFiT-MM-108k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = TextLMDataBunch.from_tokens(MODEL_PATH,\n",
    "                                      bs=128,\n",
    "                                      max_vocab=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = copy(data_lm.train_ds)\n",
    "valid_text = copy(data_lm.valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Audio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/interim/msd-aggregate/msd-aggregate-train.csv')\n",
    "df_valid = pd.read_csv('../data/interim/msd-aggregate/msd-aggregate-valid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiModalRNN(\n",
       "  (encoder): Embedding(10002, 400, padding_idx=1)\n",
       "  (encoder_dp): EmbeddingDropout(\n",
       "    (emb): Embedding(10002, 400, padding_idx=1)\n",
       "  )\n",
       "  (rnns): None\n",
       "  (input_dp): RNNDropout()\n",
       "  (hidden_dps): ModuleList(\n",
       "    (0): RNNDropout()\n",
       "    (1): RNNDropout()\n",
       "    (2): RNNDropout()\n",
       "  )\n",
       "  (multimode): ModuleList(\n",
       "    (0): WeightDropout(\n",
       "      (module): LSTM(434, 1150)\n",
       "    )\n",
       "    (1): WeightDropout(\n",
       "      (module): LSTM(1150, 1150)\n",
       "    )\n",
       "    (2): WeightDropout(\n",
       "      (module): LSTM(1150, 400)\n",
       "    )\n",
       "  )\n",
       "  (multidecoder): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=10002, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_sz = 34\n",
    "vocab_sz = 10002\n",
    "emb_sz = 400\n",
    "n_hid = 1150\n",
    "n_layers = 3\n",
    "pad_token = 1\n",
    "qrnn = False\n",
    "bidir = False\n",
    "drop_mult = 0.5\n",
    "dps = np.array([0.25, 0.1, 0.2, 0.02, 0.15]) * drop_mult\n",
    "hidden_p = dps[4]\n",
    "input_p = dps[0]\n",
    "embed_p = dps[3]\n",
    "weight_p = dps[2]\n",
    "tie_weights = True\n",
    "output_p = dps[1]\n",
    "bias = True\n",
    "\n",
    "class MultiModalRNN(RNNCore):\n",
    "    def __init__(self, audio_sz, output_p, bias, tie_encoder:bool=True, **kwargs):\n",
    "        super(MultiModalRNN, self).__init__(**kwargs)\n",
    "        self.rnns = None\n",
    "        self.audio_sz = audio_sz\n",
    "        self.multimode = [nn.LSTM(emb_sz + audio_sz if l == 0 else n_hid,\n",
    "                                  (n_hid if l != n_layers - 1 else emb_sz)//self.ndir,\n",
    "                                  1, bidirectional=bidir) for l in range(n_layers)]\n",
    "        self.multimode = [WeightDropout(rnn, weight_p) for rnn in self.multimode]\n",
    "        self.multimode = torch.nn.ModuleList(self.multimode)\n",
    "        \n",
    "        if tie_encoder:\n",
    "            enc = self.encoder\n",
    "        else:\n",
    "            enc = None\n",
    "        \n",
    "        self.multidecoder = LinearDecoder(vocab_sz,\n",
    "                                          emb_sz,\n",
    "                                          output_p,\n",
    "                                          tie_encoder=enc,\n",
    "                                          bias=bias)\n",
    "        \n",
    "    def forward(self, input:LongTensor, input_audio:Tensor)->Tuple[Tensor,Tensor,Tensor]:\n",
    "        sl,bs = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        raw_output = self.input_dp(self.encoder_dp(input))\n",
    "        raw_output = torch.cat([raw_output, input_audio], dim=2)\n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.multimode, self.hidden_dps)):\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output)\n",
    "        self.hidden = to_detach(new_hidden)\n",
    "        \n",
    "        output = self.multidecoder.output_dp(outputs[-1])\n",
    "        decoded = self.multidecoder.decoder(output.view(output.size(0)*output.size(1),\n",
    "                                                        output.size(2)))\n",
    "        \n",
    "        return decoded, raw_outputs, outputs\n",
    "    \n",
    "    def _one_hidden(self, l:int)->Tensor:\n",
    "        \"Return one hidden state.\"\n",
    "        nh = (self.n_hid if l != self.n_layers - 1 else self.emb_sz)//self.ndir\n",
    "        return self.weights.new(self.ndir, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states.\"\n",
    "        [r.reset() for r in self.multimode if hasattr(r, 'reset')]\n",
    "        self.weights = next(self.parameters()).data\n",
    "        if self.qrnn: self.hidden = [self._one_hidden(l) for l in range(self.n_layers)]\n",
    "        else: self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]\n",
    "    \n",
    "multimodal_rnn = MultiModalRNN(audio_sz=audio_sz,\n",
    "                              vocab_sz=vocab_sz,\n",
    "                              emb_sz=emb_sz,\n",
    "                              n_hid=n_hid,\n",
    "                              n_layers=n_layers,\n",
    "                              pad_token=pad_token,\n",
    "                              qrnn=qrnn,\n",
    "                              bidir=bidir,\n",
    "                              hidden_p=hidden_p,\n",
    "                              input_p=input_p,\n",
    "                              embed_p=embed_p,\n",
    "                              weight_p=weight_p,\n",
    "                              output_p=output_p,\n",
    "                              bias=bias,\n",
    "                              tie_encoder=tie_weights)\n",
    "\n",
    "multimodal_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't actually need any of this data, but the current state isn't modular enough to not include it...\n",
    "\n",
    "One major update we need is to decouple the model with `learner`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "- scikit-learn version 0.20.0 is required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def log_features(X):\n",
    "    return np.log(X)\n",
    "log_feat = FunctionTransformer(log_features, validate=False)\n",
    "\n",
    "def bin_tempo(X):\n",
    "    '''\n",
    "    ref: https://en.wikipedia.org/wiki/Tempo#Italian_tempo_markings\n",
    "    These are rough loosely based on tempo markings above\n",
    "    Have considered both classical forms of music and popular\n",
    "    '''\n",
    "    assert X.shape[1] == 1, \"Only 1 column can be binned\"\n",
    "    bins = [0, 60, 76, 108, 120, 156, 176, 200, 500]\n",
    "    return pd.DataFrame(pd.cut(X.iloc[:,0], bins=bins))\n",
    "tempo_feat = FunctionTransformer(bin_tempo, validate=False)\n",
    "\n",
    "def bin_time_signature(X):\n",
    "    assert X.shape[1] == 1, \"Only 1 column can be binned\"\n",
    "    X['time_signature_bin'] = \"Other Signature\"\n",
    "    X.loc[X['time_signature'] == 4, 'time_signature_bin'] = '4/4 Signature'\n",
    "    X.loc[X['time_signature'] == 3, 'time_signature_bin'] = '3/4 Signature'\n",
    "    return X[['time_signature_bin']]\n",
    "time_feat = FunctionTransformer(bin_time_signature, validate=False)\n",
    "\n",
    "def to_string(X):\n",
    "    return X.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continous features\n",
    "numeric_features = ['artist_familiarity',\n",
    "                    'artist_hotttnesss',\n",
    "                    'loudness',\n",
    "                    'song_hotttnesss']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "# log features\n",
    "log_features = ['duration']\n",
    "log_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('log_feat', log_feat),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "# categorical features\n",
    "categorical_features = ['key', 'mode']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('stringify', FunctionTransformer(to_string, validate=False)),\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n",
    "\n",
    "# confidence features\n",
    "confidence_features = ['key_confidence',\n",
    "                       'mode_confidence',\n",
    "                       'time_signature_confidence'\n",
    "                      ]\n",
    "confidence_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=0))\n",
    "])\n",
    "\n",
    "# time signature feature\n",
    "time_feature = ['time_signature']\n",
    "time_transformer = Pipeline(steps=[\n",
    "    ('binner', time_feat),\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('stringify', FunctionTransformer(to_string, validate=False)),\n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n",
    "\n",
    "# tempo feature\n",
    "tempo_feature = ['tempo']\n",
    "tempo_transformer = Pipeline(steps=[\n",
    "    ('binner', tempo_feat),\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('stringify', FunctionTransformer(to_string, validate=False)),\n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('log', log_transformer, log_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('con', confidence_transformer, confidence_features),\n",
    "        ('time', time_transformer, time_feature),\n",
    "        ('tempo', tempo_transformer, tempo_feature)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syang/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('num', Pipeline(memory=None,\n",
       "     steps=[('imputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "       strategy='median', verbose=0)), ('scaler', MinMaxScaler(copy=True, feature_range=(0, 1)))]), ['artist_familiarity', 'artist_hotttnesss', 'loudness', 'song_hotttnes...=<class 'numpy.float64'>, handle_unknown='error',\n",
       "       n_values=None, sparse=True))]), ['tempo'])])"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_tfm = preprocessor.transform(df_train)\n",
    "df_valid_tfm = preprocessor.transform(df_valid)\n",
    "\n",
    "train_audio = AudioDataset(df_train_tfm, train_text)\n",
    "valid_audio = AudioDataset(df_valid_tfm, valid_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_data = MultimodalDataLoader(audio_dataset=train_audio,\n",
    "                                  dataset=train_text)\n",
    "multi_data_valid = MultimodalDataLoader(audio_dataset=valid_audio,\n",
    "                                  dataset=valid_text)\n",
    "multi_db = DataBunch(multi_data, multi_data_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = RNNLearner(multi_db, multimodal_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('3.4-ULMFiT-MM-108k_best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to Generate Text For Pure LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step(learner, context, audio, context_length, temp=1):\n",
    "\n",
    "    # FIX THIS\n",
    "    audio_size = train_audio.feature_size\n",
    "\n",
    "    model = learner.model\n",
    "    \n",
    "    if GPU:\n",
    "        context = LongTensor(context[-context_length:]).view(-1,1).cuda()\n",
    "    else:\n",
    "        context = LongTensor(context[-context_length:]).view(-1,1).cpu()\n",
    "    \n",
    "    context = torch.autograd.Variable(context)\n",
    "    \n",
    "    model.reset()\n",
    "    model.eval()\n",
    "    \n",
    "    if audio is None:\n",
    "        audio_features = Tensor([0]*audio_size*len(context))\\\n",
    "        .view(-1, 1, audio_size).cuda()\n",
    "    else:\n",
    "        audio_features = np.tile(audio, len(context))\n",
    "        audio_features = Tensor(audio_features).view(-1, 1, len(audio)).cuda()\n",
    "        \n",
    "    # forward pass the \"context\" into the model\n",
    "    result, *_ = model(context, audio_features)\n",
    "    result = result[-1]\n",
    "\n",
    "    # set unk and pad to 0 prob\n",
    "    # i.e. never pick unknown or pad\n",
    "    result[0] = -np.inf\n",
    "    result[1] = -np.inf\n",
    "\n",
    "    # softmax and normalize\n",
    "    probabilities = F.softmax(result/temp, dim=0)\n",
    "    probabilities = np.asarray(probabilities.detach().cpu(), dtype=np.float)\n",
    "    probabilities /= np.sum(probabilities) \n",
    "    return probabilities\n",
    "\n",
    "def get_word_from_index(idx):\n",
    "\n",
    "    return data_lm.valid_ds.vocab.textify([idx])\n",
    "\n",
    "\n",
    "def print_words(context):\n",
    "    for i in range(len(context)):\n",
    "        \n",
    "        step = context[i]\n",
    "\n",
    "        word = data_lm.valid_ds.vocab.textify([step])\n",
    "\n",
    "        if word == 'xeol':\n",
    "            word = 'xeol \\n'\n",
    "        elif 'xbol' in word:\n",
    "            word = word\n",
    "        elif word == 'xeos': \n",
    "            print(word)\n",
    "            break\n",
    "            \n",
    "        print(word, end=' ')   \n",
    "\n",
    "def generate_text(learner, seed_text=['xbos'], audio=None,\n",
    "                  max_len=500, GPU=False, context_length=20,\n",
    "                  beam_width=5, temp=1, multinomial=True,\n",
    "                  verbose=True, graph=False, get_probs=False):\n",
    "    \"\"\"Generates text with a given learner and returns best options.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    learner : RNNLearner Language Model (RNNLearner.language_model())\n",
    "        Fastai RNNLearner with tokenized language model data already loaded \n",
    "        \n",
    "    seed_text : list or str\n",
    "        List of strings where each item is a token. (e.g. ['the', 'cat']) or string that is split on white space\n",
    "\n",
    "    max_len : int\n",
    "        Number of words in generated sequence\n",
    "        \n",
    "    gpu : bool\n",
    "        If you're using a GPU or not...\n",
    "    \n",
    "    context_length : int\n",
    "        Amount of words that get input as \"context\" into the model. Set to 0 for no limit   \n",
    "        \n",
    "    beam_width : int\n",
    "        How many new word indices to try out...computationally expensive\n",
    "    \n",
    "    verbose : bool\n",
    "        If True, prints every possible context for a given word cycle\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    context_and_scores : list of lists\n",
    "        Returns a sorted list of the entire tree search of contexts and their respective scores in the form:\n",
    "        [[context, score], [context, score], ..., [context, score]]\n",
    "    \"\"\"\n",
    "        \n",
    "    if isinstance(seed_text, str):\n",
    "        seed_text = data_lm.train_ds.vocab.numericalize(seed_text.split(' '))\n",
    "    \n",
    "    \n",
    "    # Width for the beam search, to be externalized along with general decoding\n",
    "    beam_width = beam_width\n",
    "    \n",
    "    if graph:\n",
    "        optimization_graph = Digraph()\n",
    "\n",
    "    # List of candidate word sequence. We'll maintain #beam_width top sequences here.\n",
    "    # The context is a list of words, the scores are the sum of the log probabilities of each word\n",
    "    context_and_scores = [[seed_text, 0.0]]\n",
    "    \n",
    "    # Loop over max number of words\n",
    "    for word_number in tqdm(range(max_len)):\n",
    "#         print(f'Generating word: {word_number+1} / {max_len}')\n",
    "\n",
    "        candidates = []\n",
    "        next_word_probs = []\n",
    "        \n",
    "        # For each possible context that we've generated so far, generate new probabilities, \n",
    "        # and pick an additional #beam_width next candidates\n",
    "        for i in range(len(context_and_scores)):\n",
    "            # Get a new sequence of word indices and log-probability\n",
    "            # Example: [[2, 138, 661], 23.181717]\n",
    "            context, score = context_and_scores[i]\n",
    "            \n",
    "            # Obtain probabilities for next word given the context \n",
    "            probabilities = generate_step(learner, context, audio, context_length, temp)\n",
    "\n",
    "            # Multinomial draw from the probabilities\n",
    "            if multinomial:\n",
    "                multinom_draw = np.random.multinomial(beam_width, probabilities)\n",
    "                top_probabilities = np.argwhere(multinom_draw != 0).flatten()                    \n",
    "                \n",
    "            # top-k from probabilities    \n",
    "            else:\n",
    "                top_probabilities = np.argsort(-probabilities)[:beam_width]\n",
    "                        \n",
    "            #For each possible new candidate, update the context and scores\n",
    "            for j in range(len(top_probabilities)):\n",
    "                next_word_idx = top_probabilities[j]\n",
    "                new_context = context + [next_word_idx]\n",
    "                candidate = [new_context, (score - np.log(probabilities[next_word_idx]))]\n",
    "                candidates.append(candidate)\n",
    "                \n",
    "                if get_probs:\n",
    "                    next_word_prob = probabilities[next_word_idx]\n",
    "                    potential_next_word = get_word_from_index(next_word_idx)\n",
    "                    prior_context = [get_word_from_index(w) for w in context]\n",
    "                    next_word_probs.append((prior_context, potential_next_word, next_word_prob))\n",
    "                \n",
    "                if graph:\n",
    "                    optimization_graph.node(\"%d_%d\" % (word_number, next_word_idx), \"%s (%.2f)\" % (get_word_from_index(next_word_idx), candidate[1]))\n",
    "                    optimization_graph.edge(\"%d_%d\" % (word_number - 1, context[len(context) -1]), \"%d_%d\" % (word_number, next_word_idx))\n",
    "                \n",
    "        #update the running tally of context and scores and sort by probability of each entry\n",
    "        context_and_scores = candidates\n",
    "        context_and_scores = sorted(context_and_scores, key = lambda x: x[1]) #sort by top entries\n",
    "\n",
    "        context_and_scores = context_and_scores[:30] #for now, only keep the top 30 to speed things up but we can/should change this to beam_width or something else\n",
    "        \n",
    "        if verbose:\n",
    "            for context, score in context_and_scores:\n",
    "                print_words(context)\n",
    "                print('\\n')\n",
    "\n",
    "    if graph:\n",
    "        now = str(datetime.datetime.now())\n",
    "        optimization_graph.render(directory='graph_viz/', filename=now, cleanup=True)\n",
    "        \n",
    "    if get_probs:\n",
    "        next_word_probs = sorted(next_word_probs, key=lambda x: -x[2])\n",
    "        return next_word_probs\n",
    "        \n",
    "    return context_and_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features from validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode == 1, loudness == -2.3,\n",
    "# artist_hot == 0.57, artist_familiarity = 0.77, \n",
    "# song_hot == n/a\n",
    "xx = valid_audio[9126]\n",
    "\n",
    "# mode == 0, loudness == -15.5,\n",
    "# artist_hot == 0.36, artist_familiarity = 0.51, \n",
    "# song_hot == 0\n",
    "yy = valid_audio[7650]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Artificial\" Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create artificial audio features\n",
    "# 0.9 for all continuous features\n",
    "# key = 0 (of 12)\n",
    "# mode = 1 (of 2)\n",
    "# time = 1 (of 3)\n",
    "# tempo = 7 (of 9)\n",
    "zz1 = np.array([0.9, 0.9, 0.9, 0.9, 0.9, \n",
    "                1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "                0., 1.,\n",
    "                0.9, 0.9, 0.9,\n",
    "                0., 1., 0.,\n",
    "                0., 0., 0., 0., 0., 0., 0., 1., 0.\n",
    "               ])\n",
    "# 0.2 for all continuous features\n",
    "# key = 0 (of 12)\n",
    "# mode = 0 (of 2)\n",
    "# time = 1 (of 3)\n",
    "# tempo = 2 (of 9)\n",
    "zz2 = np.array([0.2, 0.2, 0.2, 0.2, 0.2, \n",
    "                1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "                1., 0.,\n",
    "                0.9, 0.9, 0.9,\n",
    "                0., 1., 0.,\n",
    "                0., 0., 1., 0., 0., 0., 0., 0., 0.\n",
    "               ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature set #1\n",
    "These are features that appear to be more \"Pop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.35s/it]\n"
     ]
    }
   ],
   "source": [
    "next_word_probs = generate_text(learn, GPU=GPU,\n",
    "                             seed_text='xbos xbol-1 [verse-1] xeol xbol-2',\n",
    "                             audio=zz1,\n",
    "                             max_len=10, context_length=200,\n",
    "                             beam_width=1000, verbose=False,\n",
    "                             temp=1, multinomial=False, graph=False, get_probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word_probs = [s for s in next_word_probs if s[1] not in stop_words]\n",
    "next_word_probs = [s for s in next_word_probs if 'xbol' not in s[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(next_word_probs, columns=['context', 'next_word', 'probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>next_word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mixed</th>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>winters</th>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seemed</th>\n",
       "      <td>0.001576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upside</th>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worlds</th>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>important</th>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>letting</th>\n",
       "      <td>0.001006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tries</th>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>melts</th>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           probability\n",
       "next_word             \n",
       "mixed         0.000009\n",
       "winters       0.000004\n",
       "zone          0.000010\n",
       "seemed        0.001576\n",
       "upside        0.000072\n",
       "worlds        0.000002\n",
       "important     0.000027\n",
       "letting       0.001006\n",
       "tries         0.000025\n",
       "melts         0.000008"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz1_words = df.groupby('next_word').sum().sort_values(by='probability', ascending=False)\n",
    "zz1_words.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.log(df['probability']).hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:29<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "final_scores = generate_text(learn, GPU=GPU,\n",
    "                             seed_text='xbos xbol-1 [verse-1] xeol xbol-2',\n",
    "                             audio=zz1,\n",
    "                             max_len=50, context_length=200,\n",
    "                             beam_width=3, verbose=False,\n",
    "                             temp=1.4, multinomial=True, graph=False, get_probs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xbos xbol-1 [verse-1] xeol \n",
      " xbol-2 designed for being in love xeol \n",
      " xbol-3 oh , and now i know just what it means xeol \n",
      " xbol-4 yeah , yeah , yeah , yeah , c 'mon now xeol \n",
      " xbol-5 yeah , yeah , yeah , yeah , yeah , yeah , yeah xeol \n",
      " xbol-6 xeol \n",
      " xbol-7 [pre-chorus] 94.3277459041761\n"
     ]
    }
   ],
   "source": [
    "#print all of the final options of songs\n",
    "song, score = final_scores[0]\n",
    "print_words(song)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Features set #2\n",
    "These are features that are less \"Pop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.49s/it]\n"
     ]
    }
   ],
   "source": [
    "next_word_probs = generate_text(learn, GPU=GPU,\n",
    "                             seed_text='xbos xbol-1 [verse-1] xeol xbol-2',\n",
    "                             audio=zz2,\n",
    "                             max_len=10, context_length=200,\n",
    "                             beam_width=1000, verbose=False,\n",
    "                             temp=1, multinomial=False, graph=False, get_probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word_probs = [s for s in next_word_probs if s[1] not in stop_words]\n",
    "next_word_probs = [s for s in next_word_probs if 'xbol' not in s[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(next_word_probs, columns=['context', 'next_word', 'probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>next_word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(say</th>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leavin</th>\n",
       "      <td>0.000352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(like</th>\n",
       "      <td>0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nature</th>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waited</th>\n",
       "      <td>0.008582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pulled</th>\n",
       "      <td>0.000369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uhh</th>\n",
       "      <td>0.000111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>punk</th>\n",
       "      <td>0.000030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>driven</th>\n",
       "      <td>0.000029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moment</th>\n",
       "      <td>0.000120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           probability\n",
       "next_word             \n",
       "(say          0.000039\n",
       "leavin        0.000352\n",
       "(like         0.000028\n",
       "nature        0.000016\n",
       "waited        0.008582\n",
       "pulled        0.000369\n",
       "uhh           0.000111\n",
       "punk          0.000030\n",
       "driven        0.000029\n",
       "moment        0.000120"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz2_words = df.groupby('next_word').sum().sort_values(by='probability', ascending=False)\n",
    "zz2_words.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability_x</th>\n",
       "      <th>probability_y</th>\n",
       "      <th>diff</th>\n",
       "      <th>diff_abs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>next_word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>know</th>\n",
       "      <td>1.036299e+00</td>\n",
       "      <td>9.804708e-01</td>\n",
       "      <td>0.055828</td>\n",
       "      <td>0.055828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td>9.288499e-02</td>\n",
       "      <td>5.772295e-02</td>\n",
       "      <td>0.035162</td>\n",
       "      <td>0.035162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waiting</th>\n",
       "      <td>5.027934e-02</td>\n",
       "      <td>1.626138e-02</td>\n",
       "      <td>0.034018</td>\n",
       "      <td>0.034018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thought</th>\n",
       "      <td>4.616358e-02</td>\n",
       "      <td>2.407196e-02</td>\n",
       "      <td>0.022092</td>\n",
       "      <td>0.022092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guess</th>\n",
       "      <td>4.955076e-02</td>\n",
       "      <td>3.005991e-02</td>\n",
       "      <td>0.019491</td>\n",
       "      <td>0.019491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>never</th>\n",
       "      <td>5.250886e-02</td>\n",
       "      <td>3.737578e-02</td>\n",
       "      <td>0.015133</td>\n",
       "      <td>0.015133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>today</th>\n",
       "      <td>1.841231e-02</td>\n",
       "      <td>7.589045e-03</td>\n",
       "      <td>0.010823</td>\n",
       "      <td>0.010823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feel</th>\n",
       "      <td>2.491553e-02</td>\n",
       "      <td>1.563683e-02</td>\n",
       "      <td>0.009279</td>\n",
       "      <td>0.009279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>met</th>\n",
       "      <td>6.527156e-02</td>\n",
       "      <td>7.406162e-02</td>\n",
       "      <td>-0.008790</td>\n",
       "      <td>0.008790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>want</th>\n",
       "      <td>2.445821e-02</td>\n",
       "      <td>1.658109e-02</td>\n",
       "      <td>0.007877</td>\n",
       "      <td>0.007877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wanna</th>\n",
       "      <td>1.573250e-02</td>\n",
       "      <td>8.061414e-03</td>\n",
       "      <td>0.007671</td>\n",
       "      <td>0.007671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>got</th>\n",
       "      <td>4.143292e-02</td>\n",
       "      <td>3.389462e-02</td>\n",
       "      <td>0.007538</td>\n",
       "      <td>0.007538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>2.944180e-02</td>\n",
       "      <td>2.223908e-02</td>\n",
       "      <td>0.007203</td>\n",
       "      <td>0.007203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'cause</th>\n",
       "      <td>1.385125e-01</td>\n",
       "      <td>1.455358e-01</td>\n",
       "      <td>-0.007023</td>\n",
       "      <td>0.007023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>4.243662e-01</td>\n",
       "      <td>4.313585e-01</td>\n",
       "      <td>-0.006992</td>\n",
       "      <td>0.006992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>need</th>\n",
       "      <td>1.494429e-02</td>\n",
       "      <td>8.703019e-03</td>\n",
       "      <td>0.006241</td>\n",
       "      <td>0.006241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>could</th>\n",
       "      <td>2.434518e-02</td>\n",
       "      <td>1.819474e-02</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.006150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>really</th>\n",
       "      <td>2.041486e-02</td>\n",
       "      <td>1.448930e-02</td>\n",
       "      <td>0.005926</td>\n",
       "      <td>0.005926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'d</th>\n",
       "      <td>1.320298e-02</td>\n",
       "      <td>7.361814e-03</td>\n",
       "      <td>0.005841</td>\n",
       "      <td>0.005841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wish</th>\n",
       "      <td>1.271291e-02</td>\n",
       "      <td>7.099661e-03</td>\n",
       "      <td>0.005613</td>\n",
       "      <td>0.005613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>looking</th>\n",
       "      <td>1.892449e-02</td>\n",
       "      <td>1.380801e-02</td>\n",
       "      <td>0.005116</td>\n",
       "      <td>0.005116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long</th>\n",
       "      <td>6.534804e-02</td>\n",
       "      <td>7.027301e-02</td>\n",
       "      <td>-0.004925</td>\n",
       "      <td>0.004925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>used</th>\n",
       "      <td>1.299919e-02</td>\n",
       "      <td>8.284581e-03</td>\n",
       "      <td>0.004715</td>\n",
       "      <td>0.004715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hope</th>\n",
       "      <td>1.230448e-02</td>\n",
       "      <td>7.957027e-03</td>\n",
       "      <td>0.004347</td>\n",
       "      <td>0.004347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>every</th>\n",
       "      <td>2.526845e-02</td>\n",
       "      <td>2.959710e-02</td>\n",
       "      <td>-0.004329</td>\n",
       "      <td>0.004329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sleeping</th>\n",
       "      <td>1.105208e-02</td>\n",
       "      <td>1.534573e-02</td>\n",
       "      <td>-0.004294</td>\n",
       "      <td>0.004294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nothing</th>\n",
       "      <td>1.829629e-02</td>\n",
       "      <td>2.245234e-02</td>\n",
       "      <td>-0.004156</td>\n",
       "      <td>0.004156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baby</th>\n",
       "      <td>6.936895e-02</td>\n",
       "      <td>6.522052e-02</td>\n",
       "      <td>0.004148</td>\n",
       "      <td>0.004148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cause</th>\n",
       "      <td>8.305325e-02</td>\n",
       "      <td>8.706293e-02</td>\n",
       "      <td>-0.004010</td>\n",
       "      <td>0.004010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>since</th>\n",
       "      <td>3.065851e-01</td>\n",
       "      <td>3.027332e-01</td>\n",
       "      <td>0.003852</td>\n",
       "      <td>0.003852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thy</th>\n",
       "      <td>2.559936e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>3.331970e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toward</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.564892e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tragic</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.474153e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trees</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.953106e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>troubled</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.416413e-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tumble</th>\n",
       "      <td>8.996105e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twisting</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.418839e-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>una</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.209855e-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undercover</th>\n",
       "      <td>1.137236e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uptight</th>\n",
       "      <td>1.288822e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ur</th>\n",
       "      <td>1.657762e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vine</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.385004e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visions</th>\n",
       "      <td>1.680427e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>voice</th>\n",
       "      <td>3.116573e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weakness</th>\n",
       "      <td>2.230290e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wins</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.633342e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wisdom</th>\n",
       "      <td>3.750825e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wood</th>\n",
       "      <td>3.057208e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worlds</th>\n",
       "      <td>2.031700e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.848108e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yearning</th>\n",
       "      <td>2.527472e-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yell</th>\n",
       "      <td>3.117587e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yonder</th>\n",
       "      <td>1.213045e-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero</th>\n",
       "      <td>1.629570e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zu</th>\n",
       "      <td>9.740342e-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>}</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.471060e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>´ll</th>\n",
       "      <td>2.469968e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‘em</th>\n",
       "      <td>2.512883e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>”</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.557401e-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3081 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            probability_x  probability_y      diff  diff_abs\n",
       "next_word                                                   \n",
       "know         1.036299e+00   9.804708e-01  0.055828  0.055828\n",
       "think        9.288499e-02   5.772295e-02  0.035162  0.035162\n",
       "waiting      5.027934e-02   1.626138e-02  0.034018  0.034018\n",
       "thought      4.616358e-02   2.407196e-02  0.022092  0.022092\n",
       "guess        4.955076e-02   3.005991e-02  0.019491  0.019491\n",
       "never        5.250886e-02   3.737578e-02  0.015133  0.015133\n",
       "today        1.841231e-02   7.589045e-03  0.010823  0.010823\n",
       "feel         2.491553e-02   1.563683e-02  0.009279  0.009279\n",
       "met          6.527156e-02   7.406162e-02 -0.008790  0.008790\n",
       "want         2.445821e-02   1.658109e-02  0.007877  0.007877\n",
       "wanna        1.573250e-02   8.061414e-03  0.007671  0.007671\n",
       "got          4.143292e-02   3.389462e-02  0.007538  0.007538\n",
       "love         2.944180e-02   2.223908e-02  0.007203  0.007203\n",
       "'cause       1.385125e-01   1.455358e-01 -0.007023  0.007023\n",
       "time         4.243662e-01   4.313585e-01 -0.006992  0.006992\n",
       "need         1.494429e-02   8.703019e-03  0.006241  0.006241\n",
       "could        2.434518e-02   1.819474e-02  0.006150  0.006150\n",
       "really       2.041486e-02   1.448930e-02  0.005926  0.005926\n",
       "'d           1.320298e-02   7.361814e-03  0.005841  0.005841\n",
       "wish         1.271291e-02   7.099661e-03  0.005613  0.005613\n",
       "looking      1.892449e-02   1.380801e-02  0.005116  0.005116\n",
       "long         6.534804e-02   7.027301e-02 -0.004925  0.004925\n",
       "used         1.299919e-02   8.284581e-03  0.004715  0.004715\n",
       "hope         1.230448e-02   7.957027e-03  0.004347  0.004347\n",
       "every        2.526845e-02   2.959710e-02 -0.004329  0.004329\n",
       "sleeping     1.105208e-02   1.534573e-02 -0.004294  0.004294\n",
       "nothing      1.829629e-02   2.245234e-02 -0.004156  0.004156\n",
       "baby         6.936895e-02   6.522052e-02  0.004148  0.004148\n",
       "cause        8.305325e-02   8.706293e-02 -0.004010  0.004010\n",
       "since        3.065851e-01   3.027332e-01  0.003852  0.003852\n",
       "...                   ...            ...       ...       ...\n",
       "thy          2.559936e-06            NaN       NaN       NaN\n",
       "top          3.331970e-06            NaN       NaN       NaN\n",
       "toward                NaN   2.564892e-06       NaN       NaN\n",
       "tragic                NaN   4.474153e-06       NaN       NaN\n",
       "trees                 NaN   1.953106e-06       NaN       NaN\n",
       "troubled              NaN   1.416413e-05       NaN       NaN\n",
       "tumble       8.996105e-06            NaN       NaN       NaN\n",
       "twisting              NaN   1.418839e-05       NaN       NaN\n",
       "una                   NaN   1.209855e-09       NaN       NaN\n",
       "undercover   1.137236e-06            NaN       NaN       NaN\n",
       "uptight      1.288822e-06            NaN       NaN       NaN\n",
       "ur           1.657762e-06            NaN       NaN       NaN\n",
       "vine                  NaN   3.385004e-06       NaN       NaN\n",
       "visions      1.680427e-06            NaN       NaN       NaN\n",
       "voice        3.116573e-06            NaN       NaN       NaN\n",
       "weakness     2.230290e-06            NaN       NaN       NaN\n",
       "wins                  NaN   2.633342e-06       NaN       NaN\n",
       "wisdom       3.750825e-06            NaN       NaN       NaN\n",
       "wood         3.057208e-06            NaN       NaN       NaN\n",
       "worlds       2.031700e-06            NaN       NaN       NaN\n",
       "x3                    NaN   1.848108e-06       NaN       NaN\n",
       "yearning     2.527472e-05            NaN       NaN       NaN\n",
       "yell         3.117587e-06            NaN       NaN       NaN\n",
       "yonder       1.213045e-05            NaN       NaN       NaN\n",
       "zero         1.629570e-06            NaN       NaN       NaN\n",
       "zu           9.740342e-09            NaN       NaN       NaN\n",
       "}                     NaN   1.471060e-06       NaN       NaN\n",
       "´ll          2.469968e-06            NaN       NaN       NaN\n",
       "‘em          2.512883e-06            NaN       NaN       NaN\n",
       "”                     NaN   2.557401e-06       NaN       NaN\n",
       "\n",
       "[3081 rows x 4 columns]"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined = pd.merge(zz1_words, zz2_words, how='outer', left_index=True, right_index=True)\n",
    "df_combined['diff'] = df_combined['probability_x'] - df_combined['probability_y']\n",
    "df_combined['diff_abs'] = np.abs(df_combined['diff'])\n",
    "df_combined.sort_values(by='diff_abs', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.log(df['probability']).hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:29<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "final_scores = generate_text(learn, GPU=GPU,\n",
    "                             seed_text='xbos xbol-1 [verse-1] xeol xbol-2',\n",
    "                             audio=zz2,\n",
    "                             max_len=50, context_length=200,\n",
    "                             beam_width=3, verbose=False,\n",
    "                             temp=1.4, multinomial=True, graph=False, get_probs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xbos xbol-1 [verse-1] xeol \n",
      " xbol-2 here she comes now xeol \n",
      " xbol-3 here she comes now xeol \n",
      " xbol-4 she cries no more xeol \n",
      " xbol-5 here she comes now xeol \n",
      " xbol-6 here she comes xeol \n",
      " xbol-7 xeol \n",
      " xbol-8 [verse-2] xeol \n",
      " xbol-9 here she comes , here she comes xeol \n",
      " xbol-10 here she comes , here she comes 57.72286504894128\n"
     ]
    }
   ],
   "source": [
    "#print all of the final options of songs\n",
    "song, score = final_scores[0]\n",
    "print_words(song)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
