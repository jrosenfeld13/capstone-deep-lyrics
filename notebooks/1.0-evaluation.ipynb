{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.translate import bleu_score \n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import copy\n",
    "import pronouncing\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "#from src.nlp.evaluate_lyrics import *\n",
    "from src.nlp.evaluation_methods import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\justi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\justi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some basic preprocessing techniques\n",
    "def prepString(s):\n",
    "    '''removes punctuation other than apostrophes from string'''\n",
    "    return str(s).lower().translate({ord(c): None for c in string.punctuation if c not in (\"'\")})\n",
    "\n",
    "def removePunc(s):\n",
    "    '''removes punctuation from string'''\n",
    "    return str(s).lower().translate({ord(c): None for c in string.punctuation})\n",
    "\n",
    "def removeMarkupWords(s):\n",
    "    '''removes positional words generated in lyrics'''\n",
    "    s = str(s).lower()\n",
    "    for term in ['xeol','xbol','xeos','xbos','[verse-1]','[verse-2]','[chorus]']:\n",
    "        s = str(s).replace(term,'')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram perplexity (v0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(file_name):\n",
    "    '''input file name (without extention)'''\n",
    "    # load model\n",
    "    m1_pkl = open(\"../data/models/\" + file_name + \".pkl\", \"rb\")\n",
    "    model = pickle.load(m1_pkl)\n",
    "    m1_pkl.close()\n",
    "    return model\n",
    "    \n",
    "    \n",
    "def get_entropy(text, model, _n=3, _lpad = ['<s>'], _rpad = ['<s>']):\n",
    "    ''' calculate average log probability of each word in text, given context\n",
    "        \n",
    "        IMPORTANT NOTE:  For initial implementation, we do not have bigram or unigram prob in our dict 'model',\n",
    "                         and this handles missing or unknown entries naively\n",
    "    '''\n",
    "    \n",
    "    e = 0.0\n",
    "    padded_string = \"<s> \" + example1 + \" <s>\"\n",
    "    text = padded_string.split(' ')\n",
    "    for i in range(_n - 1, len(text)):\n",
    "        context = tuple(text[i - _n + 1:i])\n",
    "        token = text[i]\n",
    "        #print(context,token)\n",
    "        #print(e)\n",
    "        e += -np.log2(model.get(context,dict()).get(token,0.0000001))  # this is a poor placeholder until we get backoff dicts\n",
    "    entropy = e / float(len(text) - (_n - 1))\n",
    "    return entropy\n",
    "\n",
    "def get_perplexity(text, model, _n = 3, _lpad = ['<s>'], _rpad = ['<s>']):\n",
    "    return np.power(2,get_entropy(text=text, model=model , _n=_n, _lpad=_lpad, _rpad=_rpad))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3258828.7202875423"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example1 = \"Mary had a little lamb whose fleece was white as snow, and everywhere that Mary went the sheep was sure to go\"\n",
    "\n",
    "model1 = load_model(\"trigram-weights\")\n",
    "\n",
    "get_perplexity(example1,model1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prosodic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Well, this was a BLACKHOLE OF WASTED TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install prosodic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import prosodic as p\n",
    "#text = p.Text(\"Shall I compare thee to a summer's day?\")\n",
    "#text.parse()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging and aggregation to % of text\n",
    "def nltkPOS(text,verbose=False):\n",
    "    '''For an input text, return absolute difference from published proportions, between 0 and 1.'''\n",
    "    \n",
    "    # define lookups\n",
    "    mapping = {'CC':'CC','DT':'DT','PDT':'DT','WDT':'DT','IN':'IN','JJ':'JJ','JJR':'JJ','JJS':'JJ'\n",
    "               ,'NN':'NN','NNS':'NN','NNP':'NN','NNPS':'NN','LS':'OT','CD':'OT','EX':'OT','FW':'OT'\n",
    "               ,'POS':'OT','UH':'OT','RB':'RB','RBR':'RB','RBS':'RB','WRB':'RB','TO':'TO','MD':'VB'\n",
    "               ,'RP':'VB','VB':'VB','VBD':'VB','VBG':'VB','VBN':'VB','VBP':'VB','VBZ':'VB','PRP':'WP'\n",
    "               ,'PRP$':'WP','WP':'WP','WP$':'WP'}\n",
    "    comp_dict = {'CC':0.0212,'DT':0.0982,'IN':0.0998,'JJ':0.0613,'NN':0.3051,'RB':0.0766,'TO':0.0351\n",
    "                 ,'VB':0.285,'WP':0.0058,'OT':0.012}\n",
    "\n",
    "    # initialize\n",
    "    pos_cnt = Counter()\n",
    "    total_word_cnt = 0\n",
    "    pos_dict = defaultdict(float) \n",
    "    pos_dict['adjustment'] = 0\n",
    "    absdiff = 0\n",
    "    \n",
    "    # prepare data  \n",
    "    text = prepString(removeMarkupWords(text))\n",
    "    tokenized_text = nltk.word_tokenize(text)\n",
    "    tag_list = nltk.pos_tag(tokenized_text)\n",
    "    print(tokenized_text)\n",
    "    \n",
    "    if not tag_list:\n",
    "        raise ValueError(\"Please provide more complete text\")\n",
    "    \n",
    "    # initial proportions\n",
    "    for t in tag_list:\n",
    "        pos_cnt[t[1]] +=1\n",
    "        total_word_cnt +=1\n",
    "    pos_raw_dict = {k: v/float(total_word_cnt) for k,v in dict(pos_cnt).items()}\n",
    "        \n",
    "    # adjust for items missing in mapping (mostly punctuation)    \n",
    "    for k,v in pos_raw_dict.items():\n",
    "        if k in mapping:\n",
    "            pos_dict[mapping[k]] += v \n",
    "        else:\n",
    "            pos_dict['adjustment'] += v\n",
    "    for k,v in pos_dict.items():\n",
    "        pos_dict[k] = pos_dict[k]/(1-pos_dict['adjustment'])\n",
    "    del pos_dict['adjustment']\n",
    "    \n",
    "    # compare to observed ratios, calculate absolute difference\n",
    "    for k in comp_dict.keys():\n",
    "        absdiff += abs(comp_dict[k] - pos_dict.get(k,0))\n",
    "        if verbose==True: \n",
    "            print(k,\"- benchmark:\",comp_dict[k],\", text:\",pos_dict.get(k,0),\"abs diff:\",abs(comp_dict[k] - pos_dict.get(k,0)))\n",
    "    return absdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5789"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I jump into a lake and keep swimming.\" #  The fluffy dog went to the north and first left.\"\n",
    "nltkPOS(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2898853658536585"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long = \"Deep throat is a Python program that can synthesize speech. A simple approach to unrestricted text-to-speech translation uses a small set of letter-to-sound rules, each rule specifying a pronunciation for one or more letters in some context. Deep throat features a small set of letter-to-sound rules that translate English text to phonemes producing usably accurate pronunciations of words. Deep throat can produce sounds by combining stored representations of phoneme sounds in accordance with generated phoneme translations. It can output these sounds to computer sound hardware using PortAudio and it can save them to sound file. \\\n",
    "Deep throat can accept text as a command line option argument, from a pipe and it can be set into an interactive mode.\\\n",
    "Deep throat can be set to read the date and time in various ways, such as in a loop. It can translate text to phonemes, it can translate specified phonemes to sounds and it can translate numbers to English text. It can engage visual and sound analyses.\"\n",
    "nltkPOS(long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rhyme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "phoneText = \"as I walk through the valley of the garden of death\"\n",
    "newText = \"I take a look at my life and realize there's nothing left\"\n",
    "hickoryText = \"Hickory Dickory Dock,\\nThe mouse ran up the clock.\\nThe clock struck one,\\nThe mouse ran down!\\nHickory Dickory Dock.\"\n",
    "# convert to phonemes\n",
    "#phones = [pronouncing.phones_for_word(word) for word in prepString(phoneText).split()]\n",
    "#phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleWord = 'orange'\n",
    "examplePhones = pronouncing.phones_for_word(exampleWord)[0]\n",
    "exampleRhymePart = pronouncing.rhyming_part(examplePhones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AO1', 'R', 'AH0', 'N', 'JH']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if len(exampleRhymePart.split()) > 1:\n",
    "    rp2 = exampleRhymePart.split()\n",
    "else: \n",
    "    rp2 = exampleRhymePart\n",
    "rp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AO1']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exampleRhymePart.split()[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate rhyme density.  Hopeful enhancements include:\n",
    "# 1) extending rhymeType\n",
    "# 2) adding text-to-phoneme and applying for tokens not in CMU dictionary\n",
    "# 3) improving the calculation by taking into consideration probability of rhymes\n",
    "# 4) removing repeat tokens from consideration to avoid rewarding repeated words\n",
    "\n",
    "def calcRhymeDensity(text,rhymeType='perfect',rhymeLocation='all',lineStartStop=(1,-2),printExamples=False):\n",
    "    '''calculates rhyme density (count of rhymes over n-1 words). \\n\\n\n",
    "    \n",
    "       _parameters_\n",
    "       text: input text for measurement\n",
    "       rhymeType: 'perfect' is a perfect rhyme, 'vowel' is a rhyming in the vowel sound + stress only\n",
    "       rhymeLocation: choose to look at 'all' text, 'section' by line numbers, or 'end' (last word in each line)    \n",
    "       lineStartStop: tuple of (start,stop) line numbers\n",
    "       printExamples: if True, print most common values of the selected rhymeType\n",
    "       \n",
    "       _returns_\n",
    "       rhyme_cnt: count of rhymes of specified rhymeType and rhymeLocation\n",
    "       wordCount: count of words of specified rhymeType and rhymeLocation\n",
    "       rhymeDensity: rhyme_cnt/float(wordCount-1)\n",
    "    '''\n",
    "    # restrict location to (end=last word, internal line = line, all= full text)\n",
    "    # count tokens\n",
    "    # \n",
    "    \n",
    "    # initialize\n",
    "    rhymePart_cnt = Counter()\n",
    "    rhyme_cnt = 0\n",
    "    \n",
    "    # prepare data\n",
    "    text = prepString(removeMarkupWords(text))\n",
    "    \n",
    "    if rhymeLocation == 'all':\n",
    "        words = text.split()\n",
    "    \n",
    "    if rhymeLocation == 'end':\n",
    "        lines = text.split(\"\\n\")\n",
    "        words = [line.split()[-1] for line in lines if len(line.split())>0]\n",
    "    \n",
    "    if rhymeLocation == 'section':\n",
    "        lines = text.split(\"\\n\")\n",
    "        words = [line.split()[-1] for line in lines[lineStartStop[0]:lineStartStop[1]+1] if len(line.split())>0]\n",
    "    \n",
    "    # \n",
    "    wordCount = len(words)\n",
    "    print(wordCount,words)\n",
    "    for word in words:\n",
    "        pros = pronouncing.phones_for_word(word)\n",
    "        if pros:     \n",
    "            phonelist = pros[0]  #using first pronunciation for now\n",
    "            if len(phonelist) > 0:\n",
    "                if rhymeType == 'perfect':\n",
    "                    rhymePart_cnt[pronouncing.rhyming_part(phonelist)] +=1\n",
    "\n",
    "                #if rhymeType == 'rime':\n",
    "                #    pass\n",
    "                #if rhymeType == 'soft':\n",
    "                #    pass\n",
    "                #if rhymeType == 'consonant':\n",
    "                #    pass\n",
    "\n",
    "                elif rhymeType == 'vowel':\n",
    "                    rhymePart_cnt[pronouncing.rhyming_part(phonelist).split()[0]] +=1\n",
    "    \n",
    "    for v in rhymePart_cnt.values():\n",
    "        rhyme_cnt += v-1\n",
    "    \n",
    "    if wordCount>1: \n",
    "        rhymeDensity = rhyme_cnt/float(wordCount-1)\n",
    "    else:\n",
    "        rhymeDensity = 0.0\n",
    "    \n",
    "    if printExamples == True:\n",
    "        print(rhymePart_cnt.most_common(5))\n",
    "    \n",
    "    return rhymeDensity, rhyme_cnt, wordCount\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AY1', 2), ('EY1 K', 1), ('AH0', 1), ('UH1 K', 1), ('AE1 T', 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.09090909090909091, 1, 12)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcRhymeDensity(newText,rhymeType='perfect',printExamples=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('OW1', 2), ('UW1', 2), ('AY1', 1), ('IH1 S', 1)]\n",
      "(0.4, 2, 6)\n"
     ]
    }
   ],
   "source": [
    "with open('../data/lyrics/current/samp1.txt') as sampf:\n",
    "    samp = sampf.read()\n",
    "    text = samp\n",
    "    text = removeMarkupWords(text)\n",
    "    text = prepString(text)\n",
    "    #print(text)\n",
    "    lines = text.split(\"\\n\")\n",
    "    #print(lines)\n",
    "    words = [line.split()[-1] for line in lines if len(line.split())>0]\n",
    "    #print(words)\n",
    "    print(calcRhymeDensity(text,rhymeType='perfect',rhymeLocation='end',printExamples=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.36363636363636365, 4, 12)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcRhymeDensity(newText,rhymeType='vowel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['orange', 'orange-green', 'orangeburg', 'oranges']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pronouncing.search(exampleRhymePart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 2, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcRhymeDensity(hickoryText,rhymeType='vowel',rhymeLocation='end')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(ref_list,candidateText,nGram=4,nGramType='cumulative',shouldSmooth=True):\n",
    "    '''calculates BLEU score \n",
    "    \n",
    "        _parameters_\n",
    "        ref_list: expects a list of reference texts to compare (as strings)\n",
    "        candidateText: the new text needing to be scored\n",
    "        nGram: choose between 1-4.  Determines which ngram(s) to use in the scoring\n",
    "        nGramType: 'cumulative' uses a simple average of all ngrams from 1 to nGram\n",
    "        shouldSmooth: if False, calculates the BLEU score without smoothing. Recommended to use smoothing (set to True)\n",
    "        \n",
    "        _returns_\n",
    "        score: BLEU score using nGram settings input, smoothed by default (can be turned off)\n",
    "    '''\n",
    "    \n",
    "    # basic checks\n",
    "    if nGram not in [1,2,3,4]:\n",
    "        raise ValueError('nGram must be between 1 and 4')\n",
    "    \n",
    "    if nGramType not in ['cumulative','exclusive']:\n",
    "        raise ValueError('nGramType must either be cumulative (average of nGrams less than n) or exclusive (1=unigram, etc.)')\n",
    "    \n",
    "    # pre-score\n",
    "    weight_dict = {('cumulative',1):(1,0,0,0)\n",
    "                  ,('cumulative',2):(.5,.5,0,0)\n",
    "                  ,('cumulative',3):(.3333,.3333,.3333,0)\n",
    "                  ,('cumulative',4):(.25,.25,.25,.25)\n",
    "                  ,('exclusive',1):(1,0,0,0)\n",
    "                  ,('exclusive',2):(0,1,0,0)\n",
    "                  ,('exclusive',3):(0,0,1,0)\n",
    "                  ,('exclusive',4):(0,0,0,1)}\n",
    "    candidate = [removePunc(str(removeMarkupWords(candidateText))).split()]\n",
    "    references = [[removePunc(str(removeMarkupWords(ref))).split() for ref in ref_list]]\n",
    "    weights = weight_dict[(nGramType,nGram)]\n",
    "       \n",
    "    #print(candidate)\n",
    "    # scoring\n",
    "    if shouldSmooth==True:\n",
    "        smoother = bleu_score.SmoothingFunction().method5\n",
    "    else:\n",
    "        smoother = None\n",
    "    score = bleu_score.corpus_bleu(references, candidate, weights, smoothing_function=smoother)\n",
    "    #print(score)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0692255179440046"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu([newText],phoneText,4,'cumulative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0769800358919501"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu([phoneText],hickoryText,4,'cumulative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findLineStress(line):\n",
    "    '''find accentual stress of a given line, based on CMU dict.  Still a bit unclever.\n",
    "    \n",
    "    _parameters_\n",
    "    line: line of text\n",
    "    \n",
    "    _returns_\n",
    "    parselist: list of potential stresses after parsing. 0 is unstressed, 1 is primary stress, 2 is secondary stress (middle)\n",
    "    syllableLengths: list of syllable lengths corresponding to the parses in parselist\n",
    "    wordCount: count of words in the line \n",
    "    '''\n",
    "    line = prepString(removeMarkupWords(line))\n",
    "    words = line.split()\n",
    "    wordCount = len(words)\n",
    "    parses = ['']\n",
    "    for word in words:\n",
    "        pros = pronouncing.phones_for_word(word)\n",
    "        if pros:\n",
    "            for phonelist in [pronouncing.phones_for_word(word)]:           \n",
    "                stressOptions = copy.deepcopy(parses)\n",
    "                currLen = len(parses)\n",
    "                newparse = []\n",
    "                # I don't really need to loop through pronunciations, just distinct stress patterns, so a little inefficient here\n",
    "                for pronunciation in phonelist:\n",
    "                    wordStress = pronouncing.stresses(pronunciation)\n",
    "                    for option in range(currLen):\n",
    "                        newparse.append(''+str(stressOptions[option]) + str(wordStress))\n",
    "            parses = newparse \n",
    "\n",
    "    return list(set(parses)), [len(parse) for parse in list(set(parses))], wordCount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['1111010101011', '1111110111011', '1111110101011', '1111010111011'],\n",
       " [13, 13, 13, 13],\n",
       " 11)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findLineStress(phoneText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['110111111021101', '110111101021101', '111111101021101', '111111111021101'],\n",
       " [15, 15, 15, 15],\n",
       " 12)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findLineStress(newText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(s1, s2):\n",
    "    '''calculate levenshtein distance for two input strings\n",
    "    \n",
    "    _parameters_\n",
    "    s1: first input string\n",
    "    s2: second input string\n",
    "    \n",
    "    _returns_\n",
    "    distance: levenshtein distance between two strings...that is, the lowest number of modifications to turn s1 into s2\n",
    "    '''\n",
    "    s1 = str(s1)\n",
    "    s2 = str(s2)\n",
    "    \n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein(s2, s1)\n",
    "\n",
    "    # otherwise len(s1) >= len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer\n",
    "            deletions = current_row[j] + 1       # than s2\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]\n",
    "    \n",
    "def findMeter(text):\n",
    "    '''finds meter with smallest edit distance\n",
    "    \n",
    "    _parameters_\n",
    "    text: input text, usually a poem of some kind\n",
    "    \n",
    "    _returns_\n",
    "    lowest: lowest edit distance for any standard accentual-syllabic verse\n",
    "    options: list of potential meters for the lowest edit distance.\n",
    "    '''\n",
    "    # define\n",
    "    meter_dict = {'0101':'Iambic dimeter'\n",
    "                  ,'010101':'Iambic trimeter'\n",
    "                  ,'01010101':'Iambic tetrameter'\n",
    "                  ,'0101010101':'Iambic pentameter'\n",
    "                  ,'010101010101':'Iambic hexameter'\n",
    "                  ,'01010101010101':'Iambic heptameter'\n",
    "                  ,'0101010101010101':'Iambic octameter'\n",
    "                  ,'1010':'Trochaic dimeter'\n",
    "                  ,'101010':'Trochaic trimeter'\n",
    "                  ,'10101010':'Trochaic tetrameter'\n",
    "                  ,'1010101010':'Trochaic pentameter'\n",
    "                  ,'101010101010':'Trochaic hexameter'\n",
    "                  ,'10101010101010':'Trochaic heptameter'\n",
    "                  ,'1010101010101010':'Trochaic octameter'\n",
    "                  ,'001001':'Anapestic dimeter'\n",
    "                  ,'001001001':'Anapestic trimeter'\n",
    "                  ,'001001001001':'Anapestic tetrameter'\n",
    "                  ,'001001001001001':'Anapestic pentameter'\n",
    "                  ,'001001001001001001':'Anapestic hexameter'\n",
    "                  ,'001001001001001001001':'Anapestic heptameter'\n",
    "                  ,'100100':'Dactyllic dimeter'\n",
    "                  ,'100100100':'Dactyllic trimeter'\n",
    "                  ,'100100100100':'Dactyllic tetrameter'\n",
    "                  ,'100100100100100':'Dactyllic pentameter'\n",
    "                  ,'100100100100100100':'Dactyllic hexameter'\n",
    "                  ,'100100100100100100100':'Dactyllic heptameter'}\n",
    "\n",
    "    # initialize\n",
    "    vote_cnt = Counter()\n",
    "    text = prepString(removeMarkupWords(text))\n",
    "    lines = text.split('\\n')\n",
    "    line_cnt = len(lines)\n",
    "    minDist = 999\n",
    "    \n",
    "    # update distances\n",
    "    for line in lines:\n",
    "        for k,v in meter_dict.items():\n",
    "            minDist = 999\n",
    "            for reading in findLineStress(line)[0]:\n",
    "                dist = levenshtein(k,reading)\n",
    "                if dist < minDist:\n",
    "                    minDist = dist    \n",
    "            vote_cnt[v] += minDist\n",
    "    \n",
    "    #options = min(vote_cnt, key=vote_cnt.get)  #chooses one in the event of ties\n",
    "    lowest = min(vote_cnt.values()) \n",
    "    options = [k for k,v in vote_cnt.items() if v==lowest]\n",
    "    return lowest, options, line_cnt, lowest/float(line_cnt) #, vote_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as I walk through the valley of the garden of death \n",
      " ['1111010101011', '1111110111011', '1111110101011', '1111010111011'] \n",
      " (3, ['Iambic hexameter', 'Iambic heptameter', 'Trochaic hexameter', 'Trochaic heptameter'], 1, 3.0)\n"
     ]
    }
   ],
   "source": [
    "print(phoneText,'\\n',findLineStress(phoneText)[0],'\\n',findMeter(phoneText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hickory Dickory Dock,\n",
      "The mouse ran up the clock.\n",
      "The clock struck one,\n",
      "The mouse ran down!\n",
      "Hickory Dickory Dock.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6, ['Iambic dimeter'], 5, 1.2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(hickoryText)\n",
    "findMeter(hickoryText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But, soft! what light through yonder window breaks? \n",
      " It is the east, and Juliet is the sun \n",
      " (4, ['Iambic pentameter'], 2, 2.0)\n"
     ]
    }
   ],
   "source": [
    "romeoText = \"But, soft! what light through yonder window breaks? \\n It is the east, and Juliet is the sun\"\n",
    "print(romeoText,'\\n',findMeter(romeoText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputText1 = \"xbos xbol listen to my tale of woe , xeol \\n  xbol it ''s terribly sad but true , xeol \\n  xbol all dressed up , no place to go xeol \\n  xbol each evening i ''m awfully blue . xeol \\n  xbol xeol \\n xbol i must win some handsome guy xeol \\n xbol can ''t go on like this , xeol\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xbos xbol listen to my tale of woe , xeol  (['1011111', '1001111'], [7, 7], 6)\n",
      "  xbol it ''s terribly sad but true , xeol  (['0100111', '1100111'], [7, 7], 6)\n",
      "  xbol all dressed up , no place to go xeol  (['1111111', '1111101'], [7, 7], 7)\n",
      "  xbol each evening i ''m awfully blue . xeol  (['11011001', '1101101'], [8, 7], 6)\n",
      "  xbol xeol  ([''], [0], 0)\n",
      " xbol i must win some handsome guy xeol  (['1111101'], [7], 6)\n",
      " xbol can ''t go on like this , xeol (['11111', '01110', '11110', '01111'], [5, 5, 5, 5], 6)\n"
     ]
    }
   ],
   "source": [
    "for line in outputText1.split('\\n'):\n",
    "    print(line, findLineStress(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('UW1', 4), ('AY1', 4), ('OW1', 4), ('IH1 S AH0 N', 1), ('EY1 L', 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.25, 9, 37)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcRhymeDensity(outputText1,rhymeType='perfect',printExamples=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('IY1 P AH0 L', 2)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 1, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"people steeple\"\n",
    "b = \"poodle stroooudel\"\n",
    "calcRhymeDensity(a,rhymeType='perfect',printExamples=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scoreDirectory(source_dir='../data/lyrics/current/'\n",
    "                   ,destination_dir='../data/scores/'\n",
    "                   ,output_file_name='output.csv'\n",
    "                   ,reference_dir='../data/lyrics/reference/'):\n",
    "    \n",
    "    '''create .csv with scores from files in specified directory'''\n",
    "    \n",
    "    songs = glob.glob(source_dir+'/*.txt')  \n",
    "    refs = glob.glob(reference_dir+'/*.txt')\n",
    "    ref_list = []\n",
    "    \n",
    "    ## add BLEU reference code\n",
    "    for ref in refs:\n",
    "        with open(ref) as rf:\n",
    "            ref_raw_text = rf.read()\n",
    "            ref_list.append(ref_raw_text)\n",
    "    \n",
    "    with open(destination_dir+output_file_name, 'w') as outf:\n",
    "        cw = csv.writer(outf,quoting=csv.QUOTE_NONNUMERIC)    \n",
    "        cw.writerow(['filename'\n",
    "                    ,'POSConfirmity'\n",
    "                    ,'RD_PerfectAll'\n",
    "                    ,'RD_PerfectEnd'\n",
    "                    ,'RD_VowelAll'\n",
    "                    ,'RD_VowelEnd'\n",
    "                    ,'ClosestMeter'\n",
    "                    ,'AvgDistanceToMeter'\n",
    "                    ,'BLEU_1_excl_Unsmoothed'\n",
    "                    ,'BLEU_2_excl_Unsmoothed'\n",
    "                    ,'BLEU_3_excl_Unsmoothed'\n",
    "                    ,'BLEU_4_excl_Unsmoothed'\n",
    "                    ,'BLEU_3_cumul_Smoothed'\n",
    "                    ,'BLEU_4_cumul_Smoothed'\n",
    "                    ])\n",
    "\n",
    "        for song in songs: \n",
    "            with open(song) as f: \n",
    "                text = f.read()\n",
    "                cw.writerow([str(song.split('\\\\')[-1])\n",
    "                            ,round(nltkPOS(text),4)\n",
    "                            ,round(calcRhymeDensity(text,rhymeType='perfect',rhymeLocation='all')[0],4)\n",
    "                            ,round(calcRhymeDensity(text,rhymeType='perfect',rhymeLocation='end',printExamples=False)[0],4)\n",
    "                            ,round(calcRhymeDensity(text,rhymeType='vowel',rhymeLocation='all')[0],4)\n",
    "                            ,round(calcRhymeDensity(text,rhymeType='vowel',rhymeLocation='end')[0],4) \n",
    "                            ,findMeter(text)[1][0]\n",
    "                            ,round(findMeter(text)[3],4)\n",
    "                            ,round(bleu(ref_list,text,nGram=1,nGramType='exclusive',shouldSmooth=False),4)\n",
    "                            ,round(bleu(ref_list,text,nGram=2,nGramType='exclusive',shouldSmooth=False),4)\n",
    "                            ,round(bleu(ref_list,text,nGram=3,nGramType='exclusive',shouldSmooth=False),4)\n",
    "                            ,round(bleu(ref_list,text,nGram=4,nGramType='exclusive',shouldSmooth=False),4)\n",
    "                            ,round(bleu(ref_list,text,nGram=3,nGramType='cumulative',shouldSmooth=True),4)\n",
    "                            ,round(bleu(ref_list,text,nGram=4,nGramType='cumulative',shouldSmooth=True),4)\n",
    "                            ]\n",
    "                           )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['listen', 'to', 'my', 'tale', 'of', 'woe', 'it', 's', 'terribly', 'sad', 'but', 'true', 'all', 'dressed', 'up', 'no', 'place', 'to', 'go', 'each', 'evening', 'i', 'm', 'awfully', 'blue', 'i', 'must', 'win', 'some', 'handsome', 'guy', 'can', 't', 'go', 'on', 'like', 'this']]\n",
      "[['listen', 'to', 'my', 'tale', 'of', 'woe', 'it', 's', 'terribly', 'sad', 'but', 'true', 'all', 'dressed', 'up', 'no', 'place', 'to', 'go', 'each', 'evening', 'i', 'm', 'awfully', 'blue', 'i', 'must', 'win', 'some', 'handsome', 'guy', 'can', 't', 'go', 'on', 'like', 'this']]\n",
      "[['listen', 'to', 'my', 'tale', 'of', 'woe', 'it', 's', 'terribly', 'sad', 'but', 'true', 'all', 'dressed', 'up', 'no', 'place', 'to', 'go', 'each', 'evening', 'i', 'm', 'awfully', 'blue', 'i', 'must', 'win', 'some', 'handsome', 'guy', 'can', 't', 'go', 'on', 'like', 'this']]\n",
      "[['listen', 'to', 'my', 'tale', 'of', 'woe', 'it', 's', 'terribly', 'sad', 'but', 'true', 'all', 'dressed', 'up', 'no', 'place', 'to', 'go', 'each', 'evening', 'i', 'm', 'awfully', 'blue', 'i', 'must', 'win', 'some', 'handsome', 'guy', 'can', 't', 'go', 'on', 'like', 'this']]\n",
      "[['listen', 'to', 'my', 'tale', 'of', 'woe', 'it', 's', 'terribly', 'sad', 'but', 'true', 'all', 'dressed', 'up', 'no', 'place', 'to', 'go', 'each', 'evening', 'i', 'm', 'awfully', 'blue', 'i', 'must', 'win', 'some', 'handsome', 'guy', 'can', 't', 'go', 'on', 'like', 'this']]\n",
      "[['listen', 'to', 'my', 'tale', 'of', 'woe', 'it', 's', 'terribly', 'sad', 'but', 'true', 'all', 'dressed', 'up', 'no', 'place', 'to', 'go', 'each', 'evening', 'i', 'm', 'awfully', 'blue', 'i', 'must', 'win', 'some', 'handsome', 'guy', 'can', 't', 'go', 'on', 'like', 'this']]\n",
      "[['whether', 'you', 'are', 'here', 'or', 'yonder', 'whether', 'you', 'are', 'false', 'or', 'true', 'whether', 'you', 'remain', 'or', 'wander', 'i', 'm', 'growing', 'senses', 'of']]\n",
      "[['whether', 'you', 'are', 'here', 'or', 'yonder', 'whether', 'you', 'are', 'false', 'or', 'true', 'whether', 'you', 'remain', 'or', 'wander', 'i', 'm', 'growing', 'senses', 'of']]\n",
      "[['whether', 'you', 'are', 'here', 'or', 'yonder', 'whether', 'you', 'are', 'false', 'or', 'true', 'whether', 'you', 'remain', 'or', 'wander', 'i', 'm', 'growing', 'senses', 'of']]\n",
      "[['whether', 'you', 'are', 'here', 'or', 'yonder', 'whether', 'you', 'are', 'false', 'or', 'true', 'whether', 'you', 'remain', 'or', 'wander', 'i', 'm', 'growing', 'senses', 'of']]\n",
      "[['whether', 'you', 'are', 'here', 'or', 'yonder', 'whether', 'you', 'are', 'false', 'or', 'true', 'whether', 'you', 'remain', 'or', 'wander', 'i', 'm', 'growing', 'senses', 'of']]\n",
      "[['whether', 'you', 'are', 'here', 'or', 'yonder', 'whether', 'you', 'are', 'false', 'or', 'true', 'whether', 'you', 'remain', 'or', 'wander', 'i', 'm', 'growing', 'senses', 'of']]\n",
      "[['1', 'knowin', 'that', 'you', 're', 'leavin', '2', 'i', 'can', 't', 'believe', 'it', 's', 'happening', 'to', 'me', '3', 'knowing', 'that', 'you', 're', 'leavin', 'me', 'alone', '4', 'i', 'can', 't', 'believe', 'that', 'you', 're', 'in', 'love', 'with', 'me', '5', '6', 'breakin', 'up', 'is', 'hard', 'to', 'do', '7', 'now', 'i', 'know', 'know', 'that', 'i', 've', 'been', 'true', '8', 'breakin', 'up', 'is', 'hard', 'to', 'do', '9', 'now', 'i', 'know', 'know', 'that', 'i', 've', 'been', 'true', '10', '11', 'missin', 'you', '12', 'seems', 'the', 'way', 'i', 'do', '13', 'that', 's', 'what', 'i', 'say', '14', '15', 'breakin', 'up', 'is', 'hard', 'to', 'do', '16', 'now', 'i', 'know', 'i', 'need', 'you', 'by', 'my', 'side', '17', 'how', 'can', 'i', 'prove', 'that', 'i', 'love', 'you']]\n",
      "[['1', 'knowin', 'that', 'you', 're', 'leavin', '2', 'i', 'can', 't', 'believe', 'it', 's', 'happening', 'to', 'me', '3', 'knowing', 'that', 'you', 're', 'leavin', 'me', 'alone', '4', 'i', 'can', 't', 'believe', 'that', 'you', 're', 'in', 'love', 'with', 'me', '5', '6', 'breakin', 'up', 'is', 'hard', 'to', 'do', '7', 'now', 'i', 'know', 'know', 'that', 'i', 've', 'been', 'true', '8', 'breakin', 'up', 'is', 'hard', 'to', 'do', '9', 'now', 'i', 'know', 'know', 'that', 'i', 've', 'been', 'true', '10', '11', 'missin', 'you', '12', 'seems', 'the', 'way', 'i', 'do', '13', 'that', 's', 'what', 'i', 'say', '14', '15', 'breakin', 'up', 'is', 'hard', 'to', 'do', '16', 'now', 'i', 'know', 'i', 'need', 'you', 'by', 'my', 'side', '17', 'how', 'can', 'i', 'prove', 'that', 'i', 'love', 'you']]\n",
      "[['1', 'knowin', 'that', 'you', 're', 'leavin', '2', 'i', 'can', 't', 'believe', 'it', 's', 'happening', 'to', 'me', '3', 'knowing', 'that', 'you', 're', 'leavin', 'me', 'alone', '4', 'i', 'can', 't', 'believe', 'that', 'you', 're', 'in', 'love', 'with', 'me', '5', '6', 'breakin', 'up', 'is', 'hard', 'to', 'do', '7', 'now', 'i', 'know', 'know', 'that', 'i', 've', 'been', 'true', '8', 'breakin', 'up', 'is', 'hard', 'to', 'do', '9', 'now', 'i', 'know', 'know', 'that', 'i', 've', 'been', 'true', '10', '11', 'missin', 'you', '12', 'seems', 'the', 'way', 'i', 'do', '13', 'that', 's', 'what', 'i', 'say', '14', '15', 'breakin', 'up', 'is', 'hard', 'to', 'do', '16', 'now', 'i', 'know', 'i', 'need', 'you', 'by', 'my', 'side', '17', 'how', 'can', 'i', 'prove', 'that', 'i', 'love', 'you']]\n",
      "[['1', 'knowin', 'that', 'you', 're', 'leavin', '2', 'i', 'can', 't', 'believe', 'it', 's', 'happening', 'to', 'me', '3', 'knowing', 'that', 'you', 're', 'leavin', 'me', 'alone', '4', 'i', 'can', 't', 'believe', 'that', 'you', 're', 'in', 'love', 'with', 'me', '5', '6', 'breakin', 'up', 'is', 'hard', 'to', 'do', '7', 'now', 'i', 'know', 'know', 'that', 'i', 've', 'been', 'true', '8', 'breakin', 'up', 'is', 'hard', 'to', 'do', '9', 'now', 'i', 'know', 'know', 'that', 'i', 've', 'been', 'true', '10', '11', 'missin', 'you', '12', 'seems', 'the', 'way', 'i', 'do', '13', 'that', 's', 'what', 'i', 'say', '14', '15', 'breakin', 'up', 'is', 'hard', 'to', 'do', '16', 'now', 'i', 'know', 'i', 'need', 'you', 'by', 'my', 'side', '17', 'how', 'can', 'i', 'prove', 'that', 'i', 'love', 'you']]\n",
      "[['1', 'knowin', 'that', 'you', 're', 'leavin', '2', 'i', 'can', 't', 'believe', 'it', 's', 'happening', 'to', 'me', '3', 'knowing', 'that', 'you', 're', 'leavin', 'me', 'alone', '4', 'i', 'can', 't', 'believe', 'that', 'you', 're', 'in', 'love', 'with', 'me', '5', '6', 'breakin', 'up', 'is', 'hard', 'to', 'do', '7', 'now', 'i', 'know', 'know', 'that', 'i', 've', 'been', 'true', '8', 'breakin', 'up', 'is', 'hard', 'to', 'do', '9', 'now', 'i', 'know', 'know', 'that', 'i', 've', 'been', 'true', '10', '11', 'missin', 'you', '12', 'seems', 'the', 'way', 'i', 'do', '13', 'that', 's', 'what', 'i', 'say', '14', '15', 'breakin', 'up', 'is', 'hard', 'to', 'do', '16', 'now', 'i', 'know', 'i', 'need', 'you', 'by', 'my', 'side', '17', 'how', 'can', 'i', 'prove', 'that', 'i', 'love', 'you']]\n",
      "[['1', 'knowin', 'that', 'you', 're', 'leavin', '2', 'i', 'can', 't', 'believe', 'it', 's', 'happening', 'to', 'me', '3', 'knowing', 'that', 'you', 're', 'leavin', 'me', 'alone', '4', 'i', 'can', 't', 'believe', 'that', 'you', 're', 'in', 'love', 'with', 'me', '5', '6', 'breakin', 'up', 'is', 'hard', 'to', 'do', '7', 'now', 'i', 'know', 'know', 'that', 'i', 've', 'been', 'true', '8', 'breakin', 'up', 'is', 'hard', 'to', 'do', '9', 'now', 'i', 'know', 'know', 'that', 'i', 've', 'been', 'true', '10', '11', 'missin', 'you', '12', 'seems', 'the', 'way', 'i', 'do', '13', 'that', 's', 'what', 'i', 'say', '14', '15', 'breakin', 'up', 'is', 'hard', 'to', 'do', '16', 'now', 'i', 'know', 'i', 'need', 'you', 'by', 'my', 'side', '17', 'how', 'can', 'i', 'prove', 'that', 'i', 'love', 'you']]\n",
      "[['1', 'each', 'time', 'i', 'look', 'at', 'you', '2', 'is', 'like', 'the', 'first', 'time', '3', 'each', 'time', 'you', 're', 'near', 'me', '4', 'the', 'thrill', 'is', 'new', '5', 'and', 'there', 'is', 'nothing', '6', 'that', 'i', 'wouldn', 't', 'do', 'for', '7', 'the', 'rare', 'delight', 'of', 'the', 'sight', '8', 'of', 'you', 'for', '9', '10', 'the', 'more', 'i', 'see', 'you', '11', 'the', 'more', 'i', 'want', 'you', '12', 'somehow', 'this', 'feeling', '13', 'just', 'grows', 'and', 'grows', '14', 'with', 'every', 'sigh', '15', 'i', 'become', 'more', 'mad', 'about', 'you', '16', 'more', 'lost', 'without', 'you', 'and', 'so', 'it', 'goes', '17', '18', 'can', 'you', 'imagine', 'how', 'much', 'i', 'love', 'you']]\n",
      "[['1', 'each', 'time', 'i', 'look', 'at', 'you', '2', 'is', 'like', 'the', 'first', 'time', '3', 'each', 'time', 'you', 're', 'near', 'me', '4', 'the', 'thrill', 'is', 'new', '5', 'and', 'there', 'is', 'nothing', '6', 'that', 'i', 'wouldn', 't', 'do', 'for', '7', 'the', 'rare', 'delight', 'of', 'the', 'sight', '8', 'of', 'you', 'for', '9', '10', 'the', 'more', 'i', 'see', 'you', '11', 'the', 'more', 'i', 'want', 'you', '12', 'somehow', 'this', 'feeling', '13', 'just', 'grows', 'and', 'grows', '14', 'with', 'every', 'sigh', '15', 'i', 'become', 'more', 'mad', 'about', 'you', '16', 'more', 'lost', 'without', 'you', 'and', 'so', 'it', 'goes', '17', '18', 'can', 'you', 'imagine', 'how', 'much', 'i', 'love', 'you']]\n",
      "[['1', 'each', 'time', 'i', 'look', 'at', 'you', '2', 'is', 'like', 'the', 'first', 'time', '3', 'each', 'time', 'you', 're', 'near', 'me', '4', 'the', 'thrill', 'is', 'new', '5', 'and', 'there', 'is', 'nothing', '6', 'that', 'i', 'wouldn', 't', 'do', 'for', '7', 'the', 'rare', 'delight', 'of', 'the', 'sight', '8', 'of', 'you', 'for', '9', '10', 'the', 'more', 'i', 'see', 'you', '11', 'the', 'more', 'i', 'want', 'you', '12', 'somehow', 'this', 'feeling', '13', 'just', 'grows', 'and', 'grows', '14', 'with', 'every', 'sigh', '15', 'i', 'become', 'more', 'mad', 'about', 'you', '16', 'more', 'lost', 'without', 'you', 'and', 'so', 'it', 'goes', '17', '18', 'can', 'you', 'imagine', 'how', 'much', 'i', 'love', 'you']]\n",
      "[['1', 'each', 'time', 'i', 'look', 'at', 'you', '2', 'is', 'like', 'the', 'first', 'time', '3', 'each', 'time', 'you', 're', 'near', 'me', '4', 'the', 'thrill', 'is', 'new', '5', 'and', 'there', 'is', 'nothing', '6', 'that', 'i', 'wouldn', 't', 'do', 'for', '7', 'the', 'rare', 'delight', 'of', 'the', 'sight', '8', 'of', 'you', 'for', '9', '10', 'the', 'more', 'i', 'see', 'you', '11', 'the', 'more', 'i', 'want', 'you', '12', 'somehow', 'this', 'feeling', '13', 'just', 'grows', 'and', 'grows', '14', 'with', 'every', 'sigh', '15', 'i', 'become', 'more', 'mad', 'about', 'you', '16', 'more', 'lost', 'without', 'you', 'and', 'so', 'it', 'goes', '17', '18', 'can', 'you', 'imagine', 'how', 'much', 'i', 'love', 'you']]\n",
      "[['1', 'each', 'time', 'i', 'look', 'at', 'you', '2', 'is', 'like', 'the', 'first', 'time', '3', 'each', 'time', 'you', 're', 'near', 'me', '4', 'the', 'thrill', 'is', 'new', '5', 'and', 'there', 'is', 'nothing', '6', 'that', 'i', 'wouldn', 't', 'do', 'for', '7', 'the', 'rare', 'delight', 'of', 'the', 'sight', '8', 'of', 'you', 'for', '9', '10', 'the', 'more', 'i', 'see', 'you', '11', 'the', 'more', 'i', 'want', 'you', '12', 'somehow', 'this', 'feeling', '13', 'just', 'grows', 'and', 'grows', '14', 'with', 'every', 'sigh', '15', 'i', 'become', 'more', 'mad', 'about', 'you', '16', 'more', 'lost', 'without', 'you', 'and', 'so', 'it', 'goes', '17', '18', 'can', 'you', 'imagine', 'how', 'much', 'i', 'love', 'you']]\n",
      "[['1', 'each', 'time', 'i', 'look', 'at', 'you', '2', 'is', 'like', 'the', 'first', 'time', '3', 'each', 'time', 'you', 're', 'near', 'me', '4', 'the', 'thrill', 'is', 'new', '5', 'and', 'there', 'is', 'nothing', '6', 'that', 'i', 'wouldn', 't', 'do', 'for', '7', 'the', 'rare', 'delight', 'of', 'the', 'sight', '8', 'of', 'you', 'for', '9', '10', 'the', 'more', 'i', 'see', 'you', '11', 'the', 'more', 'i', 'want', 'you', '12', 'somehow', 'this', 'feeling', '13', 'just', 'grows', 'and', 'grows', '14', 'with', 'every', 'sigh', '15', 'i', 'become', 'more', 'mad', 'about', 'you', '16', 'more', 'lost', 'without', 'you', 'and', 'so', 'it', 'goes', '17', '18', 'can', 'you', 'imagine', 'how', 'much', 'i', 'love', 'you']]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1', 'through', 'night', 'and', 'day', 'i', 'hear', 'it', 'say', '2', 'i', 'm', 'always', 'chasing', 'rainbows', '3', 'watching', 'clouds', 'drifting', 'by', '4', 'my', 'schemes', 'are', 'just', 'like', 'all', 'my', 'dreams', '5', 'ending', 'in', 'the', 'sky', '6', '7', 'some', 'fellows', 'look', 'and', 'find', 'the', 'sunshine', '8', 'i', 'always', 'look', 'and', 'find', 'the', 'rain', '9', 'some', 'fellows', 'make', 'a', 'winning', 'sometimes', '10', 'i', 'never', 'even', 'make', 'a', 'gain', '11', 'believe', 'me', 'i', 'm', 'always', 'chasing', 'rainbows', '12', 'waiting', 'to', 'find', 'a', 'little', 'bluebird', 'in', 'vain', '13', '14', 'i', 'm', 'always', 'chasing', 'rainbows', '15', 'watching', 'clouds', 'drifting', 'by', '16', 'my', 'schemes', 'are', 'just', 'like', 'all', 'my', 'dreams']]\n",
      "[['1', 'through', 'night', 'and', 'day', 'i', 'hear', 'it', 'say', '2', 'i', 'm', 'always', 'chasing', 'rainbows', '3', 'watching', 'clouds', 'drifting', 'by', '4', 'my', 'schemes', 'are', 'just', 'like', 'all', 'my', 'dreams', '5', 'ending', 'in', 'the', 'sky', '6', '7', 'some', 'fellows', 'look', 'and', 'find', 'the', 'sunshine', '8', 'i', 'always', 'look', 'and', 'find', 'the', 'rain', '9', 'some', 'fellows', 'make', 'a', 'winning', 'sometimes', '10', 'i', 'never', 'even', 'make', 'a', 'gain', '11', 'believe', 'me', 'i', 'm', 'always', 'chasing', 'rainbows', '12', 'waiting', 'to', 'find', 'a', 'little', 'bluebird', 'in', 'vain', '13', '14', 'i', 'm', 'always', 'chasing', 'rainbows', '15', 'watching', 'clouds', 'drifting', 'by', '16', 'my', 'schemes', 'are', 'just', 'like', 'all', 'my', 'dreams']]\n",
      "[['1', 'through', 'night', 'and', 'day', 'i', 'hear', 'it', 'say', '2', 'i', 'm', 'always', 'chasing', 'rainbows', '3', 'watching', 'clouds', 'drifting', 'by', '4', 'my', 'schemes', 'are', 'just', 'like', 'all', 'my', 'dreams', '5', 'ending', 'in', 'the', 'sky', '6', '7', 'some', 'fellows', 'look', 'and', 'find', 'the', 'sunshine', '8', 'i', 'always', 'look', 'and', 'find', 'the', 'rain', '9', 'some', 'fellows', 'make', 'a', 'winning', 'sometimes', '10', 'i', 'never', 'even', 'make', 'a', 'gain', '11', 'believe', 'me', 'i', 'm', 'always', 'chasing', 'rainbows', '12', 'waiting', 'to', 'find', 'a', 'little', 'bluebird', 'in', 'vain', '13', '14', 'i', 'm', 'always', 'chasing', 'rainbows', '15', 'watching', 'clouds', 'drifting', 'by', '16', 'my', 'schemes', 'are', 'just', 'like', 'all', 'my', 'dreams']]\n",
      "[['1', 'through', 'night', 'and', 'day', 'i', 'hear', 'it', 'say', '2', 'i', 'm', 'always', 'chasing', 'rainbows', '3', 'watching', 'clouds', 'drifting', 'by', '4', 'my', 'schemes', 'are', 'just', 'like', 'all', 'my', 'dreams', '5', 'ending', 'in', 'the', 'sky', '6', '7', 'some', 'fellows', 'look', 'and', 'find', 'the', 'sunshine', '8', 'i', 'always', 'look', 'and', 'find', 'the', 'rain', '9', 'some', 'fellows', 'make', 'a', 'winning', 'sometimes', '10', 'i', 'never', 'even', 'make', 'a', 'gain', '11', 'believe', 'me', 'i', 'm', 'always', 'chasing', 'rainbows', '12', 'waiting', 'to', 'find', 'a', 'little', 'bluebird', 'in', 'vain', '13', '14', 'i', 'm', 'always', 'chasing', 'rainbows', '15', 'watching', 'clouds', 'drifting', 'by', '16', 'my', 'schemes', 'are', 'just', 'like', 'all', 'my', 'dreams']]\n",
      "[['1', 'through', 'night', 'and', 'day', 'i', 'hear', 'it', 'say', '2', 'i', 'm', 'always', 'chasing', 'rainbows', '3', 'watching', 'clouds', 'drifting', 'by', '4', 'my', 'schemes', 'are', 'just', 'like', 'all', 'my', 'dreams', '5', 'ending', 'in', 'the', 'sky', '6', '7', 'some', 'fellows', 'look', 'and', 'find', 'the', 'sunshine', '8', 'i', 'always', 'look', 'and', 'find', 'the', 'rain', '9', 'some', 'fellows', 'make', 'a', 'winning', 'sometimes', '10', 'i', 'never', 'even', 'make', 'a', 'gain', '11', 'believe', 'me', 'i', 'm', 'always', 'chasing', 'rainbows', '12', 'waiting', 'to', 'find', 'a', 'little', 'bluebird', 'in', 'vain', '13', '14', 'i', 'm', 'always', 'chasing', 'rainbows', '15', 'watching', 'clouds', 'drifting', 'by', '16', 'my', 'schemes', 'are', 'just', 'like', 'all', 'my', 'dreams']]\n",
      "[['1', 'through', 'night', 'and', 'day', 'i', 'hear', 'it', 'say', '2', 'i', 'm', 'always', 'chasing', 'rainbows', '3', 'watching', 'clouds', 'drifting', 'by', '4', 'my', 'schemes', 'are', 'just', 'like', 'all', 'my', 'dreams', '5', 'ending', 'in', 'the', 'sky', '6', '7', 'some', 'fellows', 'look', 'and', 'find', 'the', 'sunshine', '8', 'i', 'always', 'look', 'and', 'find', 'the', 'rain', '9', 'some', 'fellows', 'make', 'a', 'winning', 'sometimes', '10', 'i', 'never', 'even', 'make', 'a', 'gain', '11', 'believe', 'me', 'i', 'm', 'always', 'chasing', 'rainbows', '12', 'waiting', 'to', 'find', 'a', 'little', 'bluebird', 'in', 'vain', '13', '14', 'i', 'm', 'always', 'chasing', 'rainbows', '15', 'watching', 'clouds', 'drifting', 'by', '16', 'my', 'schemes', 'are', 'just', 'like', 'all', 'my', 'dreams']]\n"
     ]
    }
   ],
   "source": [
    "scoreDirectory(source_dir='../data/lyrics/current/'\n",
    "                   ,destination_dir='../data/scores/'\n",
    "                   ,output_file_name='output.csv'\n",
    "                   ,reference_dir='../data/lyrics/reference/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list = outputText1.split()\n",
    "#output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 18, 28, 37, 39, 47, 56]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [i for i, x in enumerate(output_list) if x == 'xeol']\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', ',', 'go', '.', 'xbol', 'guy', ',']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [i for i, x in enumerate(output_list) if x == 'xeol']\n",
    "for i in indices:\n",
    "    output_list[i] = '\\n'\n",
    "[output_list[idx-1] for idx in indices if idx>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['xbos', 'xbol', 'listen', 'to', 'my', 'tale', 'of', 'woe', ','],\n",
       " ['xbol', 'it', \"''s\", 'terribly', 'sad', 'but', 'true', ','],\n",
       " ['xbol', 'all', 'dressed', 'up', ',', 'no', 'place', 'to', 'go'],\n",
       " ['xbol', 'each', 'evening', 'i', \"''m\", 'awfully', 'blue', '.'],\n",
       " ['xbol'],\n",
       " ['xbol', 'i', 'must', 'win', 'some', 'handsome', 'guy'],\n",
       " ['xbol', 'can', \"''t\", 'go', 'on', 'like', 'this', ',']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = 0\n",
    "ll = []\n",
    "for idx in indices:\n",
    "    if len(output_list[start:idx]) >0:\n",
    "        ll.append(output_list[start:idx])\n",
    "    start = idx+1\n",
    "ll    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _combine_contraction(token_list, sign=\"'\"):\n",
    "    \"\"\"\n",
    "    combine sequent items in a list that compose a single contraction\n",
    "    \"\"\"\n",
    "    newList= []\n",
    "    for token in token_list:\n",
    "        if not token.startswith(sign):\n",
    "            newList.append(token)\n",
    "        else:\n",
    "            prior = newList.pop()\n",
    "            newList.append(prior+token)\n",
    "    return newList\n",
    "\n",
    "def _remove_markup_and_punc(token_list):\n",
    "    \"\"\"\n",
    "    Removes tags and punctuation other than an apostrophe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    token_list : list\n",
    "        list of tokens\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    cleaned token list\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    tags = ['xbos','xgenre','xtitle','\\n']  #need complete list\n",
    "    ## remove tags and punctuation other than apostrophe\n",
    "    clean_token_list = [token for token in token_list[[i for i,x in enumerate(token_list) if x.startswith('xbol')][0]:] \\\n",
    "                        if token not in tags \\\n",
    "                        and not token.startswith('xbol') \\\n",
    "                        and not token.startswith('[verse') \\\n",
    "                        and not token.startswith('[chorus') \\\n",
    "                        and token not in [c for c in string.punctuation if c not in (\"'\")]] \n",
    "    return clean_token_list\n",
    "\n",
    "def _bleu(ref_list,candidate_token_list,nGram=4,nGramType='cumulative',shouldSmooth=True):\n",
    "    '''calculates BLEU score \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ref_list: list\n",
    "            expects a list of reference texts to compare (as strings)\n",
    "        candidate_token_list: list\n",
    "            the new token list that represents the string that needs to be scored\n",
    "        nGram: int\n",
    "            choose between 1-4.  Determines which ngram(s) to use in the scoring\n",
    "        nGramType: string \n",
    "            'cumulative' uses a simple average of all ngrams from 1 to nGram. 'exclusive' is the chosen nGram only.\n",
    "        shouldSmooth: boolean\n",
    "            if False, calculates the BLEU score without smoothing. Recommended to use smoothing (set to True)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        score: BLEU score using nGram settings input, smoothed by default (can be turned off)\n",
    "    '''\n",
    "\n",
    "    # basic checks\n",
    "    if nGram not in [1,2,3,4]:\n",
    "        raise ValueError('nGram must be between 1 and 4')\n",
    "\n",
    "    if nGramType not in ['cumulative','exclusive']:\n",
    "        raise ValueError('nGramType must either be cumulative (average of nGrams less than n) or exclusive (1=unigram, etc.)')\n",
    "\n",
    "    # pre-score\n",
    "    weight_dict = {('cumulative',1):(1,0,0,0)\n",
    "                  ,('cumulative',2):(.5,.5,0,0)\n",
    "                  ,('cumulative',3):(.3333,.3333,.3333,0)\n",
    "                  ,('cumulative',4):(.25,.25,.25,.25)\n",
    "                  ,('exclusive',1):(1,0,0,0)\n",
    "                  ,('exclusive',2):(0,1,0,0)\n",
    "                  ,('exclusive',3):(0,0,1,0)\n",
    "                  ,('exclusive',4):(0,0,0,1)}\n",
    "\n",
    "\n",
    "\n",
    "    candidate = [[token for token in _remove_markup_and_punc(candidate_token_list) if token != 'xeol']]\n",
    "\n",
    "    references = [[[token for token in _remove_markup_and_punc(ref) if token != 'xeol'] for ref in ref_list]]\n",
    "    weights = weight_dict[(nGramType,nGram)]\n",
    "\n",
    "\n",
    "    # scoring\n",
    "    if shouldSmooth==True:\n",
    "        smoother = bleu_score.SmoothingFunction().method5\n",
    "    else:\n",
    "        smoother = None\n",
    "    score = bleu_score.corpus_bleu(references, candidate, weights, smoothing_function=smoother)\n",
    "    return score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _levenshtein(s1, s2):\n",
    "    '''calculate levenshtein distance for two input strings\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s1: string\n",
    "        first string for comparison\n",
    "    s2: string\n",
    "        second string for comparison\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    distance: levenshtein distance between two strings...that is, the lowest number of modifications to turn s1 into s2\n",
    "    '''\n",
    "    s1 = str(s1)\n",
    "    s2 = str(s2)\n",
    "\n",
    "    if len(s1) < len(s2):\n",
    "        return _levenshtein(s2, s1)\n",
    "\n",
    "    # otherwise len(s1) >= len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer\n",
    "            deletions = current_row[j] + 1       # than s2\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]\n",
    "\n",
    "def _tokens_to_lines(token_list):\n",
    "    \"\"\"\n",
    "    Transforms input of a list of tokens and returns a list of a list of tokens in a given line\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    token_list : list\n",
    "        list of tokens making up a song\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list of lines of tokens making up a song\n",
    "\n",
    "    \"\"\"\n",
    "    start = 0\n",
    "    lines = []\n",
    "    tokens = _combine_contraction(_remove_markup_and_punc(token_list))\n",
    "    tokens.append('xeol')\n",
    "    xeol_idx = [i for i, x in enumerate(tokens) if x == 'xeol']\n",
    "\n",
    "    for idx in xeol_idx:\n",
    "        if len(tokens[start:idx]) >0:\n",
    "            lines.append(tokens[start:idx])\n",
    "        start = idx+1\n",
    "    return lines \n",
    "\n",
    "def _findLineStress(tokenized_line):\n",
    "    '''find accentual stress of a given tokenized line, based on CMU dict.  Uses relative stress per word, so somewhat limited.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenized_line : list\n",
    "        list of tokens from line, usually preprocessed to remove non-words\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    parselist: list of potential stresses after parsing. 0 is unstressed, 1 is primary stress, 2 is secondary stress (middle)\n",
    "    '''\n",
    "\n",
    "    parses = ['']\n",
    "    for word in tokenized_line:\n",
    "        pros = pronouncing.phones_for_word(word)\n",
    "        if pros:\n",
    "            for phonelist in [pronouncing.phones_for_word(word)]:           \n",
    "                stressOptions = copy.deepcopy(parses)\n",
    "                currLen = len(parses)\n",
    "                newparse = []\n",
    "                # I don't really need to loop through pronunciations, just distinct stress patterns, so a little inefficient here\n",
    "                for pronunciation in phonelist:\n",
    "                    wordStress = pronouncing.stresses(pronunciation)\n",
    "                    for option in range(currLen):\n",
    "                        newparse.append(''+str(stressOptions[option]) + str(wordStress))\n",
    "            parses = newparse \n",
    "\n",
    "    return list(set(parses))\n",
    "\n",
    "def get_POS_conformity(tokenized_list):\n",
    "    \"\"\"\n",
    "    Calculates absolute difference from published proportions of POS, between 0 and 1.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #try:\n",
    "    #    self.generated_song\n",
    "    #except AttributeError as e:\n",
    "    #    print(f\"{e} : first generate song using `set_lyric_state=True`\")\n",
    "    #    raise   \n",
    "\n",
    "\n",
    "    # define lookups\n",
    "    mapping = {'CC':'CC','DT':'DT','PDT':'DT','WDT':'DT','IN':'IN','JJ':'JJ','JJR':'JJ','JJS':'JJ'\n",
    "               ,'NN':'NN','NNS':'NN','NNP':'NN','NNPS':'NN','LS':'OT','CD':'OT','EX':'OT','FW':'OT'\n",
    "               ,'POS':'OT','UH':'OT','RB':'RB','RBR':'RB','RBS':'RB','WRB':'RB','TO':'TO','MD':'VB'\n",
    "               ,'RP':'VB','VB':'VB','VBD':'VB','VBG':'VB','VBN':'VB','VBP':'VB','VBZ':'VB','PRP':'WP'\n",
    "               ,'PRP$':'WP','WP':'WP','WP$':'WP'}\n",
    "    comp_dict = {'CC':0.0212,'DT':0.0982,'IN':0.0998,'JJ':0.0613,'NN':0.3051,'RB':0.0766,'TO':0.0351\n",
    "                 ,'VB':0.285,'WP':0.0058,'OT':0.012}\n",
    "\n",
    "    # initialize\n",
    "    pos_cnt = Counter()\n",
    "    total_word_cnt = 0\n",
    "    pos_dict = defaultdict(float) \n",
    "    pos_dict['adjustment'] = 0\n",
    "    absdiff = 0\n",
    "\n",
    "    # prepare data  \n",
    "    tokenized_text = [token for token in _remove_markup_and_punc(tokenized_list) if token != 'xeol']\n",
    "    tag_list = nltk.pos_tag(tokenized_text)\n",
    "    \n",
    "    # initial proportions\n",
    "    for t in tag_list:\n",
    "        pos_cnt[t[1]] +=1\n",
    "        total_word_cnt +=1\n",
    "    pos_raw_dict = {k: v/float(total_word_cnt) for k,v in dict(pos_cnt).items()}\n",
    "\n",
    "    # adjust for items missing in mapping (mostly punctuation)    \n",
    "    for k,v in pos_raw_dict.items():\n",
    "        if k in mapping:\n",
    "            pos_dict[mapping[k]] += v \n",
    "        else:\n",
    "            pos_dict['adjustment'] += v\n",
    "    for k,v in pos_dict.items():\n",
    "        pos_dict[k] = pos_dict[k]/(1-pos_dict['adjustment'])\n",
    "    del pos_dict['adjustment']\n",
    "\n",
    "    # compare to observed ratios, calculate absolute difference\n",
    "    for k in comp_dict.keys():\n",
    "        absdiff += abs(comp_dict[k] - pos_dict.get(k,0))\n",
    "\n",
    "    # use set_metric\n",
    "    #self.set_metric('POS_confirmity', absdiff)\n",
    "    return  absdiff\n",
    "\n",
    "def get_rhyme_density(tokenized_list):\n",
    "    \"\"\"\n",
    "    Calculates Rhyme Density for given tokens\n",
    "\n",
    "    Result:\n",
    "    -------\n",
    "    Updates attributes:\n",
    "        'rhymeDensityAP': rhyme density using all words, perfect rhymes only\n",
    "        'rhymeDensityAV': rhyme density using all words, vowel rhymes only\n",
    "        'rhymeDensityEP': rhyme density using end words, perfect rhymes only\n",
    "        'rhymeDensityEV': rhyme density using end words, vowel rhymes only\n",
    "\n",
    "    \"\"\"\n",
    "    #try:\n",
    "    #    self.generated_song\n",
    "    #except AttributeError as e:\n",
    "    #    print(f\"{e} : first generate song using `set_lyric_state=True`\")\n",
    "    #    raise   \n",
    "\n",
    "    # initialize\n",
    "    rhymePart_cnt_EP = Counter()\n",
    "    rhymePart_cnt_EV = Counter()\n",
    "    rhymePart_cnt_AP = Counter()\n",
    "    rhymePart_cnt_AV = Counter()\n",
    "\n",
    "    rhyme_cnt_EP = 0\n",
    "    rhyme_cnt_EV = 0\n",
    "    rhyme_cnt_AP = 0\n",
    "    rhyme_cnt_AV = 0\n",
    "\n",
    "    # prepare data\n",
    "\n",
    "    ## remove tags and punctuation other than apostrophe\n",
    "    words_A = _combine_contraction(_remove_markup_and_punc(tokenized_list))\n",
    "    \n",
    "    ## find 'xeol' indices, then use these to replace with line break and also find 'end' words\n",
    "    xeol_idx = [i for i, x in enumerate(words_A) if x == 'xeol']\n",
    "    for i in xeol_idx:\n",
    "        words_A[i] = '\\n'\n",
    "    words_E = [words_A[idx-1] for idx in xeol_idx if idx>0]\n",
    "    \n",
    "    wordCount_A = len(words_A)\n",
    "    wordCount_E = len(words_E)\n",
    "\n",
    "    #process 'all' words \n",
    "    for word in words_A:\n",
    "        pros = pronouncing.phones_for_word(word)\n",
    "        if pros:     \n",
    "            phonelist = pros[0]  #using first pronunciation for now\n",
    "            if len(phonelist) > 0:\n",
    "                    rhymePart_cnt_AP[pronouncing.rhyming_part(phonelist)] +=1\n",
    "                    rhymePart_cnt_AV[pronouncing.rhyming_part(phonelist).split()[0]] +=1\n",
    "\n",
    "    for v in rhymePart_cnt_AP.values():\n",
    "        rhyme_cnt_AP += v-1\n",
    "    for v in rhymePart_cnt_AV.values():\n",
    "        rhyme_cnt_AV += v-1 \n",
    "\n",
    "    if wordCount_A > 1:\n",
    "        rhymeDensityAP = rhyme_cnt_AP/float(wordCount_A-1)\n",
    "        rhymeDensityAV = rhyme_cnt_AV/float(wordCount_A-1)\n",
    "    else:\n",
    "        rhymeDensityAP = 0.0\n",
    "        rhymeDensityAV = 0.0\n",
    "\n",
    "    #process 'end' words \n",
    "    for word in words_E:\n",
    "        pros = pronouncing.phones_for_word(word)\n",
    "        if pros:     \n",
    "            phonelist = pros[0]  #using first pronunciation for now\n",
    "            if len(phonelist) > 0:\n",
    "                    rhymePart_cnt_EP[pronouncing.rhyming_part(phonelist)] +=1\n",
    "                    rhymePart_cnt_EV[pronouncing.rhyming_part(phonelist).split()[0]] +=1\n",
    "\n",
    "    for v in rhymePart_cnt_EP.values():\n",
    "        rhyme_cnt_EP += v-1\n",
    "    for v in rhymePart_cnt_EV.values():\n",
    "        rhyme_cnt_EV += v-1            \n",
    "\n",
    "    if wordCount_E > 1:\n",
    "        rhymeDensityEP = rhyme_cnt_EP/float(wordCount_E-1)\n",
    "        rhymeDensityEV = rhyme_cnt_EV/float(wordCount_E-1)\n",
    "    else:\n",
    "        rhymeDensityEP = 0.0\n",
    "        rhymeDensityEV = 0.0\n",
    "\n",
    "    # use set_metric\n",
    "    #self.set_metric('rhymeDensityAP', rhymeDensityAP)\n",
    "    #self.set_metric('rhymeDensityAV', rhymeDensityAV)\n",
    "    #self.set_metric('rhymeDensityEP', rhymeDensityEP)\n",
    "    #self.set_metric('rhymeDensityEV', rhymeDensityEV)\n",
    "    return (rhymeDensityAP, rhymeDensityAV, rhymeDensityEP, rhymeDensityEV)\n",
    "\n",
    "\n",
    "def get_bleu(candidate_token_list, reference_dir='../data/lyrics/reference/', nGram=4, nGramType='cumulative'\n",
    "             , shouldSmooth=True, max_refs=None):\n",
    "    '''\n",
    "    Calculates BLEU score for nGrams 1-4 and smoothed cumulative for nGram in (3,4).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reference_dir: string\n",
    "        expects a string of a directory with reference .txt files\n",
    "    candidate_token_list: list\n",
    "        the new token list that represents the string that needs to be scored\n",
    "    nGram: int\n",
    "        choose between 1-4.  Determines which ngram(s) to use in the scoring\n",
    "    nGramType: string \n",
    "        'cumulative' uses a simple average of all ngrams from 1 to nGram. 'exclusive' is the chosen nGram only.\n",
    "    shouldSmooth: boolean\n",
    "        if False, calculates the BLEU score without smoothing. Recommended to use smoothing (set to True)\n",
    "    max_refs: int or None\n",
    "        maximum amount of reference texts to be considered\n",
    "\n",
    "    Result\n",
    "    -------\n",
    "    Updates attributes scores: \n",
    "        BLEU_1_excl_Unsmoothed\n",
    "        BLEU_2_excl_Unsmoothed\n",
    "        BLEU_3_excl_Unsmoothed\n",
    "        BLEU_4_excl_Unsmoothed\n",
    "        BLEU_3_cumul_Smoothed\n",
    "        BLEU_4_cumul_Smoothed\n",
    "\n",
    "\n",
    "    '''\n",
    "    #try:\n",
    "    #    self.generated_song\n",
    "    #except AttributeError as e:\n",
    "    #    print(f\"{e} : first generate song using `set_lyric_state=True`\")\n",
    "    #    raise   \n",
    "\n",
    "    refs = glob.glob(reference_dir+'/*.txt')\n",
    "    if max_refs is not None:\n",
    "        if len(refs) < max_refs:\n",
    "            max_refs = len(refs)\n",
    "        refs = refs[0:max_refs]\n",
    "    ref_list = []\n",
    "\n",
    "    ## add BLEU reference code\n",
    "    for ref in refs:\n",
    "        with open(ref) as rf:\n",
    "            ref_raw_text = rf.read()\n",
    "            ref_list.append(ref_raw_text)\n",
    "\n",
    "    # use set_metric\n",
    "    return (bleu(ref_list,candidate_token_list,nGram=1,nGramType='exclusive',shouldSmooth=False)\n",
    "            ,bleu(ref_list,candidate_token_list,nGram=2,nGramType='exclusive',shouldSmooth=False)\n",
    "            ,bleu(ref_list,candidate_token_list,nGram=3,nGramType='exclusive',shouldSmooth=False)\n",
    "            ,bleu(ref_list,candidate_token_list,nGram=4,nGramType='exclusive',shouldSmooth=False)\n",
    "            ,bleu(ref_list,candidate_token_list,nGram=3,nGramType='cumulative',shouldSmooth=True)\n",
    "            ,bleu(ref_list,candidate_token_list,nGram=4,nGramType='cumulative',shouldSmooth=True)\n",
    "            )\n",
    "\n",
    "\n",
    "def find_Meter(candidate_token_list):\n",
    "    '''finds meter with smallest edit distance\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    token_list : list\n",
    "        list of tokens making up a song\n",
    "\n",
    "    Result\n",
    "    ------\n",
    "    Updates attributes:       \n",
    "        edits_per_line: average lowest edit distance per line for any standard accentual-syllabic verse\n",
    "        options: list of potential meters for the lowest edit distance\n",
    "    '''\n",
    "\n",
    "    #try:\n",
    "    #   self.generated_song\n",
    "    #except AttributeError as e:\n",
    "    #    print(f\"{e} : first generate song using `set_lyric_state=True`\")\n",
    "    #    raise   \n",
    "\n",
    "\n",
    "    # define\n",
    "    meter_dict = {'0101':'Iambic dimeter'\n",
    "                  ,'010101':'Iambic trimeter'\n",
    "                  ,'01010101':'Iambic tetrameter'\n",
    "                  ,'0101010101':'Iambic pentameter'\n",
    "                  ,'010101010101':'Iambic hexameter'\n",
    "                  ,'01010101010101':'Iambic heptameter'\n",
    "                  ,'0101010101010101':'Iambic octameter'\n",
    "                  ,'1010':'Trochaic dimeter'\n",
    "                  ,'101010':'Trochaic trimeter'\n",
    "                  ,'10101010':'Trochaic tetrameter'\n",
    "                  ,'1010101010':'Trochaic pentameter'\n",
    "                  ,'101010101010':'Trochaic hexameter'\n",
    "                  ,'10101010101010':'Trochaic heptameter'\n",
    "                  ,'1010101010101010':'Trochaic octameter'\n",
    "                  ,'001001':'Anapestic dimeter'\n",
    "                  ,'001001001':'Anapestic trimeter'\n",
    "                  ,'001001001001':'Anapestic tetrameter'\n",
    "                  ,'001001001001001':'Anapestic pentameter'\n",
    "                  ,'001001001001001001':'Anapestic hexameter'\n",
    "                  ,'001001001001001001001':'Anapestic heptameter'\n",
    "                  ,'100100':'Dactyllic dimeter'\n",
    "                  ,'100100100':'Dactyllic trimeter'\n",
    "                  ,'100100100100':'Dactyllic tetrameter'\n",
    "                  ,'100100100100100':'Dactyllic pentameter'\n",
    "                  ,'100100100100100100':'Dactyllic hexameter'\n",
    "                  ,'100100100100100100100':'Dactyllic heptameter'}\n",
    "\n",
    "    # initialize\n",
    "    vote_cnt = Counter()\n",
    "\n",
    "    lines = _tokens_to_lines(candidate_token_list)\n",
    "    line_cnt = len(lines)\n",
    "    minDist = 999\n",
    "\n",
    "    # update distances\n",
    "    for line in lines:\n",
    "        for k,v in meter_dict.items():\n",
    "            minDist = 999\n",
    "            for reading in _findLineStress(line):\n",
    "                dist = _levenshtein(k,reading)\n",
    "                if dist < minDist:\n",
    "                    minDist = dist    \n",
    "            vote_cnt[v] += minDist\n",
    "    print(vote_cnt)\n",
    "\n",
    "    lowest = min(vote_cnt.values()) \n",
    "    options = [k for k,v in vote_cnt.items() if v==lowest]\n",
    "\n",
    "    # use set_metric\n",
    "    return options, lowest/float(line_cnt)\n",
    "    #self.set_metric('closestMeters', options)\n",
    "    #self.set_metric('editsPerLine', lowest/float(line_cnt))\n",
    "\n",
    "def get_all_metrics(candidate_token_list):\n",
    "    \"\"\"\n",
    "    Runs all available metrics and updates `self.metrics` state\n",
    "    \"\"\"\n",
    "\n",
    "    get_rhyme_density(candidate_token_list)\n",
    "    get_POS_conformity(candidate_token_list)\n",
    "    findMeter(candidate_token_list)\n",
    "    get_bleu(candidate_token_list=candidate_token_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyric</th>\n",
       "      <th>meta_GPU</th>\n",
       "      <th>meta_audio</th>\n",
       "      <th>meta_beam_width</th>\n",
       "      <th>meta_context_length</th>\n",
       "      <th>meta_genre</th>\n",
       "      <th>meta_max_len</th>\n",
       "      <th>meta_model_name</th>\n",
       "      <th>meta_model_type</th>\n",
       "      <th>meta_multinomial</th>\n",
       "      <th>...</th>\n",
       "      <th>metrics_BLEU_4_excl_Unsmoothed</th>\n",
       "      <th>metrics_POS_conformity</th>\n",
       "      <th>metrics_closestMeters</th>\n",
       "      <th>metrics_editsPerLine</th>\n",
       "      <th>metrics_rhymeDensityAP</th>\n",
       "      <th>metrics_rhymeDensityAS</th>\n",
       "      <th>metrics_rhymeDensityAV</th>\n",
       "      <th>metrics_rhymeDensityEP</th>\n",
       "      <th>metrics_rhymeDensityES</th>\n",
       "      <th>metrics_rhymeDensityEV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[xbos, xgenre, pop, xtitle, dear, god, xbol-1,...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>pop</td>\n",
       "      <td>40</td>\n",
       "      <td>4.2-LM-108k-lines-genre-song_title</td>\n",
       "      <td>language</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>4.166667e-02</td>\n",
       "      <td>0.664781</td>\n",
       "      <td>[Iambic dimeter, Trochaic dimeter]</td>\n",
       "      <td>202.2</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[xbos, xgenre, pop, xtitle, can, 't, get, enou...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>pop</td>\n",
       "      <td>40</td>\n",
       "      <td>4.2-LM-108k-lines-genre-song_title</td>\n",
       "      <td>language</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>2.225074e-308</td>\n",
       "      <td>0.634762</td>\n",
       "      <td>[Iambic dimeter, Trochaic dimeter]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[xbos, xgenre, pop, xtitle, you, 're, the, one...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>pop</td>\n",
       "      <td>40</td>\n",
       "      <td>4.2-LM-108k-lines-genre-song_title</td>\n",
       "      <td>language</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>2.225074e-308</td>\n",
       "      <td>0.725700</td>\n",
       "      <td>[Iambic dimeter, Trochaic dimeter]</td>\n",
       "      <td>252.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[xbos, xgenre, pop, xtitle, hit, the, street, ...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>pop</td>\n",
       "      <td>40</td>\n",
       "      <td>4.2-LM-108k-lines-genre-song_title</td>\n",
       "      <td>language</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>2.225074e-308</td>\n",
       "      <td>0.744167</td>\n",
       "      <td>[Iambic dimeter, Trochaic dimeter]</td>\n",
       "      <td>501.0</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[xbos, xgenre, pop, xtitle, bye, bye, love, xb...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>pop</td>\n",
       "      <td>40</td>\n",
       "      <td>4.2-LM-108k-lines-genre-song_title</td>\n",
       "      <td>language</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>2.225074e-308</td>\n",
       "      <td>0.844445</td>\n",
       "      <td>[Iambic dimeter, Trochaic dimeter]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[xbos, xgenre, pop, xtitle, you, say, you, lov...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>pop</td>\n",
       "      <td>40</td>\n",
       "      <td>4.2-LM-108k-lines-genre-song_title</td>\n",
       "      <td>language</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>2.225074e-308</td>\n",
       "      <td>1.418500</td>\n",
       "      <td>[Iambic dimeter, Trochaic dimeter]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[xbos, xgenre, pop, xtitle, there, 's, somethi...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>pop</td>\n",
       "      <td>40</td>\n",
       "      <td>4.2-LM-108k-lines-genre-song_title</td>\n",
       "      <td>language</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>4.761905e-02</td>\n",
       "      <td>0.848233</td>\n",
       "      <td>[Iambic dimeter, Trochaic dimeter]</td>\n",
       "      <td>252.0</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[xbos, xgenre, pop, xtitle, hey, xbol-1, hey, ...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>pop</td>\n",
       "      <td>40</td>\n",
       "      <td>4.2-LM-108k-lines-genre-song_title</td>\n",
       "      <td>language</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>4.668327e-02</td>\n",
       "      <td>5.154606</td>\n",
       "      <td>[Iambic dimeter, Trochaic dimeter]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[xbos, xgenre, pop, xtitle, i, 'll, never, be,...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>pop</td>\n",
       "      <td>40</td>\n",
       "      <td>4.2-LM-108k-lines-genre-song_title</td>\n",
       "      <td>language</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>2.225074e-308</td>\n",
       "      <td>0.624567</td>\n",
       "      <td>[Iambic dimeter, Trochaic dimeter]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[xbos, xgenre, pop, xtitle, million, eyes, (lp...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>pop</td>\n",
       "      <td>40</td>\n",
       "      <td>4.2-LM-108k-lines-genre-song_title</td>\n",
       "      <td>language</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>8.333333e-02</td>\n",
       "      <td>1.221826</td>\n",
       "      <td>[Iambic dimeter, Trochaic dimeter]</td>\n",
       "      <td>335.0</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[xbos, xgenre, pop, xtitle, the, only, thing, ...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>pop</td>\n",
       "      <td>40</td>\n",
       "      <td>4.2-LM-108k-lines-genre-song_title</td>\n",
       "      <td>language</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>2.225074e-308</td>\n",
       "      <td>0.885014</td>\n",
       "      <td>[Iambic dimeter, Trochaic dimeter]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>1.074074</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[xbos, xgenre, pop, xtitle, don, 't, turn, aro...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>pop</td>\n",
       "      <td>40</td>\n",
       "      <td>4.2-LM-108k-lines-genre-song_title</td>\n",
       "      <td>language</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>2.225074e-308</td>\n",
       "      <td>0.871703</td>\n",
       "      <td>[Iambic dimeter, Trochaic dimeter]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[xbos, xgenre, pop, xtitle, invisible, xbol-1,...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>pop</td>\n",
       "      <td>40</td>\n",
       "      <td>4.2-LM-108k-lines-genre-song_title</td>\n",
       "      <td>language</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000e-02</td>\n",
       "      <td>0.749195</td>\n",
       "      <td>[Iambic dimeter, Trochaic dimeter]</td>\n",
       "      <td>202.2</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[xbos, xgenre, pop, xtitle, stormy, weather, (...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>pop</td>\n",
       "      <td>40</td>\n",
       "      <td>4.2-LM-108k-lines-genre-song_title</td>\n",
       "      <td>language</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>2.225074e-308</td>\n",
       "      <td>0.646300</td>\n",
       "      <td>[Iambic dimeter, Trochaic dimeter]</td>\n",
       "      <td>600.6</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[xbos, xgenre, pop, xtitle, ain, 't, got, time...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>pop</td>\n",
       "      <td>40</td>\n",
       "      <td>4.2-LM-108k-lines-genre-song_title</td>\n",
       "      <td>language</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>4.166667e-02</td>\n",
       "      <td>0.694781</td>\n",
       "      <td>[Iambic dimeter, Trochaic dimeter]</td>\n",
       "      <td>252.0</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[xbos, xgenre, pop, xtitle, shooting, star, xb...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>pop</td>\n",
       "      <td>40</td>\n",
       "      <td>4.2-LM-108k-lines-genre-song_title</td>\n",
       "      <td>language</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>2.225074e-308</td>\n",
       "      <td>1.136443</td>\n",
       "      <td>[Iambic dimeter, Trochaic dimeter]</td>\n",
       "      <td>252.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[xbos, xgenre, pop, xtitle, build, me, up, xbo...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>pop</td>\n",
       "      <td>40</td>\n",
       "      <td>4.2-LM-108k-lines-genre-song_title</td>\n",
       "      <td>language</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1.153846e-01</td>\n",
       "      <td>0.701820</td>\n",
       "      <td>[Iambic dimeter, Trochaic dimeter]</td>\n",
       "      <td>335.0</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[xbos, xgenre, pop, xtitle, walkin, ', in, my,...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>pop</td>\n",
       "      <td>40</td>\n",
       "      <td>4.2-LM-108k-lines-genre-song_title</td>\n",
       "      <td>language</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>2.225074e-308</td>\n",
       "      <td>0.658465</td>\n",
       "      <td>[Iambic dimeter, Trochaic dimeter]</td>\n",
       "      <td>202.2</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[xbos, xgenre, pop, xtitle, rewind, xbol-1, wh...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>pop</td>\n",
       "      <td>40</td>\n",
       "      <td>4.2-LM-108k-lines-genre-song_title</td>\n",
       "      <td>language</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>2.225074e-308</td>\n",
       "      <td>0.853186</td>\n",
       "      <td>[Iambic dimeter, Trochaic dimeter]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[xbos, xgenre, pop, xtitle, life, goes, on, (a...</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>pop</td>\n",
       "      <td>40</td>\n",
       "      <td>4.2-LM-108k-lines-genre-song_title</td>\n",
       "      <td>language</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>2.225074e-308</td>\n",
       "      <td>0.686967</td>\n",
       "      <td>[Iambic dimeter, Trochaic dimeter]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                lyric  meta_GPU meta_audio  \\\n",
       "0   [xbos, xgenre, pop, xtitle, dear, god, xbol-1,...      True       None   \n",
       "1   [xbos, xgenre, pop, xtitle, can, 't, get, enou...      True       None   \n",
       "2   [xbos, xgenre, pop, xtitle, you, 're, the, one...      True       None   \n",
       "3   [xbos, xgenre, pop, xtitle, hit, the, street, ...      True       None   \n",
       "4   [xbos, xgenre, pop, xtitle, bye, bye, love, xb...      True       None   \n",
       "5   [xbos, xgenre, pop, xtitle, you, say, you, lov...      True       None   \n",
       "6   [xbos, xgenre, pop, xtitle, there, 's, somethi...      True       None   \n",
       "7   [xbos, xgenre, pop, xtitle, hey, xbol-1, hey, ...      True       None   \n",
       "8   [xbos, xgenre, pop, xtitle, i, 'll, never, be,...      True       None   \n",
       "9   [xbos, xgenre, pop, xtitle, million, eyes, (lp...      True       None   \n",
       "10  [xbos, xgenre, pop, xtitle, the, only, thing, ...      True       None   \n",
       "11  [xbos, xgenre, pop, xtitle, don, 't, turn, aro...      True       None   \n",
       "12  [xbos, xgenre, pop, xtitle, invisible, xbol-1,...      True       None   \n",
       "13  [xbos, xgenre, pop, xtitle, stormy, weather, (...      True       None   \n",
       "14  [xbos, xgenre, pop, xtitle, ain, 't, got, time...      True       None   \n",
       "15  [xbos, xgenre, pop, xtitle, shooting, star, xb...      True       None   \n",
       "16  [xbos, xgenre, pop, xtitle, build, me, up, xbo...      True       None   \n",
       "17  [xbos, xgenre, pop, xtitle, walkin, ', in, my,...      True       None   \n",
       "18  [xbos, xgenre, pop, xtitle, rewind, xbol-1, wh...      True       None   \n",
       "19  [xbos, xgenre, pop, xtitle, life, goes, on, (a...      True       None   \n",
       "\n",
       "    meta_beam_width  meta_context_length meta_genre  meta_max_len  \\\n",
       "0                 3                   30        pop            40   \n",
       "1                 6                   30        pop            40   \n",
       "2                 9                   30        pop            40   \n",
       "3                 3                   30        pop            40   \n",
       "4                 6                   30        pop            40   \n",
       "5                 9                   30        pop            40   \n",
       "6                 3                   30        pop            40   \n",
       "7                 6                   30        pop            40   \n",
       "8                 9                   30        pop            40   \n",
       "9                 3                   30        pop            40   \n",
       "10                6                   30        pop            40   \n",
       "11                9                   30        pop            40   \n",
       "12                3                   30        pop            40   \n",
       "13                6                   30        pop            40   \n",
       "14                9                   30        pop            40   \n",
       "15                3                   30        pop            40   \n",
       "16                6                   30        pop            40   \n",
       "17                9                   30        pop            40   \n",
       "18                3                   30        pop            40   \n",
       "19                6                   30        pop            40   \n",
       "\n",
       "                       meta_model_name meta_model_type  meta_multinomial  \\\n",
       "0   4.2-LM-108k-lines-genre-song_title        language              True   \n",
       "1   4.2-LM-108k-lines-genre-song_title        language              True   \n",
       "2   4.2-LM-108k-lines-genre-song_title        language              True   \n",
       "3   4.2-LM-108k-lines-genre-song_title        language              True   \n",
       "4   4.2-LM-108k-lines-genre-song_title        language              True   \n",
       "5   4.2-LM-108k-lines-genre-song_title        language              True   \n",
       "6   4.2-LM-108k-lines-genre-song_title        language              True   \n",
       "7   4.2-LM-108k-lines-genre-song_title        language              True   \n",
       "8   4.2-LM-108k-lines-genre-song_title        language              True   \n",
       "9   4.2-LM-108k-lines-genre-song_title        language              True   \n",
       "10  4.2-LM-108k-lines-genre-song_title        language              True   \n",
       "11  4.2-LM-108k-lines-genre-song_title        language              True   \n",
       "12  4.2-LM-108k-lines-genre-song_title        language              True   \n",
       "13  4.2-LM-108k-lines-genre-song_title        language              True   \n",
       "14  4.2-LM-108k-lines-genre-song_title        language              True   \n",
       "15  4.2-LM-108k-lines-genre-song_title        language              True   \n",
       "16  4.2-LM-108k-lines-genre-song_title        language              True   \n",
       "17  4.2-LM-108k-lines-genre-song_title        language              True   \n",
       "18  4.2-LM-108k-lines-genre-song_title        language              True   \n",
       "19  4.2-LM-108k-lines-genre-song_title        language              True   \n",
       "\n",
       "             ...           metrics_BLEU_4_excl_Unsmoothed  \\\n",
       "0            ...                             4.166667e-02   \n",
       "1            ...                            2.225074e-308   \n",
       "2            ...                            2.225074e-308   \n",
       "3            ...                            2.225074e-308   \n",
       "4            ...                            2.225074e-308   \n",
       "5            ...                            2.225074e-308   \n",
       "6            ...                             4.761905e-02   \n",
       "7            ...                             4.668327e-02   \n",
       "8            ...                            2.225074e-308   \n",
       "9            ...                             8.333333e-02   \n",
       "10           ...                            2.225074e-308   \n",
       "11           ...                            2.225074e-308   \n",
       "12           ...                             4.000000e-02   \n",
       "13           ...                            2.225074e-308   \n",
       "14           ...                             4.166667e-02   \n",
       "15           ...                            2.225074e-308   \n",
       "16           ...                             1.153846e-01   \n",
       "17           ...                            2.225074e-308   \n",
       "18           ...                            2.225074e-308   \n",
       "19           ...                            2.225074e-308   \n",
       "\n",
       "    metrics_POS_conformity               metrics_closestMeters  \\\n",
       "0                 0.664781  [Iambic dimeter, Trochaic dimeter]   \n",
       "1                 0.634762  [Iambic dimeter, Trochaic dimeter]   \n",
       "2                 0.725700  [Iambic dimeter, Trochaic dimeter]   \n",
       "3                 0.744167  [Iambic dimeter, Trochaic dimeter]   \n",
       "4                 0.844445  [Iambic dimeter, Trochaic dimeter]   \n",
       "5                 1.418500  [Iambic dimeter, Trochaic dimeter]   \n",
       "6                 0.848233  [Iambic dimeter, Trochaic dimeter]   \n",
       "7                 5.154606  [Iambic dimeter, Trochaic dimeter]   \n",
       "8                 0.624567  [Iambic dimeter, Trochaic dimeter]   \n",
       "9                 1.221826  [Iambic dimeter, Trochaic dimeter]   \n",
       "10                0.885014  [Iambic dimeter, Trochaic dimeter]   \n",
       "11                0.871703  [Iambic dimeter, Trochaic dimeter]   \n",
       "12                0.749195  [Iambic dimeter, Trochaic dimeter]   \n",
       "13                0.646300  [Iambic dimeter, Trochaic dimeter]   \n",
       "14                0.694781  [Iambic dimeter, Trochaic dimeter]   \n",
       "15                1.136443  [Iambic dimeter, Trochaic dimeter]   \n",
       "16                0.701820  [Iambic dimeter, Trochaic dimeter]   \n",
       "17                0.658465  [Iambic dimeter, Trochaic dimeter]   \n",
       "18                0.853186  [Iambic dimeter, Trochaic dimeter]   \n",
       "19                0.686967  [Iambic dimeter, Trochaic dimeter]   \n",
       "\n",
       "    metrics_editsPerLine  metrics_rhymeDensityAP  metrics_rhymeDensityAS  \\\n",
       "0                  202.2                0.269231                0.538462   \n",
       "1                    3.0                0.640000                0.680000   \n",
       "2                  252.0                0.600000                0.520000   \n",
       "3                  501.0                0.652174                0.695652   \n",
       "4                    3.0                0.952381                0.952381   \n",
       "5                    3.0                0.875000                0.875000   \n",
       "6                  252.0                0.652174                0.826087   \n",
       "7                    3.0                0.515152                0.515152   \n",
       "8                    3.0                0.652174                0.521739   \n",
       "9                  335.0                0.538462                0.576923   \n",
       "10                   3.0                0.777778                0.740741   \n",
       "11                   3.0                0.652174                0.652174   \n",
       "12                 202.2                0.407407                0.555556   \n",
       "13                 600.6                0.304348                0.391304   \n",
       "14                 252.0                0.423077                0.461538   \n",
       "15                 252.0                0.500000                0.571429   \n",
       "16                 335.0                0.392857                0.392857   \n",
       "17                 202.2                0.772727                0.590909   \n",
       "18                   3.0                0.740741                0.777778   \n",
       "19                   3.0                0.724138                0.724138   \n",
       "\n",
       "    metrics_rhymeDensityAV  metrics_rhymeDensityEP  metrics_rhymeDensityES  \\\n",
       "0                 0.576923                0.000000                0.000000   \n",
       "1                 0.680000                1.000000                1.000000   \n",
       "2                 0.680000                0.666667                0.666667   \n",
       "3                 0.695652                0.250000                0.250000   \n",
       "4                 0.952381                1.000000                1.000000   \n",
       "5                 0.875000                1.000000                1.000000   \n",
       "6                 0.956522                0.666667                0.666667   \n",
       "7                 0.515152                1.000000                1.000000   \n",
       "8                 0.782609                1.000000                1.000000   \n",
       "9                 0.576923                0.000000                0.000000   \n",
       "10                1.074074                1.000000                1.000000   \n",
       "11                0.652174                1.000000                1.000000   \n",
       "12                0.666667                0.000000                0.250000   \n",
       "13                0.565217                0.333333                0.333333   \n",
       "14                0.461538                0.333333                0.333333   \n",
       "15                0.571429                0.000000                0.000000   \n",
       "16                0.714286                0.000000                0.000000   \n",
       "17                1.000000                0.750000                0.750000   \n",
       "18                0.777778                0.750000                0.750000   \n",
       "19                0.724138                     NaN                     NaN   \n",
       "\n",
       "    metrics_rhymeDensityEV  \n",
       "0                 0.000000  \n",
       "1                 1.000000  \n",
       "2                 0.666667  \n",
       "3                 0.250000  \n",
       "4                 1.000000  \n",
       "5                 1.000000  \n",
       "6                 0.666667  \n",
       "7                 1.000000  \n",
       "8                 1.000000  \n",
       "9                 0.000000  \n",
       "10                1.000000  \n",
       "11                1.000000  \n",
       "12                0.250000  \n",
       "13                0.666667  \n",
       "14                0.333333  \n",
       "15                0.000000  \n",
       "16                0.000000  \n",
       "17                0.750000  \n",
       "18                0.750000  \n",
       "19                     NaN  \n",
       "\n",
       "[20 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json\n",
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "DIR = '../data/json/batch-01'\n",
    "\n",
    "def open_json(file):\n",
    "    if not file.split('/')[-1].startswith('.'):\n",
    "        with open(file) as f:\n",
    "            xx = json.load(f)\n",
    "        return xx\n",
    "    \n",
    "def flatten(d, parent_key='', sep='_'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, collections.MutableMapping):\n",
    "            items.extend(flatten(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "batch_list = [open_json(os.path.join(DIR, file)) for file in os.listdir(DIR)]\n",
    "flattened = [flatten(b) for b in batch_list if b]\n",
    "df = pd.DataFrame(flattened)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xbos xgenre pop xtitle dear god xbol-1 [verse-1] \n",
      " xbol-2 it 's been a long time \n",
      " xbol-3 since i had to leave you \n",
      " xbol-4 i 've been saying \n",
      " xbol-5 that i 've been doing too much \n",
      " xbol-6 but i can\n"
     ]
    }
   ],
   "source": [
    "#print(' '.join(df['lyric'].head(1)[0]))\n",
    "samp1_tokenized = df['lyric'].head(1)[0]\n",
    "samp1 = ' '.join(df['lyric'].head(1)[0])\n",
    "print(samp1.replace('xeol','\\n'))\n",
    "#samp1_tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xbos xgenre pop xtitle can 't get enough of your love xbol-1 i can 't get enough of your love \n",
      " xbol-2 i can 't get enough of your love \n",
      " xbol-3 can 't get enough of your love \n",
      " xbol-4 i can 't\n"
     ]
    }
   ],
   "source": [
    "samp2_tokenized = df['lyric'].iloc[[1]][1]\n",
    "samp2 = ' '.join(df['lyric'].iloc[[1]][1])\n",
    "print(samp2.replace('xeol','\\n'))\n",
    "#samp2_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_bleu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5da7de597daf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_bleu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamp2_tokenized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreference_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'../data/lyrics/reference/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'get_bleu' is not defined"
     ]
    }
   ],
   "source": [
    "get_bleu(samp2_tokenized, reference_dir='../data/lyrics/reference/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xbos', 'xgenre', 'pop', 'xtitle', 'can', \"'t\", 'get', 'enough', 'of', 'your', 'love', 'xbol-1', 'i', 'can', \"'t\", 'get', 'enough', 'of', 'your', 'love', 'xeol', 'xbol-2', 'i', 'can', \"'t\", 'get', 'enough', 'of', 'your', 'love', 'xeol', 'xbol-3', 'can', \"'t\", 'get', 'enough', 'of', 'your', 'love', 'xeol', 'xbol-4', 'i', 'can', \"'t\"]\n",
      "\n",
      "[['xbos', 'xbol', 'listen', 'to', 'my', 'tale', 'of', 'woe', ',', 'xeol', 'xbol', 'it', \"'s\", 'terribly', 'sad', 'but', 'true', ',', 'xeol', 'xbol', 'all', 'dressed', 'up', ',', 'no', 'place', 'to', 'go', 'xeol', 'xbol', 'each', 'evening', 'i', \"'m\", 'awfully', 'blue', '.', 'xeol', 'xbol', 'xeol', 'xbol', 'i', 'must', 'win', 'some', 'handsome', 'guy', 'xeol', 'xbol', 'can', \"'t\", 'go', 'on', 'like', 'this', ',', 'xeol'], ['xbos', 'xbol', 'whether', 'you', 'are', 'here', 'or', 'yonder', ',', 'xeol', 'xbol', 'whether', 'you', 'are', 'false', 'or', 'true', 'xeol', 'xbol', 'whether', 'you', 'remain', 'or', 'wander', ',', 'xeol', 'xbol', 'i', \"'m\", 'growing', 'senses', 'of'], ['xbos', 'xbol-1', 'knowin', \"'\", 'that', 'you', \"'re\", 'leavin', \"'\", 'xeol', 'xbol-2', 'i', 'can', \"'t\", 'believe', 'it', \"'s\", 'happening', 'to', 'me', 'xeol', 'xbol-3', 'knowing', 'that', 'you', \"'re\", 'leavin', \"'\", 'me', 'alone', 'xeol', 'xbol-4', 'i', 'can', \"'t\", 'believe', 'that', 'you', \"'re\", 'in', 'love', 'with', 'me', 'xeol', 'xbol-5', 'xeol', 'xbol-6', 'breakin', \"'\", 'up', 'is', 'hard', 'to', 'do', 'xeol', 'xbol-7', 'now', 'i', 'know', ',', 'know', 'that', 'i', \"'ve\", 'been', 'true', 'xeol', 'xbol-8', 'breakin', \"'\", 'up', 'is', 'hard', 'to', 'do', 'xeol', 'xbol-9', 'now', 'i', 'know', ',', 'know', 'that', 'i', \"'ve\", 'been', 'true', 'xeol', 'xbol-10', 'xeol', 'xbol-11', 'missin', \"'\", 'you', 'xeol', 'xbol-12', 'seems', 'the', 'way', 'i', 'do', 'xeol', 'xbol-13', 'that', \"'s\", 'what', 'i', 'say', 'xeol', 'xbol-14', 'xeol', 'xbol-15', 'breakin', \"'\", 'up', 'is', 'hard', 'to', 'do', 'xeol', 'xbol-16', 'now', 'i', 'know', 'i', 'need', 'you', 'by', 'my', 'side', 'xeol', 'xbol-17', 'how', 'can', 'i', 'prove', 'that', 'i', 'love', 'you', '?', 'xeol'], ['xbos', 'xbol-1', 'each', 'time', 'i', 'look', 'at', 'you', 'xeol', 'xbol-2', 'is', 'like', 'the', 'first', 'time', 'xeol', 'xbol-3', 'each', 'time', 'you', \"'re\", 'near', 'me', 'xeol', 'xbol-4', 'the', 'thrill', 'is', 'new', 'xeol', 'xbol-5', 'and', 'there', 'is', 'nothing', 'xeol', 'xbol-6', 'that', 'i', 'wouldn', \"'t\", 'do', 'for', 'xeol', 'xbol-7', 'the', 'rare', 'delight', 'of', 'the', 'sight', 'xeol', 'xbol-8', 'of', 'you', 'for', 'xeol', 'xbol-9', 'xeol', 'xbol-10', 'the', 'more', 'i', 'see', 'you', ',', 'xeol', 'xbol-11', 'the', 'more', 'i', 'want', 'you', 'xeol', 'xbol-12', 'somehow', 'this', 'feeling', 'xeol', 'xbol-13', 'just', 'grows', 'and', 'grows', 'xeol', 'xbol-14', 'with', 'every', 'sigh', 'xeol', 'xbol-15', 'i', 'become', 'more', 'mad', 'about', 'you', 'xeol', 'xbol-16', 'more', 'lost', 'without', 'you', 'and', 'so', 'it', 'goes', 'xeol', 'xbol-17', 'xeol', 'xbol-18', 'can', 'you', 'imagine', 'how', 'much', 'i', 'love', 'you', '?', 'xeol'], ['xbos', 'xbol-1', 'through', 'night', 'and', 'day', ',', 'i', 'hear', 'it', 'say', 'xeol', 'xbol-2', 'i', \"'m\", 'always', 'chasing', 'rainbows', 'xeol', 'xbol-3', 'watching', 'clouds', 'drifting', 'by', 'xeol', 'xbol-4', 'my', 'schemes', 'are', 'just', 'like', 'all', 'my', 'dreams', 'xeol', 'xbol-5', 'ending', 'in', 'the', 'sky', 'xeol', 'xbol-6', 'xeol', 'xbol-7', 'some', 'fellows', 'look', 'and', 'find', 'the', 'sunshine', 'xeol', 'xbol-8', 'i', 'always', 'look', 'and', 'find', 'the', 'rain', 'xeol', 'xbol-9', 'some', 'fellows', 'make', 'a', 'winning', 'sometimes', 'xeol', 'xbol-10', 'i', 'never', 'even', 'make', 'a', 'gain', 'xeol', 'xbol-11', 'believe', 'me', ',', 'i', \"'m\", 'always', 'chasing', 'rainbows', 'xeol', 'xbol-12', 'waiting', 'to', 'find', 'a', 'little', 'bluebird', 'in', 'vain', 'xeol', 'xbol-13', 'xeol', 'xbol-14', 'i', \"'m\", 'always', 'chasing', 'rainbows', 'xeol', 'xbol-15', 'watching', 'clouds', 'drifting', 'by', 'xeol', 'xbol-16', 'my', 'schemes', 'are', 'just', 'like', 'all', 'my', 'dreams', 'xeol']]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'startswith'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-a3580de2483a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mref_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m print(round(bleu(ref_list,text,nGram=1,nGramType='exclusive',shouldSmooth=False),4)\n\u001b[0m\u001b[0;32m     16\u001b[0m                             \u001b[1;33m,\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbleu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mref_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnGram\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnGramType\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'exclusive'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshouldSmooth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                             \u001b[1;33m,\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbleu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mref_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnGram\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnGramType\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'exclusive'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshouldSmooth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\MIDS\\w210\\capstone-deep-lyrics\\src\\nlp\\evaluation_methods.py\u001b[0m in \u001b[0;36mbleu\u001b[1;34m(tokens, ref_list, nGram, nGramType, shouldSmooth)\u001b[0m\n\u001b[0;32m    254\u001b[0m                   ,('exclusive',4):(0,0,0,1)}\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m     \u001b[0mcandidate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m     \u001b[0mreferences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mparse_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mref_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\MIDS\\w210\\capstone-deep-lyrics\\src\\nlp\\evaluation_methods.py\u001b[0m in \u001b[0;36mparse_tokens\u001b[1;34m(tokens, lines, tags, contraction)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;31m# lines and no tags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontraction\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombine_contraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlines\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\MIDS\\w210\\capstone-deep-lyrics\\src\\nlp\\evaluation_methods.py\u001b[0m in \u001b[0;36mcombine_contraction\u001b[1;34m(token_list, sign)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnewList\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mnewList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msign\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m             \u001b[0mnewList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'startswith'"
     ]
    }
   ],
   "source": [
    "reference_dir = '../data/lyrics/reference/'\n",
    "refs = glob.glob(reference_dir+'*.txt')\n",
    "ref_list = []\n",
    "\n",
    "## add BLEU reference code\n",
    "for ref in refs:\n",
    "    with open(ref) as rf:\n",
    "        ref_raw_text = rf.read()\n",
    "        ref_list.append(ref_raw_text.split())\n",
    "\n",
    "text = samp2_tokenized\n",
    "print(text)\n",
    "print()\n",
    "print(ref_list)\n",
    "print(round(bleu(ref_list,text,nGram=1,nGramType='exclusive',shouldSmooth=False),4)\n",
    "                            ,round(bleu(ref_list,text,nGram=2,nGramType='exclusive',shouldSmooth=False),4)\n",
    "                            ,round(bleu(ref_list,text,nGram=3,nGramType='exclusive',shouldSmooth=False),4)\n",
    "                            ,round(bleu(ref_list,text,nGram=4,nGramType='exclusive',shouldSmooth=False),4)\n",
    "                            ,round(bleu(ref_list,text,nGram=3,nGramType='cumulative',shouldSmooth=True),4)\n",
    "                            ,round(bleu(ref_list,text,nGram=4,nGramType='cumulative',shouldSmooth=True),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', \"i'm\", 'test', 'y']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_contraction([\"a\",\"i\",\"'m\",'test','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "pop from empty list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-222c6e8519a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: pop from empty list"
     ]
    }
   ],
   "source": [
    "t = []\n",
    "if len(t)==0:\n",
    "    \n",
    "    s = t.pop()\n",
    "    t.append(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'b\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "a.append(\"'b\")\n",
    "a.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    newList= []\n",
    "    #newList.append(token_list[0])\n",
    "    for token in token_list:\n",
    "        if not token.startswith(sign):\n",
    "            newList.append(token)\n",
    "        elif len(newList)>0:\n",
    "            #print(newList)\n",
    "            prior = newList.pop()\n",
    "            newList.append(prior+token)\n",
    "        else:\n",
    "            newList.append(token)\n",
    "        print(newList)\n",
    "    return newList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'a\"]\n",
      "[\"'a\", 'i']\n",
      "[\"'a\", \"i'm\"]\n",
      "[\"'a\", \"i'm\", 'test']\n",
      "[\"'a\", \"i'm\", 'test', 'y']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"'a\", \"i'm\", 'test', 'y']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list = [\"'a\",\"i\",\"'m\",'test','y']\n",
    "sign = \"'\"\n",
    "newList= []\n",
    "for token in token_list:\n",
    "    if not token.startswith(sign):\n",
    "        newList.append(token)\n",
    "    elif len(newList)>0:\n",
    "        prior = newList.pop()\n",
    "        newList.append(prior+token)\n",
    "    else:\n",
    "        newList.append(token)\n",
    "    print(newList)\n",
    "newList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
