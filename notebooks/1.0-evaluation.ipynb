{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.translate import bleu_score \n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import re\n",
    "import copy\n",
    "#!pip install pronouncing\n",
    "import pronouncing\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\justi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\justi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some basic preprocessing techniques\n",
    "def prepString(s):\n",
    "    '''removes punctuation other than apostrophes from string'''\n",
    "    return str(s).lower().translate({ord(c): None for c in string.punctuation if c not in (\"'\")})\n",
    "\n",
    "def removePunc(s):\n",
    "    '''removes punctuation from string'''\n",
    "    return str(s).lower().translate({ord(c): None for c in string.punctuation})\n",
    "\n",
    "def removeMarkupWords(s):\n",
    "    '''removes positional words generated in lyrics'''\n",
    "    s = str(s).lower()\n",
    "    for term in ['xbol-10','xbol-11','xbol-12','xbol-13','xbol-14','xbol-15','xbol-16','xbol-17','xbol-18','xbol-19'\n",
    "                 ,'xbol-20','xbol-21','xbol-22','xbol-23','xbol-24','xbol-25','xbol-26','xbol-27','xbol-28','xbol-29'\n",
    "                 ,'xbol-30','xbol-1','xbol-2','xbol-3','xbol-4','xbol-5','xbol-6','xbol-7','xbol-8','xbol-9'\n",
    "                 ,'xgenre','xtitle','xeol','xbol','xeos','xbos','[verse-1]','[verse-2]','[chorus]']:\n",
    "        s = str(s).replace(term,'')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram perplexity (v0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(file_name):\n",
    "    '''input file name (without extention)'''\n",
    "    # load model\n",
    "    m1_pkl = open(\"../data/models/\" + file_name + \".pkl\", \"rb\")\n",
    "    model = pickle.load(m1_pkl)\n",
    "    m1_pkl.close()\n",
    "    return model\n",
    "    \n",
    "    \n",
    "def get_entropy(text, model, _n=3, _lpad = ['<s>'], _rpad = ['<s>']):\n",
    "    ''' calculate average log probability of each word in text, given context\n",
    "        \n",
    "        IMPORTANT NOTE:  For initial implementation, we do not have bigram or unigram prob in our dict 'model',\n",
    "                         and this handles missing or unknown entries naively\n",
    "    '''\n",
    "    \n",
    "    e = 0.0\n",
    "    padded_string = \"<s> \" + example1 + \" <s>\"\n",
    "    text = padded_string.split(' ')\n",
    "    for i in range(_n - 1, len(text)):\n",
    "        context = tuple(text[i - _n + 1:i])\n",
    "        token = text[i]\n",
    "        #print(context,token)\n",
    "        #print(e)\n",
    "        e += -np.log2(model.get(context,dict()).get(token,0.0000001))  # this is a poor placeholder until we get backoff dicts\n",
    "    entropy = e / float(len(text) - (_n - 1))\n",
    "    return entropy\n",
    "\n",
    "def get_perplexity(text, model, _n = 3, _lpad = ['<s>'], _rpad = ['<s>']):\n",
    "    return np.power(2,get_entropy(text=text, model=model , _n=_n, _lpad=_lpad, _rpad=_rpad))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3258828.7202875423"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example1 = \"Mary had a little lamb whose fleece was white as snow, and everywhere that Mary went the sheep was sure to go\"\n",
    "\n",
    "model1 = load_model(\"trigram-weights\")\n",
    "\n",
    "get_perplexity(example1,model1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prosodic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Well, this was a BLACKHOLE OF WASTED TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install prosodic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import prosodic as p\n",
    "#text = p.Text(\"Shall I compare thee to a summer's day?\")\n",
    "#text.parse()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging and aggregation to % of text\n",
    "def nltkPOS(text,verbose=False):\n",
    "    '''For an input text, return absolute difference from published proportions, between 0 and 1.'''\n",
    "    \n",
    "    # define lookups\n",
    "    mapping = {'CC':'CC','DT':'DT','PDT':'DT','WDT':'DT','IN':'IN','JJ':'JJ','JJR':'JJ','JJS':'JJ'\n",
    "               ,'NN':'NN','NNS':'NN','NNP':'NN','NNPS':'NN','LS':'OT','CD':'OT','EX':'OT','FW':'OT'\n",
    "               ,'POS':'OT','UH':'OT','RB':'RB','RBR':'RB','RBS':'RB','WRB':'RB','TO':'TO','MD':'VB'\n",
    "               ,'RP':'VB','VB':'VB','VBD':'VB','VBG':'VB','VBN':'VB','VBP':'VB','VBZ':'VB','PRP':'WP'\n",
    "               ,'PRP$':'WP','WP':'WP','WP$':'WP'}\n",
    "    comp_dict = {'CC':0.0212,'DT':0.0982,'IN':0.0998,'JJ':0.0613,'NN':0.3051,'RB':0.0766,'TO':0.0351\n",
    "                 ,'VB':0.285,'WP':0.0058,'OT':0.012}\n",
    "\n",
    "    # initialize\n",
    "    pos_cnt = Counter()\n",
    "    total_word_cnt = 0\n",
    "    pos_dict = defaultdict(float) \n",
    "    pos_dict['adjustment'] = 0\n",
    "    absdiff = 0\n",
    "    \n",
    "    # prepare data  \n",
    "    text = prepString(removeMarkupWords(text))\n",
    "    tokenized_text = nltk.word_tokenize(text)\n",
    "    tag_list = nltk.pos_tag(tokenized_text)\n",
    "    \n",
    "    if not tag_list:\n",
    "        raise ValueError(\"Please provide more complete text\")\n",
    "    \n",
    "    # initial proportions\n",
    "    for t in tag_list:\n",
    "        pos_cnt[t[1]] +=1\n",
    "        total_word_cnt +=1\n",
    "    pos_raw_dict = {k: v/float(total_word_cnt) for k,v in dict(pos_cnt).items()}\n",
    "        \n",
    "    # adjust for items missing in mapping (mostly punctuation)    \n",
    "    for k,v in pos_raw_dict.items():\n",
    "        if k in mapping:\n",
    "            pos_dict[mapping[k]] += v \n",
    "        else:\n",
    "            pos_dict['adjustment'] += v\n",
    "    for k,v in pos_dict.items():\n",
    "        pos_dict[k] = pos_dict[k]/(1-pos_dict['adjustment'])\n",
    "    del pos_dict['adjustment']\n",
    "    \n",
    "    # compare to observed ratios, calculate absolute difference\n",
    "    for k in comp_dict.keys():\n",
    "        absdiff += abs(comp_dict[k] - pos_dict.get(k,0))\n",
    "        if verbose==True: \n",
    "            print(k,\"- benchmark:\",comp_dict[k],\", text:\",pos_dict.get(k,0),\"abs diff:\",abs(comp_dict[k] - pos_dict.get(k,0)))\n",
    "    return absdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5789"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I jump into a lake and keep swimming.\" #  The fluffy dog went to the north and first left.\"\n",
    "nltkPOS(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2898853658536585"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long = \"Deep throat is a Python program that can synthesize speech. A simple approach to unrestricted text-to-speech translation uses a small set of letter-to-sound rules, each rule specifying a pronunciation for one or more letters in some context. Deep throat features a small set of letter-to-sound rules that translate English text to phonemes producing usably accurate pronunciations of words. Deep throat can produce sounds by combining stored representations of phoneme sounds in accordance with generated phoneme translations. It can output these sounds to computer sound hardware using PortAudio and it can save them to sound file. \\\n",
    "Deep throat can accept text as a command line option argument, from a pipe and it can be set into an interactive mode.\\\n",
    "Deep throat can be set to read the date and time in various ways, such as in a loop. It can translate text to phonemes, it can translate specified phonemes to sounds and it can translate numbers to English text. It can engage visual and sound analyses.\"\n",
    "nltkPOS(long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rhyme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "phoneText = \"as I walk through the valley of the garden of death\"\n",
    "newText = \"I take a look at my life and realize there's nothing left\"\n",
    "hickoryText = \"Hickory Dickory Dock,\\nThe mouse ran up the clock.\\nThe clock struck one,\\nThe mouse ran down!\\nHickory Dickory Dock.\"\n",
    "# convert to phonemes\n",
    "#phones = [pronouncing.phones_for_word(word) for word in prepString(phoneText).split()]\n",
    "#phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleWord = 'orange'\n",
    "examplePhones = pronouncing.phones_for_word(exampleWord)[0]\n",
    "exampleRhymePart = pronouncing.rhyming_part(examplePhones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AO1', 'R', 'AH0', 'N', 'JH']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if len(exampleRhymePart.split()) > 1:\n",
    "    rp2 = exampleRhymePart.split()\n",
    "else: \n",
    "    rp2 = exampleRhymePart\n",
    "rp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AO1']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exampleRhymePart.split()[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate rhyme density.  Hopeful enhancements include:\n",
    "# 1) extending rhymeType\n",
    "# 2) adding text-to-phoneme and applying for tokens not in CMU dictionary\n",
    "# 3) improving the calculation by taking into consideration probability of rhymes\n",
    "# 4) removing repeat tokens from consideration to avoid rewarding repeated words\n",
    "\n",
    "def calcRhymeDensity(text,rhymeType='perfect',rhymeLocation='all',lineStartStop=(1,-2),printExamples=False):\n",
    "    '''calculates rhyme density (count of rhymes over n-1 words). \\n\\n\n",
    "    \n",
    "       _parameters_\n",
    "       text: input text for measurement\n",
    "       rhymeType: 'perfect' is a perfect rhyme, 'vowel' is a rhyming in the vowel sound + stress only\n",
    "       rhymeLocation: choose to look at 'all' text, 'section' by line numbers, or 'end' (last word in each line)    \n",
    "       lineStartStop: tuple of (start,stop) line numbers\n",
    "       printExamples: if True, print most common values of the selected rhymeType\n",
    "       \n",
    "       _returns_\n",
    "       rhyme_cnt: count of rhymes of specified rhymeType and rhymeLocation\n",
    "       wordCount: count of words of specified rhymeType and rhymeLocation\n",
    "       rhymeDensity: rhyme_cnt/float(wordCount-1)\n",
    "    '''\n",
    "    # restrict location to (end=last word, internal line = line, all= full text)\n",
    "    # count tokens\n",
    "    # \n",
    "    \n",
    "    # initialize\n",
    "    rhymePart_cnt = Counter()\n",
    "    rhyme_cnt = 0\n",
    "    \n",
    "    # prepare data\n",
    "    text = prepString(removeMarkupWords(text))\n",
    "    \n",
    "    if rhymeLocation == 'all':\n",
    "        words = text.split()\n",
    "    \n",
    "    if rhymeLocation == 'end':\n",
    "        lines = text.split(\"\\n\")\n",
    "        words = [line.split()[-1] for line in lines if len(line.split())>0]\n",
    "    \n",
    "    if rhymeLocation == 'section':\n",
    "        lines = text.split(\"\\n\")\n",
    "        words = [line.split()[-1] for line in lines[lineStartStop[0]:lineStartStop[1]+1] if len(line.split())>0]\n",
    "    \n",
    "    # \n",
    "    wordCount = len(words)\n",
    "    #print(words)\n",
    "    for word in words:\n",
    "        pros = pronouncing.phones_for_word(word)\n",
    "        if pros:     \n",
    "            phonelist = pros[0]  #using first pronunciation for now\n",
    "            if len(phonelist) > 0:\n",
    "                if rhymeType == 'perfect':\n",
    "                    rhymePart_cnt[pronouncing.rhyming_part(phonelist)] +=1\n",
    "\n",
    "                #if rhymeType == 'rime':\n",
    "                #    pass\n",
    "                #if rhymeType == 'soft':\n",
    "                #    pass\n",
    "                #if rhymeType == 'consonant':\n",
    "                #    pass\n",
    "\n",
    "                elif rhymeType == 'vowel':\n",
    "                    rhymePart_cnt[pronouncing.rhyming_part(phonelist).split()[0]] +=1\n",
    "    \n",
    "    for v in rhymePart_cnt.values():\n",
    "        rhyme_cnt += v-1\n",
    "    \n",
    "    if wordCount>1: \n",
    "        rhymeDensity = rhyme_cnt/float(wordCount-1)\n",
    "    else:\n",
    "        rhymeDensity = 0.0\n",
    "    \n",
    "    if printExamples == True:\n",
    "        print(rhymePart_cnt.most_common(5))\n",
    "    \n",
    "    return rhymeDensity, rhyme_cnt, wordCount\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AY1', 2), ('EY1 K', 1), ('AH0', 1), ('UH1 K', 1), ('AE1 T', 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.09090909090909091, 1, 12)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcRhymeDensity(newText,rhymeType='perfect',printExamples=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('OW1', 2), ('UW1', 2), ('AY1', 1), ('IH1 S', 1)]\n",
      "(0.4, 2, 6)\n"
     ]
    }
   ],
   "source": [
    "with open('../data/lyrics/current/samp1.txt') as sampf:\n",
    "    samp = sampf.read()\n",
    "    text = samp\n",
    "    text = removeMarkupWords(text)\n",
    "    text = prepString(text)\n",
    "    #print(text)\n",
    "    lines = text.split(\"\\n\")\n",
    "    #print(lines)\n",
    "    words = [line.split()[-1] for line in lines if len(line.split())>0]\n",
    "    #print(words)\n",
    "    print(calcRhymeDensity(text,rhymeType='perfect',rhymeLocation='end',printExamples=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.36363636363636365, 4, 12)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcRhymeDensity(newText,rhymeType='vowel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['orange', 'orange-green', 'orangeburg', 'oranges']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pronouncing.search(exampleRhymePart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 2, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcRhymeDensity(hickoryText,rhymeType='vowel',rhymeLocation='end')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(ref_list,candidateText,nGram=4,nGramType='cumulative',shouldSmooth=True):\n",
    "    '''calculates BLEU score \n",
    "    \n",
    "        _parameters_\n",
    "        ref_list: expects a list of reference texts to compare (as strings)\n",
    "        candidateText: the new text needing to be scored\n",
    "        nGram: choose between 1-4.  Determines which ngram(s) to use in the scoring\n",
    "        nGramType: 'cumulative' uses a simple average of all ngrams from 1 to nGram\n",
    "        shouldSmooth: if False, calculates the BLEU score without smoothing. Recommended to use smoothing (set to True)\n",
    "        \n",
    "        _returns_\n",
    "        score: BLEU score using nGram settings input, smoothed by default (can be turned off)\n",
    "    '''\n",
    "    \n",
    "    # basic checks\n",
    "    if nGram not in [1,2,3,4]:\n",
    "        raise ValueError('nGram must be between 1 and 4')\n",
    "    \n",
    "    if nGramType not in ['cumulative','exclusive']:\n",
    "        raise ValueError('nGramType must either be cumulative (average of nGrams less than n) or exclusive (1=unigram, etc.)')\n",
    "    \n",
    "    # pre-score\n",
    "    weight_dict = {('cumulative',1):(1,0,0,0)\n",
    "                  ,('cumulative',2):(.5,.5,0,0)\n",
    "                  ,('cumulative',3):(.3333,.3333,.3333,0)\n",
    "                  ,('cumulative',4):(.25,.25,.25,.25)\n",
    "                  ,('exclusive',1):(1,0,0,0)\n",
    "                  ,('exclusive',2):(0,1,0,0)\n",
    "                  ,('exclusive',3):(0,0,1,0)\n",
    "                  ,('exclusive',4):(0,0,0,1)}\n",
    "    candidate = [removePunc(str(removeMarkupWords(candidateText))).split()]\n",
    "    references = [[removePunc(str(removeMarkupWords(ref))).split() for ref in ref_list]]\n",
    "    weights = weight_dict[(nGramType,nGram)]\n",
    "       \n",
    "    \n",
    "    # scoring\n",
    "    if shouldSmooth==True:\n",
    "        smoother = bleu_score.SmoothingFunction().method5\n",
    "    else:\n",
    "        smoother = None\n",
    "    score = bleu_score.corpus_bleu(references, candidate, weights, smoothing_function=smoother)\n",
    "    #print(score)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0692255179440046"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu([newText],phoneText,4,'cumulative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0769800358919501"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu([phoneText],hickoryText,4,'cumulative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findLineStress(line):\n",
    "    '''find accentual stress of a given line, based on CMU dict.  Still a bit unclever.\n",
    "    \n",
    "    _parameters_\n",
    "    line: line of text\n",
    "    \n",
    "    _returns_\n",
    "    parselist: list of potential stresses after parsing. 0 is unstressed, 1 is primary stress, 2 is secondary stress (middle)\n",
    "    syllableLengths: list of syllable lengths corresponding to the parses in parselist\n",
    "    wordCount: count of words in the line \n",
    "    '''\n",
    "    line = prepString(removeMarkupWords(line))\n",
    "    words = line.split()\n",
    "    wordCount = len(words)\n",
    "    parses = ['']\n",
    "    for word in words:\n",
    "        pros = pronouncing.phones_for_word(word)\n",
    "        if pros:\n",
    "            for phonelist in [pronouncing.phones_for_word(word)]:           \n",
    "                stressOptions = copy.deepcopy(parses)\n",
    "                currLen = len(parses)\n",
    "                newparse = []\n",
    "                # I don't really need to loop through pronunciations, just distinct stress patterns, so a little inefficient here\n",
    "                for pronunciation in phonelist:\n",
    "                    wordStress = pronouncing.stresses(pronunciation)\n",
    "                    for option in range(currLen):\n",
    "                        newparse.append(''+str(stressOptions[option]) + str(wordStress))\n",
    "            parses = newparse \n",
    "\n",
    "    return list(set(parses)), [len(parse) for parse in list(set(parses))], wordCount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['1111110111011', '1111010101011', '1111010111011', '1111110101011'],\n",
       " [13, 13, 13, 13],\n",
       " 11)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findLineStress(phoneText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['111111101021101', '110111111021101', '110111101021101', '111111111021101'],\n",
       " [15, 15, 15, 15],\n",
       " 12)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findLineStress(newText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(s1, s2):\n",
    "    '''calculate levenshtein distance for two input strings\n",
    "    \n",
    "    _parameters_\n",
    "    s1: first input string\n",
    "    s2: second input string\n",
    "    \n",
    "    _returns_\n",
    "    distance: levenshtein distance between two strings...that is, the lowest number of modifications to turn s1 into s2\n",
    "    '''\n",
    "    s1 = str(s1)\n",
    "    s2 = str(s2)\n",
    "    \n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein(s2, s1)\n",
    "\n",
    "    # otherwise len(s1) >= len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer\n",
    "            deletions = current_row[j] + 1       # than s2\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]\n",
    "    \n",
    "def findMeter(text):\n",
    "    '''finds meter with smallest edit distance\n",
    "    \n",
    "    _parameters_\n",
    "    text: input text, usually a poem of some kind\n",
    "    \n",
    "    _returns_\n",
    "    lowest: lowest edit distance for any standard accentual-syllabic verse\n",
    "    options: list of potential meters for the lowest edit distance.\n",
    "    '''\n",
    "    # define\n",
    "    meter_dict = {'0101':'Iambic dimeter'\n",
    "                  ,'010101':'Iambic trimeter'\n",
    "                  ,'01010101':'Iambic tetrameter'\n",
    "                  ,'0101010101':'Iambic pentameter'\n",
    "                  ,'010101010101':'Iambic hexameter'\n",
    "                  ,'01010101010101':'Iambic heptameter'\n",
    "                  ,'0101010101010101':'Iambic octameter'\n",
    "                  ,'1010':'Trochaic dimeter'\n",
    "                  ,'101010':'Trochaic trimeter'\n",
    "                  ,'10101010':'Trochaic tetrameter'\n",
    "                  ,'1010101010':'Trochaic pentameter'\n",
    "                  ,'101010101010':'Trochaic hexameter'\n",
    "                  ,'10101010101010':'Trochaic heptameter'\n",
    "                  ,'1010101010101010':'Trochaic octameter'\n",
    "                  ,'001001':'Anapestic dimeter'\n",
    "                  ,'001001001':'Anapestic trimeter'\n",
    "                  ,'001001001001':'Anapestic tetrameter'\n",
    "                  ,'001001001001001':'Anapestic pentameter'\n",
    "                  ,'001001001001001001':'Anapestic hexameter'\n",
    "                  ,'001001001001001001001':'Anapestic heptameter'\n",
    "                  ,'100100':'Dactyllic dimeter'\n",
    "                  ,'100100100':'Dactyllic trimeter'\n",
    "                  ,'100100100100':'Dactyllic tetrameter'\n",
    "                  ,'100100100100100':'Dactyllic pentameter'\n",
    "                  ,'100100100100100100':'Dactyllic hexameter'\n",
    "                  ,'100100100100100100100':'Dactyllic heptameter'}\n",
    "\n",
    "    # initialize\n",
    "    vote_cnt = Counter()\n",
    "    text = prepString(removeMarkupWords(text))\n",
    "    lines = text.split('\\n')\n",
    "    line_cnt = len(lines)\n",
    "    minDist = 999\n",
    "    \n",
    "    # update distances\n",
    "    for line in lines:\n",
    "        for k,v in meter_dict.items():\n",
    "            minDist = 999\n",
    "            for reading in findLineStress(line)[0]:\n",
    "                dist = levenshtein(k,reading)\n",
    "                if dist < minDist:\n",
    "                    minDist = dist    \n",
    "            vote_cnt[v] += minDist\n",
    "    \n",
    "    #options = min(vote_cnt, key=vote_cnt.get)  #chooses one in the event of ties\n",
    "    lowest = min(vote_cnt.values()) \n",
    "    options = [k for k,v in vote_cnt.items() if v==lowest]\n",
    "    return lowest, options, line_cnt, lowest/float(line_cnt) #, vote_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as I walk through the valley of the garden of death \n",
      " ['1111110111011', '1111010101011', '1111010111011', '1111110101011'] \n",
      " (3, ['Iambic hexameter', 'Iambic heptameter', 'Trochaic hexameter', 'Trochaic heptameter'], 1, 3.0)\n"
     ]
    }
   ],
   "source": [
    "print(phoneText,'\\n',findLineStress(phoneText)[0],'\\n',findMeter(phoneText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hickory Dickory Dock,\n",
      "The mouse ran up the clock.\n",
      "The clock struck one,\n",
      "The mouse ran down!\n",
      "Hickory Dickory Dock.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6, ['Iambic dimeter'], 5, 1.2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(hickoryText)\n",
    "findMeter(hickoryText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But, soft! what light through yonder window breaks? \n",
      " It is the east, and Juliet is the sun \n",
      " (4, ['Iambic pentameter'], 2, 2.0)\n"
     ]
    }
   ],
   "source": [
    "romeoText = \"But, soft! what light through yonder window breaks? \\n It is the east, and Juliet is the sun\"\n",
    "print(romeoText,'\\n',findMeter(romeoText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputText1 = \"xbos xbol listen to my tale of woe , xeol \\n  xbol it ''s terribly sad but true , xeol \\n  xbol all dressed up , no place to go xeol \\n  xbol each evening i ''m awfully blue . xeol \\n  xbol xeol \\n xbol i must win some handsome guy xeol \\n xbol can ''t go on like this , xeol\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xbos xbol listen to my tale of woe , xeol  (['1011111', '1001111'], [7, 7], 6)\n",
      "  xbol it ''s terribly sad but true , xeol  (['1100111', '0100111'], [7, 7], 6)\n",
      "  xbol all dressed up , no place to go xeol  (['1111111', '1111101'], [7, 7], 7)\n",
      "  xbol each evening i ''m awfully blue . xeol  (['1101101', '11011001'], [7, 8], 6)\n",
      "  xbol xeol  ([''], [0], 0)\n",
      " xbol i must win some handsome guy xeol  (['1111101'], [7], 6)\n",
      " xbol can ''t go on like this , xeol (['11110', '01111', '11111', '01110'], [5, 5, 5, 5], 6)\n"
     ]
    }
   ],
   "source": [
    "for line in outputText1.split('\\n'):\n",
    "    print(line, findLineStress(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('UW1', 4), ('AY1', 4), ('OW1', 4), ('IH1 S AH0 N', 1), ('EY1 L', 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.25, 9, 37)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcRhymeDensity(outputText1,rhymeType='perfect',printExamples=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('IY1 P AH0 L', 2)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 1, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"people steeple\"\n",
    "b = \"poodle stroooudel\"\n",
    "calcRhymeDensity(a,rhymeType='perfect',printExamples=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scoreDirectory(source_dir='../data/lyrics/current/'\n",
    "                   ,destination_dir='../data/scores/'\n",
    "                   ,output_file_name='output.csv'\n",
    "                   ,reference_dir='../data/lyrics/validation/'):\n",
    "    \n",
    "    '''create .csv with scores from files in specified directory'''\n",
    "    \n",
    "    songs = glob.glob(source_dir+'/*.txt')  \n",
    "    refs = glob.glob(reference_dir+'/*.txt')\n",
    "    ref_list = []\n",
    "    \n",
    "    ## add BLEU reference code\n",
    "    for ref in refs:\n",
    "        with open(ref) as rf:\n",
    "            ref_raw_text = rf.read()\n",
    "            ref_list.append(ref_raw_text)\n",
    "    \n",
    "    with open(destination_dir+output_file_name, 'w', newline='') as outf:\n",
    "        cw = csv.writer(outf,quoting=csv.QUOTE_NONNUMERIC)    \n",
    "        cw.writerow(['Model_Name'\n",
    "                    ,'Temperature'\n",
    "                    ,'Beam_Width'\n",
    "                    ,'Item_Number'\n",
    "                    ,'Genre'\n",
    "                    ,'Title'\n",
    "                    ,'POSConfirmity'\n",
    "                    ,'RD_PerfectAll'\n",
    "                    ,'RD_PerfectEnd'\n",
    "                    ,'RD_VowelAll'\n",
    "                    ,'RD_VowelEnd'\n",
    "                    ,'ClosestMeter'\n",
    "                    ,'AvgDistanceToMeter'\n",
    "                    ,'BLEU_1_excl_Unsmoothed'\n",
    "                    ,'BLEU_2_excl_Unsmoothed'\n",
    "                    ,'BLEU_3_excl_Unsmoothed'\n",
    "                    ,'BLEU_4_excl_Unsmoothed'\n",
    "                    ,'BLEU_3_cumul_Smoothed'\n",
    "                    ,'BLEU_4_cumul_Smoothed'\n",
    "                    ,'Text'\n",
    "                    ])\n",
    "\n",
    "        for song in songs: \n",
    "            with open(song, newline='') as f: \n",
    "                model = source_dir.split('/')[-2]\n",
    "                item,temperature,beamWidth = os.path.splitext(os.path.basename(song))[0].split('-')\n",
    "                filename = os.path.basename(song)\n",
    "                rawText = f.read()            \n",
    "                \n",
    "                \n",
    "                ## if exists, extract genre\n",
    "                genres = re.search('xgenre (.*?) xtitle', rawText)\n",
    "                if genres.group(1):\n",
    "                    genre = genres.group(1)\n",
    "                else:\n",
    "                    genre = ''\n",
    "                \n",
    "                ## if exists, extract title\n",
    "                titles = re.search('xtitle (.*?) xbol',rawText)\n",
    "                if titles.group(1):\n",
    "                    title = titles.group(1)\n",
    "                else:\n",
    "                    title = ''\n",
    "                \n",
    "                ## ignore (or remove) everything before 'xbol-1'\n",
    "                ## replace 'xeol' with '\\n'\n",
    "                \n",
    "                text = removeMarkupWords(re.search('xbol-?\\d? (.*)',rawText).group(1).replace(' xeol ','\\n'))\n",
    "                \n",
    "                cw.writerow([model\n",
    "                            ,temperature\n",
    "                            ,beamWidth\n",
    "                            ,item\n",
    "                            ,genre\n",
    "                            ,title\n",
    "                            ,round(nltkPOS(text),4)\n",
    "                            ,round(calcRhymeDensity(text,rhymeType='perfect',rhymeLocation='all')[0],4)\n",
    "                            ,round(calcRhymeDensity(text,rhymeType='perfect',rhymeLocation='end',printExamples=False)[0],4)\n",
    "                            ,round(calcRhymeDensity(text,rhymeType='vowel',rhymeLocation='all')[0],4)\n",
    "                            ,round(calcRhymeDensity(text,rhymeType='vowel',rhymeLocation='end')[0],4) \n",
    "                            ,findMeter(text)[1][0]\n",
    "                            ,round(findMeter(text)[3],4)\n",
    "                            ,round(bleu(ref_list,text,nGram=1,nGramType='exclusive',shouldSmooth=False),4)\n",
    "                            ,round(bleu(ref_list,text,nGram=2,nGramType='exclusive',shouldSmooth=False),4)\n",
    "                            ,round(bleu(ref_list,text,nGram=3,nGramType='exclusive',shouldSmooth=False),4)\n",
    "                            ,round(bleu(ref_list,text,nGram=4,nGramType='exclusive',shouldSmooth=False),4)\n",
    "                            ,round(bleu(ref_list,text,nGram=3,nGramType='cumulative',shouldSmooth=True),4)\n",
    "                            ,round(bleu(ref_list,text,nGram=4,nGramType='cumulative',shouldSmooth=True),4)\n",
    "                            ,text\n",
    "                            ]\n",
    "                           )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-0d46f00ef64f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m                    \u001b[1;33m,\u001b[0m\u001b[0mdestination_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'../data/scores/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                    \u001b[1;33m,\u001b[0m\u001b[0moutput_file_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'output.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                    ,reference_dir='../data/lyrics/reference/')\n\u001b[0m",
      "\u001b[1;32m<ipython-input-36-7e290b5c10b3>\u001b[0m in \u001b[0;36mscoreDirectory\u001b[1;34m(source_dir, destination_dir, output_file_name, reference_dir)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msource_dir\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbeamWidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m                 \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                 \u001b[0mrawText\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "scoreDirectory(source_dir='../data/lyrics/current/'\n",
    "                   ,destination_dir='../data/scores/'\n",
    "                   ,output_file_name='output.csv'\n",
    "                   ,reference_dir='../data/lyrics/reference/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '4.2-LM-108k-lines-genre-song_title'\n",
    "\n",
    "scoreDirectory(source_dir=f'../../transfer/w210-capstone/lyrics/{model_name}/'\n",
    "               ,output_file_name=f'scores_{model_name}.csv'\n",
    "               ,reference_dir='../data/lyrics/validation/'\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '../../transfer/w210-capstone/lyrics/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../transfer/w210-capstone/lyrics/test\\\\1543791069-1.4-5.txt']\n",
      "1543791069-1.4-5.txt test 1543791069 1.4 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "songs = glob.glob(test_dir+'*.txt')  \n",
    "print(songs)\n",
    "for song in songs: \n",
    "    with open(song) as f: \n",
    "        text = f.read()\n",
    "        model = test_dir.split('/')[-2]\n",
    "        filename = os.path.basename(song)\n",
    "        item,temperature,beamWidth = os.path.splitext(os.path.basename(song))[0].split('-')\n",
    "        print(filename,model,item,temperature,beamWidth)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=time.time()\n",
    "scoreDirectory(source_dir='../../transfer/w210-capstone/lyrics/test2/'\n",
    "               ,output_file_name='test2.csv'\n",
    "               ,reference_dir='../data/lyrics/validation/')\n",
    "#scoreDirectory(source_dir=test_dir,output_file_name='test.csv',reference_dir='../data/lyrics/validation/')\n",
    "b=time.time()\n",
    "print(b-a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
